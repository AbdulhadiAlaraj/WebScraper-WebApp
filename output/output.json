[
    {
        "title": "Introduction | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/get_started/introduction",
        "text_content": "Get started Introduction On this page Introduction LangChain is a framework for developing applications powered by language models. It enables applications that: Are context-aware : connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.) Reason : rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.) This framework consists of several parts. LangChain Libraries : The Python and JavaScript libraries. Contains interfaces and integrations for a myriad of components, a basic run time for combining these components into chains and agents, and off-the-shelf implementations of chains and agents. LangChain Templates : A collection of easily deployable reference architectures for a wide variety of tasks. LangServe : A library for deploying LangChain chains as a REST API. LangSmith : A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain. Together, these products simplify the entire application lifecycle: Develop : Write your applications in LangChain/LangChain.js. Hit the ground running using Templates for reference. Productionize : Use LangSmith to inspect, test and monitor your chains, so that you can constantly improve and deploy with confidence. Deploy : Turn any chain into an API with LangServe. LangChain Libraries ​ The main value props of the LangChain packages are: Components : composable tools and integrations for working with language models. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not Off-the-shelf chains : built-in assemblages of components for accomplishing higher-level tasks Off-the-shelf chains make it easy to get started. Components make it easy to customize existing chains and build new ones. The LangChain libraries themselves are made up of several different packages. langchain-core : Base abstractions and LangChain Expression Language. langchain-community : Third party integrations. langchain : Chains, agents, and retrieval strategies that make up an application's cognitive architecture. Get started ​ Here’s how to install LangChain, set up your environment, and start building. We recommend following our Quickstart guide to familiarize yourself with the framework by building your first LangChain application. Read up on our Security best practices to make sure you're developing safely with LangChain. note These docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library. LangChain Expression Language (LCEL) ​ LCEL is a declarative way to compose chains. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains. Overview : LCEL and its benefits Interface : The standard interface for LCEL objects How-to : Key features of LCEL Cookbook : Example code for accomplishing common tasks Modules ​ LangChain provides standard, extendable interfaces and integrations for the following modules: Model I/O ​ Interface with language models Retrieval ​ Interface with application-specific data Agents ​ Let models choose which tools to use given high-level directives Examples, ecosystem, and resources ​ Use cases ​ Walkthroughs and techniques for common end-to-end use cases, like: Document question answering Chatbots Analyzing structured data and much more... Integrations ​ LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations . Guides ​ Best practices for developing with LangChain. API reference ​ Head to the reference section for full documentation of all classes and methods in the LangChain and LangChain Experimental Python packages. Developer's guide ​ Check out the developer's guide for guidelines on contributing and help getting your dev environment set up. Community ​ Head to the Community navigator to find places to ask questions, share feedback, meet other developers, and dream about the future of LLM’s. Previous Get started Next Installation"
    },
    {
        "title": "SQL | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/qa_structured/sql",
        "text_content": "QA over structured data SQL On this page SQL Open In Colab Use case ​ Enterprise data is often stored in SQL databases. LLMs make it possible to interact with SQL databases using natural\nlanguage. LangChain offers SQL Chains and Agents to build and run SQL queries\nbased on natural language prompts. These are compatible with any SQL dialect supported by SQLAlchemy (e.g.,\nMySQL, PostgreSQL, Oracle SQL, Databricks, SQLite). They enable use cases such as: Generating queries that will be run based on natural language\nquestions Creating chatbots that can answer questions based on database data Building custom dashboards based on insights a user wants to analyze Overview ​ LangChain provides tools to interact with SQL Databases: Build SQL queries based on natural language user questions Query a SQL database using chains for query creation and execution Interact with a SQL database using agents for robust and flexible\nquerying Quickstart ​ First, get required packages and set environment variables: ! pip install langchain langchain - experimental openai # Set env var OPENAI_API_KEY or load from a .env file # import dotenv # dotenv.load_dotenv() The below example will use a SQLite connection with Chinook database. Follow installation\nsteps to create Chinook.db in the same directory as this notebook: Save this\nfile to the directory as Chinook_Sqlite.sql Run sqlite3 Chinook.db Run .read Chinook_Sqlite.sql Test SELECT * FROM Artist LIMIT 10; Now, Chinhook.db is in our directory. Let’s create a SQLDatabaseChain to create and execute SQL queries. from langchain . llms import OpenAI from langchain . utilities import SQLDatabase from langchain_experimental . sql import SQLDatabaseChain db = SQLDatabase . from_uri ( \"sqlite:///Chinook.db\" ) llm = OpenAI ( temperature = 0 , verbose = True ) db_chain = SQLDatabaseChain . from_llm ( llm , db , verbose = True ) db_chain . run ( \"How many employees are there?\" ) > Entering new SQLDatabaseChain chain... How many employees are there? SQLQuery:SELECT COUNT(*) FROM \"Employee\"; SQLResult: [(8,)] Answer:There are 8 employees. > Finished chain. 'There are 8 employees.' Note that this both creates and executes the query. In the following sections, we will cover the 3 different use cases\nmentioned in the overview. Go deeper ​ You can load tabular data from other sources other than SQL Databases.\nFor example: - Loading a CSV\nfile - Loading a Pandas\nDataFrame Here\nyou can check full list of Document\nLoaders Case 1: Text-to-SQL query ​ from langchain . chains import create_sql_query_chain from langchain . chat_models import ChatOpenAI Let’s create the chain that will build the SQL Query: chain = create_sql_query_chain ( ChatOpenAI ( temperature = 0 ) , db ) response = chain . invoke ( { \"question\" : \"How many employees are there\" } ) print ( response ) SELECT COUNT(*) FROM Employee After building the SQL query based on a user question, we can execute\nthe query: db . run ( response ) '[(8,)]' As we can see, the SQL Query Builder chain only created the query,\nand we handled the query execution separately . Go deeper ​ Looking under the hood We can look at the LangSmith\ntrace to unpack this: Some papers have reported good\nperformance when prompting with: A CREATE TABLE description for each table, which include column\nnames, their types, etc Followed by three example rows in a SELECT statement create_sql_query_chain adopts this the best practice (see more in this blog ). Improvements The query builder can be improved in several ways, such as (but not\nlimited to): Customizing database description to your specific use case Hardcoding a few examples of questions and their corresponding SQL\nquery in the prompt Using a vector database to include dynamic examples that are\nrelevant to the specific user question All these examples involve customizing the chain’s prompt. For example, we can include a few examples in our prompt like so: from langchain . prompts import PromptTemplate TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Use the following format: Question: \"Question here\" SQLQuery: \"SQL Query to run\" SQLResult: \"Result of the SQLQuery\" Answer: \"Final answer here\" Only use the following tables: {table_info}. Some examples of SQL queries that correspond to questions are: {few_shot_examples} Question: {input}\"\"\" CUSTOM_PROMPT = PromptTemplate ( input_variables = [ \"input\" , \"few_shot_examples\" , \"table_info\" , \"dialect\" ] , template = TEMPLATE , ) We can also access this prompt in the\nLangChain prompt hub. This will work with your LangSmith API\nkey . from langchain import hub CUSTOM_PROMPT = hub . pull ( \"rlm/text-to-sql\" ) Case 2: Text-to-SQL query and execution ​ We can use SQLDatabaseChain from langchain_experimental to create\nand run SQL queries. from langchain . llms import OpenAI from langchain_experimental . sql import SQLDatabaseChain llm = OpenAI ( temperature = 0 , verbose = True ) db_chain = SQLDatabaseChain . from_llm ( llm , db , verbose = True ) db_chain . run ( \"How many employees are there?\" ) > Entering new SQLDatabaseChain chain... How many employees are there? SQLQuery:SELECT COUNT(*) FROM \"Employee\"; SQLResult: [(8,)] Answer:There are 8 employees. > Finished chain. 'There are 8 employees.' As we can see, we get the same result as the previous case. Here, the chain also handles the query execution and provides a\nfinal answer based on the user question and the query result. Be careful while using this approach as it is susceptible to SQL Injection : The chain is executing queries that are created by an LLM, and\nweren’t validated e.g. records may be created, modified or deleted unintentionally _ This is why we see the SQLDatabaseChain is inside langchain_experimental . Go deeper ​ Looking under the hood We can use the LangSmith\ntrace to see what is happening under the hood: As discussed above, first we create the query: text: ' SELECT COUNT(*) FROM \"Employee\";' Then, it executes the query and passes the results to an LLM for\nsynthesis. Improvements The performance of the SQLDatabaseChain can be enhanced in several\nways: Adding sample rows Specifying custom table\ninformation Using Query\nChecker self-correct invalid SQL using parameter use_query_checker=True Customizing the LLM\nPrompt include\nspecific instructions or relevant information, using parameter prompt=CUSTOM_PROMPT Get intermediate\nsteps access the SQL statement as well as the final result using parameter return_intermediate_steps=True Limit the number of\nrows a query will return using parameter top_k=5 You might find SQLDatabaseSequentialChain useful for cases in which the number of tables in the database is large. This Sequential Chain handles the process of: Determining which tables to use based on the user question Calling the normal SQL database chain using only relevant tables Adding Sample Rows Providing sample data can help the LLM construct correct queries when\nthe data format is not obvious. For example, we can tell LLM that artists are saved with their full\nnames by providing two rows from the Track table. db = SQLDatabase . from_uri ( \"sqlite:///Chinook.db\" , include_tables = [ \"Track\" ] , # we include only one table to save tokens in the prompt :) sample_rows_in_table_info = 2 , ) The sample rows are added to the prompt after each corresponding table’s\ncolumn information. We can use db.table_info and check which sample rows are included: print ( db . table_info ) CREATE TABLE \"Track\" ( \"TrackId\" INTEGER NOT NULL, \"Name\" NVARCHAR(200) NOT NULL, \"AlbumId\" INTEGER, \"MediaTypeId\" INTEGER NOT NULL, \"GenreId\" INTEGER, \"Composer\" NVARCHAR(220), \"Milliseconds\" INTEGER NOT NULL, \"Bytes\" INTEGER, \"UnitPrice\" NUMERIC(10, 2) NOT NULL, PRIMARY KEY (\"TrackId\"), FOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), FOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), FOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\") ) /* 2 rows from Track table: TrackId Name    AlbumId MediaTypeId GenreId Composer    Milliseconds    Bytes   UnitPrice 1   For Those About To Rock (We Salute You) 1   1   1   Angus Young, Malcolm Young, Brian Johnson   343719  11170334    0.99 2   Balls to the Wall   2   2   1   None    342562  5510424 0.99 */ Case 3: SQL agents ​ LangChain has an SQL Agent which provides a more flexible way of\ninteracting with SQL Databases than the SQLDatabaseChain . The main advantages of using the SQL Agent are: It can answer questions based on the databases’ schema as well as on\nthe databases’ content (like describing a specific table) It can recover from errors by running a generated query, catching\nthe traceback and regenerating it correctly To initialize the agent, we use create_sql_agent function. This agent contains the SQLDatabaseToolkit which contains tools to: Create and execute queries Check query syntax Retrieve table descriptions … and more from langchain . agents import create_sql_agent from langchain . agents . agent_toolkits import SQLDatabaseToolkit # from langchain.agents import AgentExecutor from langchain . agents . agent_types import AgentType db = SQLDatabase . from_uri ( \"sqlite:///Chinook.db\" ) agent_executor = create_sql_agent ( llm = OpenAI ( temperature = 0 ) , toolkit = SQLDatabaseToolkit ( db = db , llm = OpenAI ( temperature = 0 ) ) , verbose = True , agent_type = AgentType . ZERO_SHOT_REACT_DESCRIPTION , ) Agent task example #1 - Running queries ​ agent_executor . run ( \"List the total sales per country. Which country's customers spent the most?\" ) > Entering new AgentExecutor chain... Action: sql_db_list_tables Action Input: Observation: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Thought: I should query the schema of the Invoice and Customer tables. Action: sql_db_schema Action Input: Invoice, Customer Observation: CREATE TABLE \"Customer\" ( \"CustomerId\" INTEGER NOT NULL, \"FirstName\" NVARCHAR(40) NOT NULL, \"LastName\" NVARCHAR(20) NOT NULL, \"Company\" NVARCHAR(80), \"Address\" NVARCHAR(70), \"City\" NVARCHAR(40), \"State\" NVARCHAR(40), \"Country\" NVARCHAR(40), \"PostalCode\" NVARCHAR(10), \"Phone\" NVARCHAR(24), \"Fax\" NVARCHAR(24), \"Email\" NVARCHAR(60) NOT NULL, \"SupportRepId\" INTEGER, PRIMARY KEY (\"CustomerId\"), FOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\") ) /* 3 rows from Customer table: CustomerId  FirstName   LastName    Company Address City    State   Country PostalCode  Phone   Fax Email   SupportRepId 1   Luís    Gonçalves   Embraer - Empresa Brasileira de Aeronáutica S.A.    Av. Brigadeiro Faria Lima, 2170 São José dos Campos SP  Brazil  12227-000   +55 (12) 3923-5555  +55 (12) 3923-5566  luisg@embraer.com.br    3 2   Leonie  Köhler  None    Theodor-Heuss-Straße 34 Stuttgart   None    Germany 70174   +49 0711 2842222    None    leonekohler@surfeu.de   5 3   François    Tremblay    None    1498 rue Bélanger   Montréal    QC  Canada  H2G 1A7 +1 (514) 721-4711   None    ftremblay@gmail.com 3 */ CREATE TABLE \"Invoice\" ( \"InvoiceId\" INTEGER NOT NULL, \"CustomerId\" INTEGER NOT NULL, \"InvoiceDate\" DATETIME NOT NULL, \"BillingAddress\" NVARCHAR(70), \"BillingCity\" NVARCHAR(40), \"BillingState\" NVARCHAR(40), \"BillingCountry\" NVARCHAR(40), \"BillingPostalCode\" NVARCHAR(10), \"Total\" NUMERIC(10, 2) NOT NULL, PRIMARY KEY (\"InvoiceId\"), FOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\") ) /* 3 rows from Invoice table: InvoiceId   CustomerId  InvoiceDate BillingAddress  BillingCity BillingState    BillingCountry  BillingPostalCode   Total 1   2   2009-01-01 00:00:00 Theodor-Heuss-Straße 34 Stuttgart   None    Germany 70174   1.98 2   4   2009-01-02 00:00:00 Ullevålsveien 14    Oslo    None    Norway  0171    3.96 3   8   2009-01-03 00:00:00 Grétrystraat 63 Brussels    None    Belgium 1000    5.94 */ Thought: I should query the total sales per country. Action: sql_db_query Action Input: SELECT Country, SUM(Total) AS TotalSales FROM Invoice INNER JOIN Customer ON Invoice.CustomerId = Customer.CustomerId GROUP BY Country ORDER BY TotalSales DESC LIMIT 10 Observation: [('USA', 523.0600000000003), ('Canada', 303.9599999999999), ('France', 195.09999999999994), ('Brazil', 190.09999999999997), ('Germany', 156.48), ('United Kingdom', 112.85999999999999), ('Czech Republic', 90.24000000000001), ('Portugal', 77.23999999999998), ('India', 75.25999999999999), ('Chile', 46.62)] Thought: I now know the final answer Final Answer: The country with the highest total sales is the USA, with a total of $523.06. > Finished chain. 'The country with the highest total sales is the USA, with a total of $523.06.' Looking at the LangSmith\ntrace ,\nwe can see: The agent is using a ReAct style prompt First, it will look at the tables: Action: sql_db_list_tables using tool sql_db_list_tables Given the tables as an observation, it thinks and then\ndeterminates the next action : Observation: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Thought: I should query the schema of the Invoice and Customer tables. Action: sql_db_schema Action Input: Invoice, Customer It then formulates the query using the schema from tool sql_db_schema Thought: I should query the total sales per country. Action: sql_db_query Action Input: SELECT Country, SUM(Total) AS TotalSales FROM Invoice INNER JOIN Customer ON Invoice.CustomerId = Customer.CustomerId GROUP BY Country ORDER BY TotalSales DESC LIMIT 10 It finally executes the generated query using tool sql_db_query Agent task example #2 - Describing a Table ​ agent_executor . run ( \"Describe the playlisttrack table\" ) > Entering new AgentExecutor chain... Action: sql_db_list_tables Action Input: Observation: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Thought: The PlaylistTrack table is the most relevant to the question. Action: sql_db_schema Action Input: PlaylistTrack Observation: CREATE TABLE \"PlaylistTrack\" ( \"PlaylistId\" INTEGER NOT NULL, \"TrackId\" INTEGER NOT NULL, PRIMARY KEY (\"PlaylistId\", \"TrackId\"), FOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), FOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\") ) /* 3 rows from PlaylistTrack table: PlaylistId  TrackId 1   3402 1   3389 1   3390 */ Thought: I now know the final answer Final Answer: The PlaylistTrack table contains two columns, PlaylistId and TrackId, which are both integers and form a primary key. It also has two foreign keys, one to the Track table and one to the Playlist table. > Finished chain. 'The PlaylistTrack table contains two columns, PlaylistId and TrackId, which are both integers and form a primary key. It also has two foreign keys, one to the Track table and one to the Playlist table.' Extending the SQL Toolkit ​ Although the out-of-the-box SQL Toolkit contains the necessary tools to\nstart working on a database, it is often the case that some extra tools\nmay be useful for extending the agent’s capabilities. This is\nparticularly useful when trying to use domain specific knowledge in\nthe solution, in order to improve its overall performance. Some examples include: Including dynamic few shot examples Finding misspellings in proper nouns to use as column filters We can create separate tools which tackle these specific use cases and\ninclude them as a complement to the standard SQL Toolkit. Let’s see how\nto include these two custom tools. Including dynamic few-shot examples ​ In order to include dynamic few-shot examples, we need a custom Retriever Tool that handles the vector database in order to retrieve\nthe examples that are semantically similar to the user’s question. Let’s start by creating a dictionary with some examples: few_shots = { \"List all artists.\" : \"SELECT * FROM artists;\" , \"Find all albums for the artist 'AC/DC'.\" : \"SELECT * FROM albums WHERE ArtistId = (SELECT ArtistId FROM artists WHERE Name = 'AC/DC');\" , \"List all tracks in the 'Rock' genre.\" : \"SELECT * FROM tracks WHERE GenreId = (SELECT GenreId FROM genres WHERE Name = 'Rock');\" , \"Find the total duration of all tracks.\" : \"SELECT SUM(Milliseconds) FROM tracks;\" , \"List all customers from Canada.\" : \"SELECT * FROM customers WHERE Country = 'Canada';\" , \"How many tracks are there in the album with ID 5?\" : \"SELECT COUNT(*) FROM tracks WHERE AlbumId = 5;\" , \"Find the total number of invoices.\" : \"SELECT COUNT(*) FROM invoices;\" , \"List all tracks that are longer than 5 minutes.\" : \"SELECT * FROM tracks WHERE Milliseconds > 300000;\" , \"Who are the top 5 customers by total purchase?\" : \"SELECT CustomerId, SUM(Total) AS TotalPurchase FROM invoices GROUP BY CustomerId ORDER BY TotalPurchase DESC LIMIT 5;\" , \"Which albums are from the year 2000?\" : \"SELECT * FROM albums WHERE strftime('%Y', ReleaseDate) = '2000';\" , \"How many employees are there\" : 'SELECT COUNT(*) FROM \"employee\"' , } We can then create a retriever using the list of questions, assigning\nthe target SQL query as metadata: from langchain . embeddings . openai import OpenAIEmbeddings from langchain . schema import Document from langchain . vectorstores import FAISS embeddings = OpenAIEmbeddings ( ) few_shot_docs = [ Document ( page_content = question , metadata = { \"sql_query\" : few_shots [ question ] } ) for question in few_shots . keys ( ) ] vector_db = FAISS . from_documents ( few_shot_docs , embeddings ) retriever = vector_db . as_retriever ( ) Now we can create our own custom tool and append it as a new tool in the create_sql_agent function: from langchain . agents . agent_toolkits import create_retriever_tool tool_description = \"\"\" This tool will help you understand similar examples to adapt them to the user question. Input to this tool should be the user question. \"\"\" retriever_tool = create_retriever_tool ( retriever , name = \"sql_get_similar_examples\" , description = tool_description ) custom_tool_list = [ retriever_tool ] Now we can create the agent, adjusting the standard SQL Agent suffix to\nconsider our use case. Although the most straightforward way to handle\nthis would be to include it just in the tool description, this is often\nnot enough and we need to specify it in the agent prompt using the suffix argument in the constructor. from langchain . agents import AgentType , create_sql_agent from langchain . agents . agent_toolkits import SQLDatabaseToolkit from langchain . chat_models import ChatOpenAI from langchain . utilities import SQLDatabase db = SQLDatabase . from_uri ( \"sqlite:///Chinook.db\" ) llm = ChatOpenAI ( model_name = \"gpt-4\" , temperature = 0 ) toolkit = SQLDatabaseToolkit ( db = db , llm = llm ) custom_suffix = \"\"\" I should first get the similar examples I know. If the examples are enough to construct the query, I can build it. Otherwise, I can then look at the tables in the database to see what I can query. Then I should query the schema of the most relevant tables \"\"\" agent = create_sql_agent ( llm = llm , toolkit = toolkit , verbose = True , agent_type = AgentType . OPENAI_FUNCTIONS , extra_tools = custom_tool_list , suffix = custom_suffix , ) Let’s try it out: agent . run ( \"How many employees do we have?\" ) > Entering new AgentExecutor chain... Invoking: `sql_get_similar_examples` with `How many employees do we have?` [Document(page_content='How many employees are there', metadata={'sql_query': 'SELECT COUNT(*) FROM \"employee\"'}), Document(page_content='Find the total number of invoices.', metadata={'sql_query': 'SELECT COUNT(*) FROM invoices;'})] Invoking: `sql_db_query_checker` with `SELECT COUNT(*) FROM employee` responded: {content} SELECT COUNT(*) FROM employee Invoking: `sql_db_query` with `SELECT COUNT(*) FROM employee` [(8,)]We have 8 employees. > Finished chain. 'We have 8 employees.' As we can see, the agent first used the sql_get_similar_examples tool\nin order to retrieve similar examples. As the question was very similar\nto other few shot examples, the agent didn’t need to use any other\ntool from the standard Toolkit, thus saving time and tokens . Finding and correcting misspellings for proper nouns ​ In order to filter columns that contain proper nouns such as addresses,\nsong names or artists, we first need to double-check the spelling in\norder to filter the data correctly. We can achieve this by creating a vector store using all the distinct\nproper nouns that exist in the database. We can then have the agent\nquery that vector store each time the user includes a proper noun in\ntheir question, to find the correct spelling for that word. In this way,\nthe agent can make sure it understands which entity the user is\nreferring to before building the target query. Let’s follow a similar approach to the few shots, but without metadata:\njust embedding the proper nouns and then querying to get the most\nsimilar one to the misspelled user question. First we need the unique values for each entity we want, for which we\ndefine a function that parses the result into a list of elements: import ast import re def run_query_save_results ( db , query ) : res = db . run ( query ) res = [ el for sub in ast . literal_eval ( res ) for el in sub if el ] res = [ re . sub ( r\"\\b\\d+\\b\" , \"\" , string ) . strip ( ) for string in res ] return res artists = run_query_save_results ( db , \"SELECT Name FROM Artist\" ) albums = run_query_save_results ( db , \"SELECT Title FROM Album\" ) Now we can proceed with creating the custom retriever tool and the\nfinal agent: from langchain . agents . agent_toolkits import create_retriever_tool from langchain . embeddings . openai import OpenAIEmbeddings from langchain . vectorstores import FAISS texts = artists + albums embeddings = OpenAIEmbeddings ( ) vector_db = FAISS . from_texts ( texts , embeddings ) retriever = vector_db . as_retriever ( ) retriever_tool = create_retriever_tool ( retriever , name = \"name_search\" , description = \"use to learn how a piece of data is actually written, can be from names, surnames addresses etc\" , ) custom_tool_list = [ retriever_tool ] from langchain . agents import AgentType , create_sql_agent from langchain . agents . agent_toolkits import SQLDatabaseToolkit from langchain . chat_models import ChatOpenAI from langchain . utilities import SQLDatabase # db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\") llm = ChatOpenAI ( model_name = \"gpt-4\" , temperature = 0 ) toolkit = SQLDatabaseToolkit ( db = db , llm = llm ) custom_suffix = \"\"\" If a user asks for me to filter based on proper nouns, I should first check the spelling using the name_search tool. Otherwise, I can then look at the tables in the database to see what I can query. Then I should query the schema of the most relevant tables \"\"\" agent = create_sql_agent ( llm = llm , toolkit = toolkit , verbose = True , agent_type = AgentType . OPENAI_FUNCTIONS , extra_tools = custom_tool_list , suffix = custom_suffix , ) Let’s try it out: agent . run ( \"How many albums does alis in pains have?\" ) > Entering new AgentExecutor chain... Invoking: `name_search` with `alis in pains` [Document(page_content='House of Pain', metadata={}), Document(page_content='Alice In Chains', metadata={}), Document(page_content='Aisha Duo', metadata={}), Document(page_content='House Of Pain', metadata={})] Invoking: `sql_db_list_tables` with `` responded: {content} Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Invoking: `sql_db_schema` with `Album, Artist` responded: {content} CREATE TABLE \"Album\" ( \"AlbumId\" INTEGER NOT NULL, \"Title\" NVARCHAR(160) NOT NULL, \"ArtistId\" INTEGER NOT NULL, PRIMARY KEY (\"AlbumId\"), FOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\") ) /* 3 rows from Album table: AlbumId Title   ArtistId 1   For Those About To Rock We Salute You   1 2   Balls to the Wall   2 3   Restless and Wild   2 */ CREATE TABLE \"Artist\" ( \"ArtistId\" INTEGER NOT NULL, \"Name\" NVARCHAR(120), PRIMARY KEY (\"ArtistId\") ) /* 3 rows from Artist table: ArtistId    Name 1   AC/DC 2   Accept 3   Aerosmith */ Invoking: `sql_db_query_checker` with `SELECT COUNT(*) FROM Album JOIN Artist ON Album.ArtistId = Artist.ArtistId WHERE Artist.Name = 'Alice In Chains'` responded: {content} SELECT COUNT(*) FROM Album JOIN Artist ON Album.ArtistId = Artist.ArtistId WHERE Artist.Name = 'Alice In Chains' Invoking: `sql_db_query` with `SELECT COUNT(*) FROM Album JOIN Artist ON Album.ArtistId = Artist.ArtistId WHERE Artist.Name = 'Alice In Chains'` [(1,)]Alice In Chains has 1 album in the database. > Finished chain. 'Alice In Chains has 1 album in the database.' As we can see, the agent used the name_search tool in order to check\nhow to correctly query the database for this specific artist. Go deeper ​ To learn more about the SQL Agent and how it works we refer to the SQL\nAgent Toolkit documentation. You can also check Agents for other document types: - Pandas\nAgent - CSV\nAgent Elastic Search ​ Going beyond the above use-case, there are integrations with other\ndatabases. For example, we can interact with Elasticsearch analytics database. This chain builds search queries via the Elasticsearch DSL API (filters\nand aggregations). The Elasticsearch client must have permissions for index listing,\nmapping description and search queries. See here for instructions on how to run Elasticsearch locally. Make sure to install the Elasticsearch Python client before: pip install elasticsearch from elasticsearch import Elasticsearch from langchain . chains . elasticsearch_database import ElasticsearchDatabaseChain from langchain . chat_models import ChatOpenAI # Initialize Elasticsearch python client. # See https://elasticsearch-py.readthedocs.io/en/v8.8.2/api.html#elasticsearch.Elasticsearch ELASTIC_SEARCH_SERVER = \"https://elastic:pass@localhost:9200\" db = Elasticsearch ( ELASTIC_SEARCH_SERVER ) Uncomment the next cell to initially populate your db. # customers = [ #     {\"firstname\": \"Jennifer\", \"lastname\": \"Walters\"}, #     {\"firstname\": \"Monica\",\"lastname\":\"Rambeau\"}, #     {\"firstname\": \"Carol\",\"lastname\":\"Danvers\"}, #     {\"firstname\": \"Wanda\",\"lastname\":\"Maximoff\"}, #     {\"firstname\": \"Jennifer\",\"lastname\":\"Takeda\"}, # ] # for i, customer in enumerate(customers): #     db.create(index=\"customers\", document=customer, id=i) llm = ChatOpenAI ( model_name = \"gpt-4\" , temperature = 0 ) chain = ElasticsearchDatabaseChain . from_llm ( llm = llm , database = db , verbose = True ) question = \"What are the first names of all the customers?\" chain . run ( question ) We can customize the prompt. from langchain . prompts . prompt import PromptTemplate PROMPT_TEMPLATE = \"\"\"Given an input question, create a syntactically correct Elasticsearch query to run. Unless the user specifies in their question a specific number of examples they wish to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database. Unless told to do not query for all the columns from a specific index, only ask for a the few relevant columns given the question. Pay attention to use only the column names that you can see in the mapping description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which index. Return the query as valid json. Use the following format: Question: Question here ESQuery: Elasticsearch Query formatted as json \"\"\" PROMPT = PromptTemplate . from_template ( PROMPT_TEMPLATE , ) chain = ElasticsearchDatabaseChain . from_llm ( llm = llm , database = db , query_prompt = PROMPT ) Next Retrieval-augmented generation (RAG)"
    },
    {
        "title": "Providers | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/providers",
        "text_content": ""
    },
    {
        "title": "Debugging | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/guides/debugging",
        "text_content": "Debugging On this page Debugging If you're building with LLMs, at some point something will break, and you'll need to debug. A model call will fail, or the model output will be misformatted, or there will be some nested model calls and it won't be clear where along the way an incorrect output was created. Here are a few different tools and functionalities to aid in debugging. Tracing ​ Platforms with tracing capabilities like LangSmith and WandB are the most comprehensive solutions for debugging. These platforms make it easy to not only log and visualize LLM apps, but also to actively debug, test and refine them. For anyone building production-grade LLM applications, we highly recommend using a platform like this. set_debug and set_verbose ​ If you're prototyping in Jupyter Notebooks or running Python scripts, it can be helpful to print out the intermediate steps of a Chain run. There are a number of ways to enable printing at varying degrees of verbosity. Let's suppose we have a simple agent, and want to visualize the actions it takes and tool outputs it receives. Without any debugging, here's what we see: from langchain . agents import AgentType , initialize_agent , load_tools from langchain . chat_models import ChatOpenAI llm = ChatOpenAI ( model_name = \"gpt-4\" , temperature = 0 ) tools = load_tools ( [ \"ddg-search\" , \"llm-math\" ] , llm = llm ) agent = initialize_agent ( tools , llm , agent = AgentType . ZERO_SHOT_REACT_DESCRIPTION ) agent . run ( \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\" ) 'The director of the 2023 film Oppenheimer is Christopher Nolan and he is approximately 19345 days old in 2023.' set_debug(True) ​ Setting the global debug flag will cause all LangChain components with callback support (chains, models, agents, tools, retrievers) to print the inputs they receive and outputs they generate. This is the most verbose setting and will fully log raw inputs and outputs. from langchain . globals import set_debug set_debug ( True ) agent . run ( \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\" ) Console output [chain/start] [1:RunTypeEnum.chain:AgentExecutor] Entering Chain run with input: { \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\" } [chain/start] [1:RunTypeEnum.chain:AgentExecutor > 2:RunTypeEnum.chain:LLMChain] Entering Chain run with input: { \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\", \"agent_scratchpad\": \"\", \"stop\": [ \"\\nObservation:\", \"\\n\\tObservation:\" ] } [llm/start] [1:RunTypeEnum.chain:AgentExecutor > 2:RunTypeEnum.chain:LLMChain > 3:RunTypeEnum.llm:ChatOpenAI] Entering LLM run with input: { \"prompts\": [ \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nduckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator: Useful for when you need to answer questions about math.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [duckduckgo_search, Calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\\nThought:\" ] } [llm/end] [1:RunTypeEnum.chain:AgentExecutor > 2:RunTypeEnum.chain:LLMChain > 3:RunTypeEnum.llm:ChatOpenAI] [5.53s] Exiting LLM run with output: { \"generations\": [ [ { \"text\": \"I need to find out who directed the 2023 film Oppenheimer and their age. Then, I need to calculate their age in days. I will use DuckDuckGo to find out the director and their age.\\nAction: duckduckgo_search\\nAction Input: \\\"Director of the 2023 film Oppenheimer and their age\\\"\", \"generation_info\": { \"finish_reason\": \"stop\" }, \"message\": { \"lc\": 1, \"type\": \"constructor\", \"id\": [ \"langchain\", \"schema\", \"messages\", \"AIMessage\" ], \"kwargs\": { \"content\": \"I need to find out who directed the 2023 film Oppenheimer and their age. Then, I need to calculate their age in days. I will use DuckDuckGo to find out the director and their age.\\nAction: duckduckgo_search\\nAction Input: \\\"Director of the 2023 film Oppenheimer and their age\\\"\", \"additional_kwargs\": {} } } } ] ], \"llm_output\": { \"token_usage\": { \"prompt_tokens\": 206, \"completion_tokens\": 71, \"total_tokens\": 277 }, \"model_name\": \"gpt-4\" }, \"run\": null } [chain/end] [1:RunTypeEnum.chain:AgentExecutor > 2:RunTypeEnum.chain:LLMChain] [5.53s] Exiting Chain run with output: { \"text\": \"I need to find out who directed the 2023 film Oppenheimer and their age. Then, I need to calculate their age in days. I will use DuckDuckGo to find out the director and their age.\\nAction: duckduckgo_search\\nAction Input: \\\"Director of the 2023 film Oppenheimer and their age\\\"\" } [tool/start] [1:RunTypeEnum.chain:AgentExecutor > 4:RunTypeEnum.tool:duckduckgo_search] Entering Tool run with input: \"Director of the 2023 film Oppenheimer and their age\" [tool/end] [1:RunTypeEnum.chain:AgentExecutor > 4:RunTypeEnum.tool:duckduckgo_search] [1.51s] Exiting Tool run with output: \"Capturing the mad scramble to build the first atomic bomb required rapid-fire filming, strict set rules and the construction of an entire 1940s western town. By Jada Yuan. July 19, 2023 at 5:00 a ... In Christopher Nolan's new film, \"Oppenheimer,\" Cillian Murphy stars as J. Robert Oppenheimer, the American physicist who oversaw the Manhattan Project in Los Alamos, N.M. Universal Pictures... Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. Christopher Nolan goes deep on 'Oppenheimer,' his most 'extreme' film to date. By Kenneth Turan. July 11, 2023 5 AM PT. For Subscribers. Christopher Nolan is photographed in Los Angeles ... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age.\" [chain/start] [1:RunTypeEnum.chain:AgentExecutor > 5:RunTypeEnum.chain:LLMChain] Entering Chain run with input: { \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\", \"agent_scratchpad\": \"I need to find out who directed the 2023 film Oppenheimer and their age. Then, I need to calculate their age in days. I will use DuckDuckGo to find out the director and their age.\\nAction: duckduckgo_search\\nAction Input: \\\"Director of the 2023 film Oppenheimer and their age\\\"\\nObservation: Capturing the mad scramble to build the first atomic bomb required rapid-fire filming, strict set rules and the construction of an entire 1940s western town. By Jada Yuan. July 19, 2023 at 5:00 a ... In Christopher Nolan's new film, \\\"Oppenheimer,\\\" Cillian Murphy stars as J. Robert Oppenheimer, the American physicist who oversaw the Manhattan Project in Los Alamos, N.M. Universal Pictures... Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. Christopher Nolan goes deep on 'Oppenheimer,' his most 'extreme' film to date. By Kenneth Turan. July 11, 2023 5 AM PT. For Subscribers. Christopher Nolan is photographed in Los Angeles ... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age.\\nThought:\", \"stop\": [ \"\\nObservation:\", \"\\n\\tObservation:\" ] } [llm/start] [1:RunTypeEnum.chain:AgentExecutor > 5:RunTypeEnum.chain:LLMChain > 6:RunTypeEnum.llm:ChatOpenAI] Entering LLM run with input: { \"prompts\": [ \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nduckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator: Useful for when you need to answer questions about math.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [duckduckgo_search, Calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\\nThought:I need to find out who directed the 2023 film Oppenheimer and their age. Then, I need to calculate their age in days. I will use DuckDuckGo to find out the director and their age.\\nAction: duckduckgo_search\\nAction Input: \\\"Director of the 2023 film Oppenheimer and their age\\\"\\nObservation: Capturing the mad scramble to build the first atomic bomb required rapid-fire filming, strict set rules and the construction of an entire 1940s western town. By Jada Yuan. July 19, 2023 at 5:00 a ... In Christopher Nolan's new film, \\\"Oppenheimer,\\\" Cillian Murphy stars as J. Robert Oppenheimer, the American physicist who oversaw the Manhattan Project in Los Alamos, N.M. Universal Pictures... Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. Christopher Nolan goes deep on 'Oppenheimer,' his most 'extreme' film to date. By Kenneth Turan. July 11, 2023 5 AM PT. For Subscribers. Christopher Nolan is photographed in Los Angeles ... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age.\\nThought:\" ] } [llm/end] [1:RunTypeEnum.chain:AgentExecutor > 5:RunTypeEnum.chain:LLMChain > 6:RunTypeEnum.llm:ChatOpenAI] [4.46s] Exiting LLM run with output: { \"generations\": [ [ { \"text\": \"The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his age.\\nAction: duckduckgo_search\\nAction Input: \\\"Christopher Nolan age\\\"\", \"generation_info\": { \"finish_reason\": \"stop\" }, \"message\": { \"lc\": 1, \"type\": \"constructor\", \"id\": [ \"langchain\", \"schema\", \"messages\", \"AIMessage\" ], \"kwargs\": { \"content\": \"The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his age.\\nAction: duckduckgo_search\\nAction Input: \\\"Christopher Nolan age\\\"\", \"additional_kwargs\": {} } } } ] ], \"llm_output\": { \"token_usage\": { \"prompt_tokens\": 550, \"completion_tokens\": 39, \"total_tokens\": 589 }, \"model_name\": \"gpt-4\" }, \"run\": null } [chain/end] [1:RunTypeEnum.chain:AgentExecutor > 5:RunTypeEnum.chain:LLMChain] [4.46s] Exiting Chain run with output: { \"text\": \"The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his age.\\nAction: duckduckgo_search\\nAction Input: \\\"Christopher Nolan age\\\"\" } [tool/start] [1:RunTypeEnum.chain:AgentExecutor > 7:RunTypeEnum.tool:duckduckgo_search] Entering Tool run with input: \"Christopher Nolan age\" [tool/end] [1:RunTypeEnum.chain:AgentExecutor > 7:RunTypeEnum.tool:duckduckgo_search] [1.33s] Exiting Tool run with output: \"Christopher Edward Nolan CBE (born 30 July 1970) is a British and American filmmaker. Known for his Hollywood blockbusters with complex storytelling, Nolan is considered a leading filmmaker of the 21st century. His films have grossed $5 billion worldwide. The recipient of many accolades, he has been nominated for five Academy Awards, five BAFTA Awards and six Golden Globe Awards. July 30, 1970 (age 52) London England Notable Works: \"Dunkirk\" \"Tenet\" \"The Prestige\" See all related content → Recent News Jul. 13, 2023, 11:11 AM ET (AP) Cillian Murphy, playing Oppenheimer, finally gets to lead a Christopher Nolan film July 11, 2023 5 AM PT For Subscribers Christopher Nolan is photographed in Los Angeles. (Joe Pugliese / For The Times) This is not the story I was supposed to write. Oppenheimer director Christopher Nolan, Cillian Murphy, Emily Blunt and Matt Damon on the stakes of making a three-hour, CGI-free summer film. Christopher Nolan, the director behind such films as \"Dunkirk,\" \"Inception,\" \"Interstellar,\" and the \"Dark Knight\" trilogy, has spent the last three years living in Oppenheimer's world, writing ...\" [chain/start] [1:RunTypeEnum.chain:AgentExecutor > 8:RunTypeEnum.chain:LLMChain] Entering Chain run with input: { \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\", \"agent_scratchpad\": \"I need to find out who directed the 2023 film Oppenheimer and their age. Then, I need to calculate their age in days. I will use DuckDuckGo to find out the director and their age.\\nAction: duckduckgo_search\\nAction Input: \\\"Director of the 2023 film Oppenheimer and their age\\\"\\nObservation: Capturing the mad scramble to build the first atomic bomb required rapid-fire filming, strict set rules and the construction of an entire 1940s western town. By Jada Yuan. July 19, 2023 at 5:00 a ... In Christopher Nolan's new film, \\\"Oppenheimer,\\\" Cillian Murphy stars as J. Robert Oppenheimer, the American physicist who oversaw the Manhattan Project in Los Alamos, N.M. Universal Pictures... Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. Christopher Nolan goes deep on 'Oppenheimer,' his most 'extreme' film to date. By Kenneth Turan. July 11, 2023 5 AM PT. For Subscribers. Christopher Nolan is photographed in Los Angeles ... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age.\\nThought:The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his age.\\nAction: duckduckgo_search\\nAction Input: \\\"Christopher Nolan age\\\"\\nObservation: Christopher Edward Nolan CBE (born 30 July 1970) is a British and American filmmaker. Known for his Hollywood blockbusters with complex storytelling, Nolan is considered a leading filmmaker of the 21st century. His films have grossed $5 billion worldwide. The recipient of many accolades, he has been nominated for five Academy Awards, five BAFTA Awards and six Golden Globe Awards. July 30, 1970 (age 52) London England Notable Works: \\\"Dunkirk\\\" \\\"Tenet\\\" \\\"The Prestige\\\" See all related content → Recent News Jul. 13, 2023, 11:11 AM ET (AP) Cillian Murphy, playing Oppenheimer, finally gets to lead a Christopher Nolan film July 11, 2023 5 AM PT For Subscribers Christopher Nolan is photographed in Los Angeles. (Joe Pugliese / For The Times) This is not the story I was supposed to write. Oppenheimer director Christopher Nolan, Cillian Murphy, Emily Blunt and Matt Damon on the stakes of making a three-hour, CGI-free summer film. Christopher Nolan, the director behind such films as \\\"Dunkirk,\\\" \\\"Inception,\\\" \\\"Interstellar,\\\" and the \\\"Dark Knight\\\" trilogy, has spent the last three years living in Oppenheimer's world, writing ...\\nThought:\", \"stop\": [ \"\\nObservation:\", \"\\n\\tObservation:\" ] } [llm/start] [1:RunTypeEnum.chain:AgentExecutor > 8:RunTypeEnum.chain:LLMChain > 9:RunTypeEnum.llm:ChatOpenAI] Entering LLM run with input: { \"prompts\": [ \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nduckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator: Useful for when you need to answer questions about math.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [duckduckgo_search, Calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\\nThought:I need to find out who directed the 2023 film Oppenheimer and their age. Then, I need to calculate their age in days. I will use DuckDuckGo to find out the director and their age.\\nAction: duckduckgo_search\\nAction Input: \\\"Director of the 2023 film Oppenheimer and their age\\\"\\nObservation: Capturing the mad scramble to build the first atomic bomb required rapid-fire filming, strict set rules and the construction of an entire 1940s western town. By Jada Yuan. July 19, 2023 at 5:00 a ... In Christopher Nolan's new film, \\\"Oppenheimer,\\\" Cillian Murphy stars as J. Robert Oppenheimer, the American physicist who oversaw the Manhattan Project in Los Alamos, N.M. Universal Pictures... Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. Christopher Nolan goes deep on 'Oppenheimer,' his most 'extreme' film to date. By Kenneth Turan. July 11, 2023 5 AM PT. For Subscribers. Christopher Nolan is photographed in Los Angeles ... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age.\\nThought:The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his age.\\nAction: duckduckgo_search\\nAction Input: \\\"Christopher Nolan age\\\"\\nObservation: Christopher Edward Nolan CBE (born 30 July 1970) is a British and American filmmaker. Known for his Hollywood blockbusters with complex storytelling, Nolan is considered a leading filmmaker of the 21st century. His films have grossed $5 billion worldwide. The recipient of many accolades, he has been nominated for five Academy Awards, five BAFTA Awards and six Golden Globe Awards. July 30, 1970 (age 52) London England Notable Works: \\\"Dunkirk\\\" \\\"Tenet\\\" \\\"The Prestige\\\" See all related content → Recent News Jul. 13, 2023, 11:11 AM ET (AP) Cillian Murphy, playing Oppenheimer, finally gets to lead a Christopher Nolan film July 11, 2023 5 AM PT For Subscribers Christopher Nolan is photographed in Los Angeles. (Joe Pugliese / For The Times) This is not the story I was supposed to write. Oppenheimer director Christopher Nolan, Cillian Murphy, Emily Blunt and Matt Damon on the stakes of making a three-hour, CGI-free summer film. Christopher Nolan, the director behind such films as \\\"Dunkirk,\\\" \\\"Inception,\\\" \\\"Interstellar,\\\" and the \\\"Dark Knight\\\" trilogy, has spent the last three years living in Oppenheimer's world, writing ...\\nThought:\" ] } [llm/end] [1:RunTypeEnum.chain:AgentExecutor > 8:RunTypeEnum.chain:LLMChain > 9:RunTypeEnum.llm:ChatOpenAI] [2.69s] Exiting LLM run with output: { \"generations\": [ [ { \"text\": \"Christopher Nolan was born on July 30, 1970, which makes him 52 years old in 2023. Now I need to calculate his age in days.\\nAction: Calculator\\nAction Input: 52*365\", \"generation_info\": { \"finish_reason\": \"stop\" }, \"message\": { \"lc\": 1, \"type\": \"constructor\", \"id\": [ \"langchain\", \"schema\", \"messages\", \"AIMessage\" ], \"kwargs\": { \"content\": \"Christopher Nolan was born on July 30, 1970, which makes him 52 years old in 2023. Now I need to calculate his age in days.\\nAction: Calculator\\nAction Input: 52*365\", \"additional_kwargs\": {} } } } ] ], \"llm_output\": { \"token_usage\": { \"prompt_tokens\": 868, \"completion_tokens\": 46, \"total_tokens\": 914 }, \"model_name\": \"gpt-4\" }, \"run\": null } [chain/end] [1:RunTypeEnum.chain:AgentExecutor > 8:RunTypeEnum.chain:LLMChain] [2.69s] Exiting Chain run with output: { \"text\": \"Christopher Nolan was born on July 30, 1970, which makes him 52 years old in 2023. Now I need to calculate his age in days.\\nAction: Calculator\\nAction Input: 52*365\" } [tool/start] [1:RunTypeEnum.chain:AgentExecutor > 10:RunTypeEnum.tool:Calculator] Entering Tool run with input: \"52*365\" [chain/start] [1:RunTypeEnum.chain:AgentExecutor > 10:RunTypeEnum.tool:Calculator > 11:RunTypeEnum.chain:LLMMathChain] Entering Chain run with input: { \"question\": \"52*365\" } [chain/start] [1:RunTypeEnum.chain:AgentExecutor > 10:RunTypeEnum.tool:Calculator > 11:RunTypeEnum.chain:LLMMathChain > 12:RunTypeEnum.chain:LLMChain] Entering Chain run with input: { \"question\": \"52*365\", \"stop\": [ \"```output\" ] } [llm/start] [1:RunTypeEnum.chain:AgentExecutor > 10:RunTypeEnum.tool:Calculator > 11:RunTypeEnum.chain:LLMMathChain > 12:RunTypeEnum.chain:LLMChain > 13:RunTypeEnum.llm:ChatOpenAI] Entering LLM run with input: { \"prompts\": [ \"Human: Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${Question with math problem.}\\n```text\\n${single line mathematical expression that solves the problem}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${Output of running the code}\\n```\\nAnswer: ${Answer}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\\\"37593 * 67\\\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\\\"37593**(1/5)\\\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: 52*365\" ] } [llm/end] [1:RunTypeEnum.chain:AgentExecutor > 10:RunTypeEnum.tool:Calculator > 11:RunTypeEnum.chain:LLMMathChain > 12:RunTypeEnum.chain:LLMChain > 13:RunTypeEnum.llm:ChatOpenAI] [2.89s] Exiting LLM run with output: { \"generations\": [ [ { \"text\": \"```text\\n52*365\\n```\\n...numexpr.evaluate(\\\"52*365\\\")...\\n\", \"generation_info\": { \"finish_reason\": \"stop\" }, \"message\": { \"lc\": 1, \"type\": \"constructor\", \"id\": [ \"langchain\", \"schema\", \"messages\", \"AIMessage\" ], \"kwargs\": { \"content\": \"```text\\n52*365\\n```\\n...numexpr.evaluate(\\\"52*365\\\")...\\n\", \"additional_kwargs\": {} } } } ] ], \"llm_output\": { \"token_usage\": { \"prompt_tokens\": 203, \"completion_tokens\": 19, \"total_tokens\": 222 }, \"model_name\": \"gpt-4\" }, \"run\": null } [chain/end] [1:RunTypeEnum.chain:AgentExecutor > 10:RunTypeEnum.tool:Calculator > 11:RunTypeEnum.chain:LLMMathChain > 12:RunTypeEnum.chain:LLMChain] [2.89s] Exiting Chain run with output: { \"text\": \"```text\\n52*365\\n```\\n...numexpr.evaluate(\\\"52*365\\\")...\\n\" } [chain/end] [1:RunTypeEnum.chain:AgentExecutor > 10:RunTypeEnum.tool:Calculator > 11:RunTypeEnum.chain:LLMMathChain] [2.90s] Exiting Chain run with output: { \"answer\": \"Answer: 18980\" } [tool/end] [1:RunTypeEnum.chain:AgentExecutor > 10:RunTypeEnum.tool:Calculator] [2.90s] Exiting Tool run with output: \"Answer: 18980\" [chain/start] [1:RunTypeEnum.chain:AgentExecutor > 14:RunTypeEnum.chain:LLMChain] Entering Chain run with input: { \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\", \"agent_scratchpad\": \"I need to find out who directed the 2023 film Oppenheimer and their age. Then, I need to calculate their age in days. I will use DuckDuckGo to find out the director and their age.\\nAction: duckduckgo_search\\nAction Input: \\\"Director of the 2023 film Oppenheimer and their age\\\"\\nObservation: Capturing the mad scramble to build the first atomic bomb required rapid-fire filming, strict set rules and the construction of an entire 1940s western town. By Jada Yuan. July 19, 2023 at 5:00 a ... In Christopher Nolan's new film, \\\"Oppenheimer,\\\" Cillian Murphy stars as J. Robert Oppenheimer, the American physicist who oversaw the Manhattan Project in Los Alamos, N.M. Universal Pictures... Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. Christopher Nolan goes deep on 'Oppenheimer,' his most 'extreme' film to date. By Kenneth Turan. July 11, 2023 5 AM PT. For Subscribers. Christopher Nolan is photographed in Los Angeles ... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age.\\nThought:The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his age.\\nAction: duckduckgo_search\\nAction Input: \\\"Christopher Nolan age\\\"\\nObservation: Christopher Edward Nolan CBE (born 30 July 1970) is a British and American filmmaker. Known for his Hollywood blockbusters with complex storytelling, Nolan is considered a leading filmmaker of the 21st century. His films have grossed $5 billion worldwide. The recipient of many accolades, he has been nominated for five Academy Awards, five BAFTA Awards and six Golden Globe Awards. July 30, 1970 (age 52) London England Notable Works: \\\"Dunkirk\\\" \\\"Tenet\\\" \\\"The Prestige\\\" See all related content → Recent News Jul. 13, 2023, 11:11 AM ET (AP) Cillian Murphy, playing Oppenheimer, finally gets to lead a Christopher Nolan film July 11, 2023 5 AM PT For Subscribers Christopher Nolan is photographed in Los Angeles. (Joe Pugliese / For The Times) This is not the story I was supposed to write. Oppenheimer director Christopher Nolan, Cillian Murphy, Emily Blunt and Matt Damon on the stakes of making a three-hour, CGI-free summer film. Christopher Nolan, the director behind such films as \\\"Dunkirk,\\\" \\\"Inception,\\\" \\\"Interstellar,\\\" and the \\\"Dark Knight\\\" trilogy, has spent the last three years living in Oppenheimer's world, writing ...\\nThought:Christopher Nolan was born on July 30, 1970, which makes him 52 years old in 2023. Now I need to calculate his age in days.\\nAction: Calculator\\nAction Input: 52*365\\nObservation: Answer: 18980\\nThought:\", \"stop\": [ \"\\nObservation:\", \"\\n\\tObservation:\" ] } [llm/start] [1:RunTypeEnum.chain:AgentExecutor > 14:RunTypeEnum.chain:LLMChain > 15:RunTypeEnum.llm:ChatOpenAI] Entering LLM run with input: { \"prompts\": [ \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nduckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator: Useful for when you need to answer questions about math.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [duckduckgo_search, Calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\\nThought:I need to find out who directed the 2023 film Oppenheimer and their age. Then, I need to calculate their age in days. I will use DuckDuckGo to find out the director and their age.\\nAction: duckduckgo_search\\nAction Input: \\\"Director of the 2023 film Oppenheimer and their age\\\"\\nObservation: Capturing the mad scramble to build the first atomic bomb required rapid-fire filming, strict set rules and the construction of an entire 1940s western town. By Jada Yuan. July 19, 2023 at 5:00 a ... In Christopher Nolan's new film, \\\"Oppenheimer,\\\" Cillian Murphy stars as J. Robert Oppenheimer, the American physicist who oversaw the Manhattan Project in Los Alamos, N.M. Universal Pictures... Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. Christopher Nolan goes deep on 'Oppenheimer,' his most 'extreme' film to date. By Kenneth Turan. July 11, 2023 5 AM PT. For Subscribers. Christopher Nolan is photographed in Los Angeles ... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age.\\nThought:The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his age.\\nAction: duckduckgo_search\\nAction Input: \\\"Christopher Nolan age\\\"\\nObservation: Christopher Edward Nolan CBE (born 30 July 1970) is a British and American filmmaker. Known for his Hollywood blockbusters with complex storytelling, Nolan is considered a leading filmmaker of the 21st century. His films have grossed $5 billion worldwide. The recipient of many accolades, he has been nominated for five Academy Awards, five BAFTA Awards and six Golden Globe Awards. July 30, 1970 (age 52) London England Notable Works: \\\"Dunkirk\\\" \\\"Tenet\\\" \\\"The Prestige\\\" See all related content → Recent News Jul. 13, 2023, 11:11 AM ET (AP) Cillian Murphy, playing Oppenheimer, finally gets to lead a Christopher Nolan film July 11, 2023 5 AM PT For Subscribers Christopher Nolan is photographed in Los Angeles. (Joe Pugliese / For The Times) This is not the story I was supposed to write. Oppenheimer director Christopher Nolan, Cillian Murphy, Emily Blunt and Matt Damon on the stakes of making a three-hour, CGI-free summer film. Christopher Nolan, the director behind such films as \\\"Dunkirk,\\\" \\\"Inception,\\\" \\\"Interstellar,\\\" and the \\\"Dark Knight\\\" trilogy, has spent the last three years living in Oppenheimer's world, writing ...\\nThought:Christopher Nolan was born on July 30, 1970, which makes him 52 years old in 2023. Now I need to calculate his age in days.\\nAction: Calculator\\nAction Input: 52*365\\nObservation: Answer: 18980\\nThought:\" ] } [llm/end] [1:RunTypeEnum.chain:AgentExecutor > 14:RunTypeEnum.chain:LLMChain > 15:RunTypeEnum.llm:ChatOpenAI] [3.52s] Exiting LLM run with output: { \"generations\": [ [ { \"text\": \"I now know the final answer\\nFinal Answer: The director of the 2023 film Oppenheimer is Christopher Nolan and he is 52 years old. His age in days is approximately 18980 days.\", \"generation_info\": { \"finish_reason\": \"stop\" }, \"message\": { \"lc\": 1, \"type\": \"constructor\", \"id\": [ \"langchain\", \"schema\", \"messages\", \"AIMessage\" ], \"kwargs\": { \"content\": \"I now know the final answer\\nFinal Answer: The director of the 2023 film Oppenheimer is Christopher Nolan and he is 52 years old. His age in days is approximately 18980 days.\", \"additional_kwargs\": {} } } } ] ], \"llm_output\": { \"token_usage\": { \"prompt_tokens\": 926, \"completion_tokens\": 43, \"total_tokens\": 969 }, \"model_name\": \"gpt-4\" }, \"run\": null } [chain/end] [1:RunTypeEnum.chain:AgentExecutor > 14:RunTypeEnum.chain:LLMChain] [3.52s] Exiting Chain run with output: { \"text\": \"I now know the final answer\\nFinal Answer: The director of the 2023 film Oppenheimer is Christopher Nolan and he is 52 years old. His age in days is approximately 18980 days.\" } [chain/end] [1:RunTypeEnum.chain:AgentExecutor] [21.96s] Exiting Chain run with output: { \"output\": \"The director of the 2023 film Oppenheimer is Christopher Nolan and he is 52 years old. His age in days is approximately 18980 days.\" } 'The director of the 2023 film Oppenheimer is Christopher Nolan and he is 52 years old. His age in days is approximately 18980 days.' set_verbose(True) ​ Setting the verbose flag will print out inputs and outputs in a slightly more readable format and will skip logging certain raw outputs (like the token usage stats for an LLM call) so that you can focus on application logic. from langchain . globals import set_verbose set_verbose ( True ) agent . run ( \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\" ) Console output > Entering new AgentExecutor chain... > Entering new LLMChain chain... Prompt after formatting: Answer the following questions as best you can. You have access to the following tools: duckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query. Calculator: Useful for when you need to answer questions about math. Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [duckduckgo_search, Calculator] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! Question: Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)? Thought: > Finished chain. First, I need to find out who directed the film Oppenheimer in 2023 and their birth date to calculate their age. Action: duckduckgo_search Action Input: \"Director of the 2023 film Oppenheimer\" Observation: Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. In Christopher Nolan's new film, \"Oppenheimer,\" Cillian Murphy stars as J. Robert ... 2023, 12:16 p.m. ET. ... including his role as the director of the Manhattan Engineer District, better ... J Robert Oppenheimer was the director of the secret Los Alamos Laboratory. It was established under US president Franklin D Roosevelt as part of the Manhattan Project to build the first atomic bomb. He oversaw the first atomic bomb detonation in the New Mexico desert in July 1945, code-named \"Trinity\". In this opening salvo of 2023's Oscar battle, Nolan has enjoined a star-studded cast for a retelling of the brilliant and haunted life of J. Robert Oppenheimer, the American physicist whose... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age. Thought: > Entering new LLMChain chain... Prompt after formatting: Answer the following questions as best you can. You have access to the following tools: duckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query. Calculator: Useful for when you need to answer questions about math. Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [duckduckgo_search, Calculator] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! Question: Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)? Thought:First, I need to find out who directed the film Oppenheimer in 2023 and their birth date to calculate their age. Action: duckduckgo_search Action Input: \"Director of the 2023 film Oppenheimer\" Observation: Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. In Christopher Nolan's new film, \"Oppenheimer,\" Cillian Murphy stars as J. Robert ... 2023, 12:16 p.m. ET. ... including his role as the director of the Manhattan Engineer District, better ... J Robert Oppenheimer was the director of the secret Los Alamos Laboratory. It was established under US president Franklin D Roosevelt as part of the Manhattan Project to build the first atomic bomb. He oversaw the first atomic bomb detonation in the New Mexico desert in July 1945, code-named \"Trinity\". In this opening salvo of 2023's Oscar battle, Nolan has enjoined a star-studded cast for a retelling of the brilliant and haunted life of J. Robert Oppenheimer, the American physicist whose... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age. Thought: > Finished chain. The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his birth date to calculate his age. Action: duckduckgo_search Action Input: \"Christopher Nolan birth date\" Observation: July 30, 1970 (age 52) London England Notable Works: \"Dunkirk\" \"Tenet\" \"The Prestige\" See all related content → Recent News Jul. 13, 2023, 11:11 AM ET (AP) Cillian Murphy, playing Oppenheimer, finally gets to lead a Christopher Nolan film Christopher Edward Nolan CBE (born 30 July 1970) is a British and American filmmaker. Known for his Hollywood blockbusters with complex storytelling, Nolan is considered a leading filmmaker of the 21st century. His films have grossed $5 billion worldwide. The recipient of many accolades, he has been nominated for five Academy Awards, five BAFTA Awards and six Golden Globe Awards. Christopher Nolan is currently 52 according to his birthdate July 30, 1970 Sun Sign Leo Born Place Westminster, London, England, United Kingdom Residence Los Angeles, California, United States Nationality Education Chris attended Haileybury and Imperial Service College, in Hertford Heath, Hertfordshire. Christopher Nolan's next movie will study the man who developed the atomic bomb, J. Robert Oppenheimer. Here's the release date, plot, trailers & more. July 2023 sees the release of Christopher Nolan's new film, Oppenheimer, his first movie since 2020's Tenet and his split from Warner Bros. Billed as an epic thriller about \"the man who ... Thought: > Entering new LLMChain chain... Prompt after formatting: Answer the following questions as best you can. You have access to the following tools: duckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query. Calculator: Useful for when you need to answer questions about math. Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [duckduckgo_search, Calculator] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! Question: Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)? Thought:First, I need to find out who directed the film Oppenheimer in 2023 and their birth date to calculate their age. Action: duckduckgo_search Action Input: \"Director of the 2023 film Oppenheimer\" Observation: Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. In Christopher Nolan's new film, \"Oppenheimer,\" Cillian Murphy stars as J. Robert ... 2023, 12:16 p.m. ET. ... including his role as the director of the Manhattan Engineer District, better ... J Robert Oppenheimer was the director of the secret Los Alamos Laboratory. It was established under US president Franklin D Roosevelt as part of the Manhattan Project to build the first atomic bomb. He oversaw the first atomic bomb detonation in the New Mexico desert in July 1945, code-named \"Trinity\". In this opening salvo of 2023's Oscar battle, Nolan has enjoined a star-studded cast for a retelling of the brilliant and haunted life of J. Robert Oppenheimer, the American physicist whose... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age. Thought:The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his birth date to calculate his age. Action: duckduckgo_search Action Input: \"Christopher Nolan birth date\" Observation: July 30, 1970 (age 52) London England Notable Works: \"Dunkirk\" \"Tenet\" \"The Prestige\" See all related content → Recent News Jul. 13, 2023, 11:11 AM ET (AP) Cillian Murphy, playing Oppenheimer, finally gets to lead a Christopher Nolan film Christopher Edward Nolan CBE (born 30 July 1970) is a British and American filmmaker. Known for his Hollywood blockbusters with complex storytelling, Nolan is considered a leading filmmaker of the 21st century. His films have grossed $5 billion worldwide. The recipient of many accolades, he has been nominated for five Academy Awards, five BAFTA Awards and six Golden Globe Awards. Christopher Nolan is currently 52 according to his birthdate July 30, 1970 Sun Sign Leo Born Place Westminster, London, England, United Kingdom Residence Los Angeles, California, United States Nationality Education Chris attended Haileybury and Imperial Service College, in Hertford Heath, Hertfordshire. Christopher Nolan's next movie will study the man who developed the atomic bomb, J. Robert Oppenheimer. Here's the release date, plot, trailers & more. July 2023 sees the release of Christopher Nolan's new film, Oppenheimer, his first movie since 2020's Tenet and his split from Warner Bros. Billed as an epic thriller about \"the man who ... Thought: > Finished chain. Christopher Nolan was born on July 30, 1970. Now I need to calculate his age in 2023 and then convert it into days. Action: Calculator Action Input: (2023 - 1970) * 365 > Entering new LLMMathChain chain... (2023 - 1970) * 365 > Entering new LLMChain chain... Prompt after formatting: Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question. Question: ${Question with math problem.} ```text ${single line mathematical expression that solves the problem} ``` ...numexpr.evaluate(text)... ```output ${Output of running the code} ``` Answer: ${Answer} Begin. Question: What is 37593 * 67? ```text 37593 * 67 ``` ...numexpr.evaluate(\"37593 * 67\")... ```output 2518731 ``` Answer: 2518731 Question: 37593^(1/5) ```text 37593**(1/5) ``` ...numexpr.evaluate(\"37593**(1/5)\")... ```output 8.222831614237718 ``` Answer: 8.222831614237718 Question: (2023 - 1970) * 365 > Finished chain. ```text (2023 - 1970) * 365 ``` ...numexpr.evaluate(\"(2023 - 1970) * 365\")... Answer: 19345 > Finished chain. Observation: Answer: 19345 Thought: > Entering new LLMChain chain... Prompt after formatting: Answer the following questions as best you can. You have access to the following tools: duckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query. Calculator: Useful for when you need to answer questions about math. Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [duckduckgo_search, Calculator] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! Question: Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)? Thought:First, I need to find out who directed the film Oppenheimer in 2023 and their birth date to calculate their age. Action: duckduckgo_search Action Input: \"Director of the 2023 film Oppenheimer\" Observation: Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. In Christopher Nolan's new film, \"Oppenheimer,\" Cillian Murphy stars as J. Robert ... 2023, 12:16 p.m. ET. ... including his role as the director of the Manhattan Engineer District, better ... J Robert Oppenheimer was the director of the secret Los Alamos Laboratory. It was established under US president Franklin D Roosevelt as part of the Manhattan Project to build the first atomic bomb. He oversaw the first atomic bomb detonation in the New Mexico desert in July 1945, code-named \"Trinity\". In this opening salvo of 2023's Oscar battle, Nolan has enjoined a star-studded cast for a retelling of the brilliant and haunted life of J. Robert Oppenheimer, the American physicist whose... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age. Thought:The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his birth date to calculate his age. Action: duckduckgo_search Action Input: \"Christopher Nolan birth date\" Observation: July 30, 1970 (age 52) London England Notable Works: \"Dunkirk\" \"Tenet\" \"The Prestige\" See all related content → Recent News Jul. 13, 2023, 11:11 AM ET (AP) Cillian Murphy, playing Oppenheimer, finally gets to lead a Christopher Nolan film Christopher Edward Nolan CBE (born 30 July 1970) is a British and American filmmaker. Known for his Hollywood blockbusters with complex storytelling, Nolan is considered a leading filmmaker of the 21st century. His films have grossed $5 billion worldwide. The recipient of many accolades, he has been nominated for five Academy Awards, five BAFTA Awards and six Golden Globe Awards. Christopher Nolan is currently 52 according to his birthdate July 30, 1970 Sun Sign Leo Born Place Westminster, London, England, United Kingdom Residence Los Angeles, California, United States Nationality Education Chris attended Haileybury and Imperial Service College, in Hertford Heath, Hertfordshire. Christopher Nolan's next movie will study the man who developed the atomic bomb, J. Robert Oppenheimer. Here's the release date, plot, trailers & more. July 2023 sees the release of Christopher Nolan's new film, Oppenheimer, his first movie since 2020's Tenet and his split from Warner Bros. Billed as an epic thriller about \"the man who ... Thought:Christopher Nolan was born on July 30, 1970. Now I need to calculate his age in 2023 and then convert it into days. Action: Calculator Action Input: (2023 - 1970) * 365 Observation: Answer: 19345 Thought: > Finished chain. I now know the final answer Final Answer: The director of the 2023 film Oppenheimer is Christopher Nolan and he is 53 years old in 2023. His age in days is 19345 days. > Finished chain. 'The director of the 2023 film Oppenheimer is Christopher Nolan and he is 53 years old in 2023. His age in days is 19345 days.' Chain(..., verbose=True) ​ You can also scope verbosity down to a single object, in which case only the inputs and outputs to that object are printed (along with any additional callbacks calls made specifically by that object). # Passing verbose=True to initialize_agent will pass that along to the AgentExecutor (which is a Chain). agent = initialize_agent ( tools , llm , agent = AgentType . ZERO_SHOT_REACT_DESCRIPTION , verbose = True , ) agent . run ( \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\" ) Console output > Entering new AgentExecutor chain... First, I need to find out who directed the film Oppenheimer in 2023 and their birth date. Then, I can calculate their age in years and days. Action: duckduckgo_search Action Input: \"Director of 2023 film Oppenheimer\" Observation: Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. In Christopher Nolan's new film, \"Oppenheimer,\" Cillian Murphy stars as J. Robert Oppenheimer, the American physicist who oversaw the Manhattan Project in Los Alamos, N.M. Universal Pictures... J Robert Oppenheimer was the director of the secret Los Alamos Laboratory. It was established under US president Franklin D Roosevelt as part of the Manhattan Project to build the first atomic bomb. He oversaw the first atomic bomb detonation in the New Mexico desert in July 1945, code-named \"Trinity\". A Review of Christopher Nolan's new film 'Oppenheimer' , the story of the man who fathered the Atomic Bomb. Cillian Murphy leads an all star cast ... Release Date: July 21, 2023. Director ... For his new film, \"Oppenheimer,\" starring Cillian Murphy and Emily Blunt, director Christopher Nolan set out to build an entire 1940s western town. Thought:The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his birth date to calculate his age. Action: duckduckgo_search Action Input: \"Christopher Nolan birth date\" Observation: July 30, 1970 (age 52) London England Notable Works: \"Dunkirk\" \"Tenet\" \"The Prestige\" See all related content → Recent News Jul. 13, 2023, 11:11 AM ET (AP) Cillian Murphy, playing Oppenheimer, finally gets to lead a Christopher Nolan film Christopher Edward Nolan CBE (born 30 July 1970) is a British and American filmmaker. Known for his Hollywood blockbusters with complex storytelling, Nolan is considered a leading filmmaker of the 21st century. His films have grossed $5 billion worldwide. The recipient of many accolades, he has been nominated for five Academy Awards, five BAFTA Awards and six Golden Globe Awards. Christopher Nolan is currently 52 according to his birthdate July 30, 1970 Sun Sign Leo Born Place Westminster, London, England, United Kingdom Residence Los Angeles, California, United States Nationality Education Chris attended Haileybury and Imperial Service College, in Hertford Heath, Hertfordshire. Christopher Nolan's next movie will study the man who developed the atomic bomb, J. Robert Oppenheimer. Here's the release date, plot, trailers & more. Date of Birth: 30 July 1970 . ... Christopher Nolan is a British-American film director, producer, and screenwriter. His films have grossed more than US$5 billion worldwide, and have garnered 11 Academy Awards from 36 nominations. ... Thought:Christopher Nolan was born on July 30, 1970. Now I can calculate his age in years and then in days. Action: Calculator Action Input: {\"operation\": \"subtract\", \"operands\": [2023, 1970]} Observation: Answer: 53 Thought:Christopher Nolan is 53 years old in 2023. Now I need to calculate his age in days. Action: Calculator Action Input: {\"operation\": \"multiply\", \"operands\": [53, 365]} Observation: Answer: 19345 Thought:I now know the final answer Final Answer: The director of the 2023 film Oppenheimer is Christopher Nolan. He is 53 years old in 2023, which is approximately 19345 days. > Finished chain. 'The director of the 2023 film Oppenheimer is Christopher Nolan. He is 53 years old in 2023, which is approximately 19345 days.' Other callbacks ​ Callbacks are what we use to execute any functionality within a component outside the primary component logic. All of the above solutions use Callbacks under the hood to log intermediate steps of components. There are a number of Callbacks relevant for debugging that come with LangChain out of the box, like the FileCallbackHandler . You can also implement your own callbacks to execute custom functionality. See here for more info on Callbacks , how to use them, and customize them. Next Deployment"
    },
    {
        "title": "Templates | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/",
        "text_content": "Templates On this page Templates Highlighting a few different categories of templates ⭐ Popular ​ These are some of the more popular templates to get started with. Retrieval Augmented Generation Chatbot : Build a chatbot over your data. Defaults to OpenAI and Pinecone. Extraction with OpenAI Functions : Do extraction of structured data from unstructured data. Uses OpenAI function calling. Local Retrieval Augmented Generation : Build a chatbot over your data. Uses only local tooling: Ollama, GPT4all, Chroma. OpenAI Functions Agent : Build a chatbot that can take actions. Uses OpenAI function calling and Tavily. XML Agent : Build a chatbot that can take actions. Uses Anthropic and You.com. 📥 Advanced Retrieval ​ These templates cover advanced retrieval techniques, which can be used for chat and QA over databases or documents. Reranking : This retrieval technique uses Cohere's reranking endpoint to rerank documents from an initial retrieval step. Anthropic Iterative Search : This retrieval technique uses iterative prompting to determine what to retrieve and whether the retriever documents are good enough. Parent Document Retrieval using Neo4j or MongoDB : This retrieval technique stores embeddings for smaller chunks, but then returns larger chunks to pass to the model for generation. Semi-Structured RAG : The template shows how to do retrieval over semi-structured data (e.g. data that involves both text and tables). Temporal RAG : The template shows how to do hybrid search over data with a time-based component using Timescale Vector . 🔍Advanced Retrieval - Query Transformation ​ A selection of advanced retrieval methods that involve transforming the original user query, which can improve retrieval quality. Hypothetical Document Embeddings : A retrieval technique that generates a hypothetical document for a given query, and then uses the embedding of that document to do semantic search. Paper . Rewrite-Retrieve-Read : A retrieval technique that rewrites a given query before passing it to a search engine. Paper . Step-back QA Prompting : A retrieval technique that generates a \"step-back\" question and then retrieves documents relevant to both that question and the original question. Paper . RAG-Fusion : A retrieval technique that generates multiple queries and then reranks the retrieved documents using reciprocal rank fusion. Article . Multi-Query Retriever : This retrieval technique uses an LLM to generate multiple queries and then fetches documents for all queries. 🧠Advanced Retrieval - Query Construction ​ A selection of advanced retrieval methods that involve constructing a query in a separate DSL from natural language, which enable natural languge chat over various structured databases. Elastic Query Generator : Generate elastic search queries from natural language. Neo4j Cypher Generation : Generate cypher statements from natural language. Available with a \"full text\" option as well. Supabase Self Query : Parse a natural language query into a semantic query as well as a metadata filter for Supabase. 🦙 OSS Models ​ These templates use OSS models, which enable privacy for sensitive data. Local Retrieval Augmented Generation : Build a chatbot over your data. Uses only local tooling: Ollama, GPT4all, Chroma. SQL Question Answering (Replicate) : Question answering over a SQL database, using Llama2 hosted on Replicate . SQL Question Answering (LlamaCpp) : Question answering over a SQL database, using Llama2 through LlamaCpp . SQL Question Answering (Ollama) : Question answering over a SQL database, using Llama2 through Ollama . ⛏️ Extraction ​ These templates extract data in a structured format based upon a user-specified schema. Extraction Using OpenAI Functions : Extract information from text using OpenAI Function Calling. Extraction Using Anthropic Functions : Extract information from text using a LangChain wrapper around the Anthropic endpoints intended to simulate function calling. Extract BioTech Plate Data : Extract microplate data from messy Excel spreadsheets into a more normalized format. ⛏️Summarization and tagging ​ These templates summarize or categorize documents and text. Summarization using Anthropic : Uses Anthropic's Claude2 to summarize long documents. 🤖 Agents ​ These templates build chatbots that can take actions, helping to automate tasks. OpenAI Functions Agent : Build a chatbot that can take actions. Uses OpenAI function calling and Tavily. XML Agent : Build a chatbot that can take actions. Uses Anthropic and You.com. 🚨 Safety and evaluation ​ These templates enable moderation or evaluation of LLM outputs. Guardrails Output Parser : Use guardrails-ai to validate LLM output. Chatbot Feedback : Use LangSmith to evaluate chatbot responses. Next anthropic-iterative-search"
    },
    {
        "title": "Community navigator | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/community",
        "text_content": "Community navigator Hi! Thanks for being here. We’re lucky to have a community of so many passionate developers building with LangChain–we have so much to teach and learn from each other. Community members contribute code, host meetups, write blog posts, amplify each other’s work, become each other's customers and collaborators, and so much more. Whether you’re new to LangChain, looking to go deeper, or just want to get more exposure to the world of building with LLMs, this page can point you in the right direction. 🦜 Contribute to LangChain 🌍 Meetups, Events, and Hackathons 📣 Help Us Amplify Your Work 💬 Stay in the loop 🦜 Contribute to LangChain LangChain is the product of over 5,000+ contributions by 1,500+ contributors, and there is ** still ** so much to do together. Here are some ways to get involved: Open a pull request : We’d appreciate all forms of contributions–new features, infrastructure improvements, better documentation, bug fixes, etc. If you have an improvement or an idea, we’d love to work on it with you. Read our contributor guidelines: We ask contributors to follow a \"fork and pull request\" workflow, run a few local checks for formatting, linting, and testing before submitting, and follow certain documentation and testing conventions. First time contributor? Try one of these PRs with the “good first issue” tag . Become an expert: Our experts help the community by answering product questions in Discord. If that’s a role you’d like to play, we’d be so grateful! (And we have some special experts-only goodies/perks we can tell you more about). Send us an email to introduce yourself at hello@langchain.dev and we’ll take it from there! Integrate with LangChain: If your product integrates with LangChain–or aspires to–we want to help make sure the experience is as smooth as possible for you and end users. Send us an email at hello@langchain.dev and tell us what you’re working on. Become an Integration Maintainer: Partner with our team to ensure your integration stays up-to-date and talk directly with users (and answer their inquiries) in our Discord. Introduce yourself at hello@langchain.dev if you’d like to explore this role. 🌍 Meetups, Events, and Hackathons One of our favorite things about working in AI is how much enthusiasm there is for building together. We want to help make that as easy and impactful for you as possible! Find a meetup, hackathon, or webinar: You can find the one for you on our global events calendar . Submit an event to our calendar: Email us at events@langchain.dev with a link to your event page! We can also help you spread the word with our local communities. Host a meetup: If you want to bring a group of builders together, we want to help! We can publicize your event on our event calendar/Twitter, share it with our local communities in Discord, send swag, or potentially hook you up with a sponsor. Email us at events@langchain.dev to tell us about your event! Become a meetup sponsor: We often hear from groups of builders that want to get together, but are blocked or limited on some dimension (space to host, budget for snacks, prizes to distribute, etc.). If you’d like to help, send us an email to events@langchain.dev we can share more about how it works! Speak at an event: Meetup hosts are always looking for great speakers, presenters, and panelists. If you’d like to do that at an event, send us an email to hello@langchain.dev with more information about yourself, what you want to talk about, and what city you’re based in and we’ll try to match you with an upcoming event! Tell us about your LLM community: If you host or participate in a community that would welcome support from LangChain and/or our team, send us an email at hello@langchain.dev and let us know how we can help. 📣 Help Us Amplify Your Work If you’re working on something you’re proud of, and think the LangChain community would benefit from knowing about it, we want to help you show it off. Post about your work and mention us: We love hanging out on Twitter to see what people in the space are talking about and working on. If you tag @langchainai , we’ll almost certainly see it and can show you some love. Publish something on our blog: If you’re writing about your experience building with LangChain, we’d love to post (or crosspost) it on our blog! E-mail hello@langchain.dev with a draft of your post! Or even an idea for something you want to write about. Get your product onto our integrations hub : Many developers take advantage of our seamless integrations with other products, and come to our integrations hub to find out who those are. If you want to get your product up there, tell us about it (and how it works with LangChain) at hello@langchain.dev . ☀️ Stay in the loop Here’s where our team hangs out, talks shop, spotlights cool work, and shares what we’re up to. We’d love to see you there too. Twitter : We post about what we’re working on and what cool things we’re seeing in the space. If you tag @langchainai in your post, we’ll almost certainly see it, and can show you some love! Discord : connect with over 30,000 developers who are building with LangChain. GitHub : Open pull requests, contribute to a discussion, and/or contribute Subscribe to our bi-weekly Release Notes : a twice/month email roundup of the coolest things going on in our orbit"
    },
    {
        "title": "Welcome Contributors | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/contributing",
        "text_content": "Welcome Contributors On this page Welcome Contributors Hi there! Thank you for even being interested in contributing to LangChain.\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether they involve new features, improved infrastructure, better documentation, or bug fixes. 🗺️ Guidelines ​ 👩‍💻 Ways to contribute ​ There are many ways to contribute to LangChain. Here are some common ways people contribute: Documentation : Help improve our docs, including this one! Code : Help us write code, fix bugs, or improve our infrastructure. Integrations : Help us integrate with your favorite vendors and tools. 🚩GitHub Issues ​ Our issues page is kept up to date with bugs, improvements, and feature requests. There is a taxonomy of labels to help with sorting and discovery of issues of interest. Please use these to help organize issues. If you start working on an issue, please assign it to yourself. If you are adding an issue, please try to keep it focused on a single, modular bug/improvement/feature.\nIf two issues are related, or blocking, please link them rather than combining them. We will try to keep these issues as up-to-date as possible, though\nwith the rapid rate of development in this field some may get out of date.\nIf you notice this happening, please let us know. 🙋Getting Help ​ Our goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please\ncontact a maintainer! Not only do we want to help get you unblocked, but we also want to make sure that the process is\nsmooth for future contributors. In a similar vein, we do enforce certain linting, formatting, and documentation standards in the codebase.\nIf you are finding these difficult (or even just annoying) to work with, feel free to contact a maintainer for help -\nwe do not want these to get in the way of getting good code into the codebase. Next Contribute Code"
    },
    {
        "title": "Dependents | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/additional_resources/dependents",
        "text_content": "Dependents Dependents stats for langchain-ai/langchain [update: 2023-12-08 ; only dependent repositories with Stars > 100] Repository Stars AntonOsika/gpt-engineer 46514 imartinez/privateGPT 44439 LAION-AI/Open-Assistant 35906 hpcaitech/ColossalAI 35528 moymix/TaskMatrix 34342 geekan/MetaGPT 31126 streamlit/streamlit 28911 reworkd/AgentGPT 27833 StanGirard/quivr 26032 OpenBB-finance/OpenBBTerminal 24946 run-llama/llama_index 24859 jmorganca/ollama 20849 openai/chatgpt-retrieval-plugin 20249 chatchat-space/Langchain-Chatchat 19305 mindsdb/mindsdb 19172 PromtEngineer/localGPT 17528 cube-js/cube 16575 mlflow/mlflow 16000 mudler/LocalAI 14067 logspace-ai/langflow 13679 GaiZhenbiao/ChuanhuChatGPT 13648 arc53/DocsGPT 13423 openai/evals 12649 airbytehq/airbyte 12460 langgenius/dify 11859 databrickslabs/dolly 10672 AIGC-Audio/AudioGPT 9437 langchain-ai/langchainjs 9227 gventuri/pandas-ai 9203 aws/amazon-sagemaker-examples 9079 h2oai/h2ogpt 8945 PipedreamHQ/pipedream 7550 bentoml/OpenLLM 6957 THUDM/ChatGLM3 6801 microsoft/promptflow 6776 cpacker/MemGPT 6642 joshpxyne/gpt-migrate 6482 zauberzeug/nicegui 6037 embedchain/embedchain 6023 mage-ai/mage-ai 6019 assafelovic/gpt-researcher 5936 sweepai/sweep 5855 wenda-LLM/wenda 5766 zilliztech/GPTCache 5710 pdm-project/pdm 5665 GreyDGL/PentestGPT 5568 gkamradt/langchain-tutorials 5507 Shaunwei/RealChar 5501 facebookresearch/llama-recipes 5477 serge-chat/serge 5221 run-llama/rags 4916 openchatai/OpenChat 4870 danswer-ai/danswer 4774 langchain-ai/opengpts 4709 postgresml/postgresml 4639 MineDojo/Voyager 4582 intel-analytics/BigDL 4581 yihong0618/xiaogpt 4359 RayVentura/ShortGPT 4357 Azure-Samples/azure-search-openai-demo 4317 madawei2699/myGPTReader 4289 apache/nifi 4098 langchain-ai/chat-langchain 4091 aiwaves-cn/agents 4073 krishnaik06/The-Grand-Complete-Data-Science-Materials 4065 khoj-ai/khoj 4016 Azure/azure-sdk-for-python 3941 PrefectHQ/marvin 3915 OpenBMB/ToolBench 3799 marqo-ai/marqo 3771 kyegomez/tree-of-thoughts 3688 Unstructured-IO/unstructured 3543 llm-workflow-engine/llm-workflow-engine 3515 shroominic/codeinterpreter-api 3425 openchatai/OpenCopilot 3418 josStorer/RWKV-Runner 3297 whitead/paper-qa 3280 homanp/superagent 3258 ParisNeo/lollms-webui 3199 OpenBMB/AgentVerse 3099 project-baize/baize-chatbot 3090 OpenGVLab/InternGPT 2989 xlang-ai/OpenAgents 2825 dataelement/bisheng 2797 Mintplex-Labs/anything-llm 2784 OpenBMB/BMTools 2734 run-llama/llama-hub 2721 SamurAIGPT/EmbedAI 2647 NVIDIA/NeMo-Guardrails 2637 X-D-Lab/LangChain-ChatGLM-Webui 2532 GerevAI/gerev 2517 keephq/keep 2448 yanqiangmiffy/Chinese-LangChain 2397 OpenGVLab/Ask-Anything 2324 IntelligenzaArtificiale/Free-Auto-GPT 2241 YiVal/YiVal 2232 jupyterlab/jupyter-ai 2189 Farama-Foundation/PettingZoo 2136 microsoft/TaskWeaver 2126 hwchase17/notion-qa 2083 FlagOpen/FlagEmbedding 2053 paulpierre/RasaGPT 1999 hegelai/prompttools 1984 mckinsey/vizro 1951 vocodedev/vocode-python 1868 dot-agent/openAMS 1796 explodinggradients/ragas 1766 AI-Citizen/SolidGPT 1761 Kav-K/GPTDiscord 1696 run-llama/sec-insights 1654 avinashkranjan/Amazing-Python-Scripts 1635 microsoft/WhatTheHack 1629 noahshinn/reflexion 1625 psychic-api/psychic 1618 Forethought-Technologies/AutoChain 1611 pinterest/querybook 1586 refuel-ai/autolabel 1553 jina-ai/langchain-serve 1537 jina-ai/dev-gpt 1522 agiresearch/OpenAGI 1493 ttengwang/Caption-Anything 1484 greshake/llm-security 1483 promptfoo/promptfoo 1480 milvus-io/bootcamp 1477 richardyc/Chrome-GPT 1475 melih-unsal/DemoGPT 1428 YORG-AI/Open-Assistant 1419 101dotxyz/GPTeam 1416 jina-ai/thinkgpt 1408 mmz-001/knowledge_gpt 1398 intel/intel-extension-for-transformers 1387 Azure/azureml-examples 1385 lunasec-io/lunasec 1367 eyurtsev/kor 1355 xusenlinzy/api-for-open-llm 1325 griptape-ai/griptape 1323 SuperDuperDB/superduperdb 1290 cofactoryai/textbase 1284 psychic-api/rag-stack 1260 filip-michalsky/SalesGPT 1250 nod-ai/SHARK 1237 pluralsh/plural 1234 cheshire-cat-ai/core 1194 LC1332/Chat-Haruhi-Suzumiya 1184 poe-platform/server-bot-quick-start 1182 microsoft/X-Decoder 1180 juncongmoo/chatllama 1171 visual-openllm/visual-openllm 1156 alejandro-ao/ask-multiple-pdfs 1153 ThousandBirdsInc/chidori 1152 irgolic/AutoPR 1137 SamurAIGPT/Camel-AutoGPT 1083 ray-project/llm-applications 1080 run-llama/llama-lab 1072 jiran214/GPT-vup 1041 MetaGLM/FinGLM 1035 peterw/Chat-with-Github-Repo 1020 Anil-matcha/ChatPDF 991 langchain-ai/langserve 983 THUDM/AgentTuning 976 rlancemartin/auto-evaluator 975 codeacme17/examor 964 all-in-aigc/gpts-works 946 Ikaros-521/AI-Vtuber 946 microsoft/Llama-2-Onnx 898 cirediatpl/FigmaChain 895 ricklamers/shell-ai 893 modelscope/modelscope-agent 893 seanpixel/Teenage-AGI 886 ajndkr/lanarky 880 kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference 872 corca-ai/EVAL 846 hwchase17/chat-your-data 841 kreneskyp/ix 821 Link-AGI/AutoAgents 820 truera/trulens 794 Dataherald/dataherald 788 sunlabuiuc/PyHealth 783 jondurbin/airoboros 783 pyspark-ai/pyspark-ai 782 confident-ai/deepeval 780 billxbf/ReWOO 777 langchain-ai/streamlit-agent 776 akshata29/entaoai 771 LambdaLabsML/examples 770 getmetal/motorhead 768 Dicklesworthstone/swiss_army_llama 757 ruoccofabrizio/azure-open-ai-embeddings-qna 757 msoedov/langcorn 754 e-johnstonn/BriefGPT 753 microsoft/sample-app-aoai-chatGPT 749 explosion/spacy-llm 731 MiuLab/Taiwan-LLM 716 whyiyhw/chatgpt-wechat 702 Azure-Samples/openai 692 iusztinpaul/hands-on-llms 687 safevideo/autollm 682 OpenGenerativeAI/GenossGPT 669 NoDataFound/hackGPT 663 AILab-CVC/GPT4Tools 662 langchain-ai/auto-evaluator 657 yvann-ba/Robby-chatbot 639 alexanderatallah/window.ai 635 amosjyng/langchain-visualizer 630 microsoft/PodcastCopilot 621 aws-samples/aws-genai-llm-chatbot 616 NeumTry/NeumAI 605 namuan/dr-doc-search 599 plastic-labs/tutor-gpt 595 marimo-team/marimo 591 yakami129/VirtualWife 586 xuwenhao/geektime-ai-course 584 jonra1993/fastapi-alembic-sqlmodel-async 573 dgarnitz/vectorflow 568 yeagerai/yeagerai-agent 564 daveebbelaar/langchain-experiments 563 traceloop/openllmetry 559 Agenta-AI/agenta 546 michaelthwan/searchGPT 545 jina-ai/agentchain 544 mckaywrigley/repo-chat 533 marella/chatdocs 532 opentensor/bittensor 532 DjangoPeng/openai-quickstart 527 freddyaboulton/gradio-tools 517 sidhq/Multi-GPT 515 alejandro-ao/langchain-ask-pdf 514 sajjadium/ctf-archives 507 continuum-llms/chatgpt-memory 502 llmOS/opencopilot 495 steamship-core/steamship-langchain 494 mpaepper/content-chatbot 493 langchain-ai/langchain-aiplugin 492 logan-markewich/llama_index_starter_pack 483 datawhalechina/llm-universe 475 leondz/garak 464 RedisVentures/ArXivChatGuru 461 Anil-matcha/Chatbase 455 Aiyu-awa/luna-ai 450 DataDog/dd-trace-py 450 Azure-Samples/miyagi 449 poe-platform/poe-protocol 447 onlyphantom/llm-python 446 junruxiong/IncarnaMind 441 CarperAI/OpenELM 441 daodao97/chatdoc 437 showlab/VLog 436 wandb/weave 420 QwenLM/Qwen-Agent 419 huchenxucs/ChatDB 416 jerlendds/osintbuddy 411 monarch-initiative/ontogpt 408 mallorbc/Finetune_LLMs 406 JayZeeDesign/researcher-gpt 405 rsaryev/talk-codebase 401 langchain-ai/langsmith-cookbook 398 mtenenholtz/chat-twitter 398 morpheuslord/GPT_Vuln-analyzer 391 MagnivOrg/prompt-layer-library 387 JohnSnowLabs/langtest 384 mrwadams/attackgen 381 codefuse-ai/Test-Agent 380 personoids/personoids-lite 379 mosaicml/examples 378 steamship-packages/langchain-production-starter 370 FlagAI-Open/Aquila2 365 Mintplex-Labs/vector-admin 365 NimbleBoxAI/ChainFury 357 BlackHC/llm-strategy 354 lilacai/lilac 352 preset-io/promptimize 351 yuanjie-ai/ChatLLM 347 andylokandy/gpt-4-search 346 zhoudaquan/ChatAnything 343 rgomezcasas/dotfiles 343 tigerlab-ai/tiger 342 HumanSignal/label-studio-ml-backend 334 nasa-petal/bidara 334 momegas/megabots 334 Cheems-Seminar/grounded-segment-any-parts 330 CambioML/pykoi 326 Nuggt-dev/Nuggt 326 wandb/edu 326 Haste171/langchain-chatbot 324 sugarforever/LangChain-Tutorials 322 liangwq/Chatglm_lora_multi-gpu 321 ur-whitelab/chemcrow-public 320 itamargol/openai 318 gia-guar/JARVIS-ChatGPT 304 SpecterOps/Nemesis 302 facebookresearch/personal-timeline 302 hnawaz007/pythondataanalysis 301 Chainlit/cookbook 300 airobotlab/KoChatGPT 300 GPT-Fathom/GPT-Fathom 299 kaarthik108/snowChat 299 kyegomez/swarms 296 LangStream/langstream 295 genia-dev/GeniA 294 shamspias/customizable-gpt-chatbot 291 TsinghuaDatabaseGroup/DB-GPT 290 conceptofmind/toolformer 283 sullivan-sean/chat-langchainjs 283 AutoPackAI/beebot 282 pablomarin/GPT-Azure-Search-Engine 282 gkamradt/LLMTest_NeedleInAHaystack 280 gustavz/DataChad 280 Safiullah-Rahu/CSV-AI 278 hwchase17/chroma-langchain 275 AkshitIreddy/Interactive-LLM-Powered-NPCs 268 ennucore/clippinator 267 artitw/text2text 264 anarchy-ai/LLM-VM 263 wpydcr/LLM-Kit 262 streamlit/llm-examples 262 paolorechia/learn-langchain 262 yym68686/ChatGPT-Telegram-Bot 261 PradipNichite/Youtube-Tutorials 259 radi-cho/datasetGPT 259 ur-whitelab/exmol 259 ml6team/fondant 254 bborn/howdoi.ai 254 rahulnyk/knowledge_graph 253 recalign/RecAlign 248 hwchase17/langchain-streamlit-template 248 fetchai/uAgents 247 arthur-ai/bench 247 miaoshouai/miaoshouai-assistant 246 RoboCoachTechnologies/GPT-Synthesizer 244 langchain-ai/web-explorer 242 kaleido-lab/dolphin 242 PJLab-ADG/DriveLikeAHuman 241 stepanogil/autonomous-hr-chatbot 238 WongSaang/chatgpt-ui-server 236 nexus-stc/stc 235 yeagerai/genworlds 235 Gentopia-AI/Gentopia 235 alphasecio/langchain-examples 235 grumpyp/aixplora 232 shaman-ai/agent-actors 232 darrenburns/elia 231 orgexyz/BlockAGI 231 handrew/browserpilot 226 su77ungr/CASALIOY 225 nicknochnack/LangchainDocuments 225 dbpunk-labs/octogen 224 langchain-ai/weblangchain 222 CL-lau/SQL-GPT 222 alvarosevilla95/autolang 221 showlab/UniVTG 220 edreisMD/plugnplai 219 hardbyte/qabot 216 microsoft/azure-openai-in-a-day-workshop 215 Azure-Samples/chat-with-your-data-solution-accelerator 214 amadad/agentcy 213 snexus/llm-search 212 afaqueumer/DocQA 206 plchld/InsightFlow 205 yasyf/compress-gpt 205 benthecoder/ClassGPT 205 voxel51/voxelgpt 204 jbrukh/gpt-jargon 204 emarco177/ice_breaker 204 tencentmusic/supersonic 202 Azure-Samples/azure-search-power-skills 202 blob42/Instrukt 201 langchain-ai/langsmith-sdk 200 SamPink/dev-gpt 200 ju-bezdek/langchain-decorators 198 KMnO4-zx/huanhuan-chat 196 Azure-Samples/jp-azureopenai-samples 192 hongbo-miao/hongbomiao.com 190 CakeCrusher/openplugin 190 PaddlePaddle/ERNIE-Bot-SDK 189 retr0reg/Ret2GPT 189 AmineDiro/cria 187 lancedb/vectordb-recipes 186 vaibkumr/prompt-optimizer 185 aws-ia/ecs-blueprints 184 ethanyanjiali/minChatGPT 183 MuhammadMoinFaisal/LargeLanguageModelsProjects 182 shauryr/S2QA 181 summarizepaper/summarizepaper 180 NomaDamas/RAGchain 179 pnkvalavala/repochat 179 ibiscp/LLM-IMDB 177 fengyuli-dev/multimedia-gpt 177 langchain-ai/text-split-explorer 175 iMagist486/ElasticSearch-Langchain-Chatglm2 175 limaoyi1/Auto-PPT 175 Open-Swarm-Net/GPT-Swarm 175 morpheuslord/HackBot 174 v7labs/benchllm 174 Coding-Crashkurse/Langchain-Full-Course 174 dongyh20/Octopus 173 kimtth/azure-openai-llm-vector-langchain 173 mayooear/private-chatbot-mpt30b-langchain 173 zilliztech/akcio 172 jmpaz/promptlib 172 ccurme/yolopandas 172 joaomdmoura/CrewAI 170 katanaml/llm-mistral-invoice-cpu 170 chakkaradeep/pyCodeAGI 170 mudler/LocalAGI 167 dssjon/biblos 165 kjappelbaum/gptchem 165 xxw1995/chatglm3-finetune 164 ArjanCodes/examples 163 AIAnytime/Llama2-Medical-Chatbot 163 RCGAI/SimplyRetrieve 162 langchain-ai/langchain-teacher 162 menloparklab/falcon-langchain 162 flurb18/AgentOoba 162 homanp/vercel-langchain 161 jiran214/langup-ai 160 JorisdeJong123/7-Days-of-LangChain 160 GoogleCloudPlatform/data-analytics-golden-demo 159 positive666/Prompt-Can-Anything 159 luisroque/large_laguage_models 159 mlops-for-all/mlops-for-all.github.io 158 wandb/wandbot 158 elastic/elasticsearch-labs 157 shroominic/funcchain 157 deeppavlov/dream 156 mluogh/eastworld 154 georgesung/llm_qlora 154 RUC-GSAI/YuLan-Rec 153 KylinC/ChatFinance 152 Dicklesworthstone/llama2_aided_tesseract 152 c0sogi/LLMChat 152 eunomia-bpf/GPTtrace 152 ErikBjare/gptme 152 Klingefjord/chatgpt-telegram 152 RoboCoachTechnologies/ROScribe 151 Aggregate-Intellect/sherpa 151 3Alan/DocsMind 151 tangqiaoyu/ToolAlpaca 150 kulltc/chatgpt-sql 150 mallahyari/drqa 150 MedalCollector/Orator 149 Teahouse-Studios/akari-bot 149 realminchoi/babyagi-ui 148 ssheng/BentoChain 148 lmstudio-ai/examples 147 solana-labs/chatgpt-plugin 147 aurelio-labs/arxiv-bot 147 Jaseci-Labs/jaseci 146 menloparklab/langchain-cohere-qdrant-doc-retrieval 146 trancethehuman/entities-extraction-web-scraper 144 peterw/StoryStorm 144 grumpyp/chroma-langchain-tutorial 144 gh18l/CrawlGPT 142 langchain-ai/langchain-aws-template 142 yasyf/summ 141 petehunt/langchain-github-bot 141 hirokidaichi/wanna 140 jina-ai/fastapi-serve 139 zenml-io/zenml-projects 139 jlonge4/local_llama 139 smyja/blackmaria 138 ChuloAI/BrainChulo 137 log1stics/voice-generator-webui 137 davila7/file-gpt 137 dcaribou/transfermarkt-datasets 136 ciare-robotics/world-creator 135 Undertone0809/promptulate 134 fixie-ai/fixie-examples 134 run-llama/ai-engineer-workshop 133 definitive-io/code-indexer-loop 131 mortium91/langchain-assistant 131 baidubce/bce-qianfan-sdk 130 Ngonie-x/langchain_csv 130 IvanIsCoding/ResuLLMe 130 AnchoringAI/anchoring-ai 129 Azure/business-process-automation 128 athina-ai/athina-sdk 126 thunlp/ChatEval 126 prof-frink-lab/slangchain 126 vietanhdev/pautobot 125 awslabs/generative-ai-cdk-constructs 124 sdaaron/QueryGPT 124 rabbitmetrics/langchain-13-min 124 AutoLLM/AutoAgents 122 nicknochnack/Nopenai 122 wombyz/HormoziGPT 122 dotvignesh/PDFChat 122 topoteretes/PromethAI-Backend 121 nftblackmagic/flask-langchain 121 vishwasg217/finsight 120 snap-stanford/MLAgentBench 120 Azure/app-service-linux-docs 120 nyanp/chat2plot 120 ant4g0nist/polar 119 aws-samples/cdk-eks-blueprints-patterns 119 aws-samples/amazon-kendra-langchain-extensions 119 Xueheng-Li/SynologyChatbotGPT 119 CodeAlchemyAI/ViLT-GPT 117 Lin-jun-xiang/docGPT-langchain 117 ademakdogan/ChatSQL 116 aniketmaurya/llm-inference 115 xuwenhao/mactalk-ai-course 115 cmooredev/RepoReader 115 abi/autocommit 115 MIDORIBIN/langchain-gpt4free 114 finaldie/auto-news 114 Anil-matcha/Youtube-to-chatbot 114 avrabyt/MemoryBot 114 Capsize-Games/airunner 113 atisharma/llama_farm 113 mbchang/data-driven-characters 112 fiddler-labs/fiddler-auditor 112 dirkjbreeuwer/gpt-automated-web-scraper 111 Appointat/Chat-with-Document-s-using-ChatGPT-API-and-Text-Embedding 111 hwchase17/langchain-gradio-template 111 artas728/spelltest 110 NVIDIA/GenerativeAIExamples 109 Azure/aistudio-copilot-sample 108 codefuse-ai/codefuse-chatbot 108 apirrone/Memento 108 e-johnstonn/GPT-Doc-Summarizer 108 salesforce/BOLAA 107 Erol444/gpt4-openai-api 106 linjungz/chat-with-your-doc 106 crosleythomas/MirrorGPT 106 panaverse/learn-generative-ai 105 Azure/azure-sdk-tools 105 malywut/gpt_examples 105 ritun16/chain-of-verification 104 langchain-ai/langchain-benchmarks 104 lightninglabs/LangChainBitcoin 104 flepied/second-brain-agent 103 llmapp/openai.mini 102 gimlet-ai/tddGPT 102 jlonge4/gpt_chatwithPDF 102 agentification/RAFA_code 101 pacman100/DHS-LLM-Workshop 101 aws-samples/private-llm-qa-bot 101 Generated by github-dependents-info github-dependents-info --repo \"langchain-ai/langchain\" --markdownfile dependents.md --minstars 100 --sort stars"
    },
    {
        "title": "Tutorials | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/additional_resources/tutorials",
        "text_content": "On this page Tutorials Below are links to tutorials and courses on LangChain. For written guides on common use cases for LangChain, check out the use cases guides . ⛓ icon marks a new addition [last update 2023-09-21] LangChain on Wikipedia ​ Books ​ ⛓ Generative AI with LangChain by Ben Auffrath , ©️ 2023 Packt Publishing ​ DeepLearning.AI courses ​ by Harrison Chase and Andrew Ng LangChain for LLM Application Development LangChain Chat with Your Data ⛓ Functions, Tools and Agents with LangChain Handbook ​ LangChain AI Handbook By James Briggs and Francisco Ingham Short Tutorials ​ LangChain Explained in 13 Minutes | QuickStart Tutorial for Beginners by Rabbitmetrics LangChain Crash Course: Build an AutoGPT app in 25 minutes by Nicholas Renotte LangChain Crash Course - Build apps with language models by Patrick Loeber Tutorials ​ LangChain for Gen AI and LLMs by James Briggs ​ #1 Getting Started with GPT-3 vs. Open Source LLMs #2 Prompt Templates for GPT 3.5 and other LLMs #3 LLM Chains using GPT 3.5 and other LLMs LangChain Data Loaders, Tokenizers, Chunking, and Datasets - Data Prep 101 #4 Chatbot Memory for Chat-GPT , Davinci + other LLMs #5 Chat with OpenAI in LangChain #6 Fixing LLM Hallucinations with Retrieval Augmentation in LangChain #7 LangChain Agents Deep Dive with GPT 3.5 #8 Create Custom Tools for Chatbots in LangChain #9 Build Conversational Agents with Vector DBs Using NEW MPT-7B in Hugging Face and LangChain MPT-30B Chatbot with LangChain ⛓ Fine-tuning OpenAI's GPT 3.5 for LangChain Agents ⛓ Chatbots with RAG : LangChain Full Walkthrough LangChain 101 by Greg Kamradt (Data Indy) ​ What Is LangChain? - LangChain + ChatGPT Overview Quickstart Guide Beginner's Guide To 7 Essential Concepts Beginner's Guide To 9 Use Cases Agents Overview + Google Searches OpenAI + Wolfram Alpha Ask Questions On Your Custom (or Private) Files Connect Google Drive Files To OpenAI YouTube Transcripts + OpenAI Question A 300 Page Book (w/ OpenAI + Pinecone ) Workaround OpenAI's Token Limit With Chain Types Build Your Own OpenAI + LangChain Web App in 23 Minutes Working With The New ChatGPT API OpenAI + LangChain Wrote Me 100 Custom Sales Emails Structured Output From OpenAI (Clean Dirty Data) Connect OpenAI To +5,000 Tools (LangChain + Zapier ) Use LLMs To Extract Data From Text (Expert Mode) Extract Insights From Interview Transcripts Using LLMs 5 Levels Of LLM Summarizing: Novice to Expert Control Tone & Writing Style Of Your LLM Output Build Your Own AI Twitter Bot Using LLMs ChatGPT made my interview questions for me ( Streamlit + LangChain) Function Calling via ChatGPT API - First Look With LangChain Extract Topics From Video/Audio With LLMs (Topic Modeling w/ LangChain) LangChain How to and guides by Sam Witteveen ​ LangChain Basics - LLMs & PromptTemplates with Colab LangChain Basics - Tools and Chains ChatGPT API Announcement & Code Walkthrough with LangChain Conversations with Memory (explanation & code walkthrough) Chat with Flan20B Using Hugging Face Models locally (code walkthrough) PAL : Program-aided Language Models with LangChain code Building a Summarization System with LangChain and GPT-3 - Part 1 Building a Summarization System with LangChain and GPT-3 - Part 2 Microsoft's Visual ChatGPT using LangChain LangChain Agents - Joining Tools and Chains with Decisions Comparing LLMs with LangChain Using Constitutional AI in LangChain Talking to Alpaca with LangChain - Creating an Alpaca Chatbot Talk to your CSV & Excel with LangChain BabyAGI : Discover the Power of Task-Driven Autonomous Agents! Improve your BabyAGI with LangChain Master PDF Chat with LangChain - Your essential guide to queries on documents Using LangChain with DuckDuckGO , Wikipedia & PythonREPL Tools Building Custom Tools and Agents with LangChain (gpt-3.5-turbo) LangChain Retrieval QA Over Multiple Files with ChromaDB LangChain Retrieval QA with Instructor Embeddings & ChromaDB for PDFs LangChain + Retrieval Local LLMs for Retrieval QA - No OpenAI!!! Camel + LangChain for Synthetic Data & Market Research Information Extraction with LangChain & Kor Converting a LangChain App from OpenAI to OpenSource Using LangChain Output Parsers to get what you want out of LLMs Building a LangChain Custom Medical Agent with Memory Understanding ReACT with LangChain OpenAI Functions + LangChain : Building a Multi Tool Agent What can you do with 16K tokens in LangChain? Tagging and Extraction - Classification using OpenAI Functions HOW to Make Conversational Form with LangChain ⛓ Claude-2 meets LangChain! ⛓ PaLM 2 Meets LangChain ⛓ LLaMA2 with LangChain - Basics | LangChain TUTORIAL ⛓ Serving LLaMA2 with Replicate ⛓ NEW LangChain Expression Language ⛓ Building a RCI Chain for Agents with LangChain Expression Language ⛓ How to Run LLaMA-2-70B on the Together AI ⛓ RetrievalQA with LLaMA 2 70b & Chroma DB ⛓ How to use BGE Embeddings for LangChain ⛓ How to use Custom Prompts for RetrievalQA on LLaMA-2 7B LangChain by Prompt Engineering ​ LangChain Crash Course — All You Need to Know to Build Powerful Apps with LLMs Working with MULTIPLE PDF Files in LangChain: ChatGPT for your Data ChatGPT for YOUR OWN PDF files with LangChain Talk to YOUR DATA without OpenAI APIs: LangChain LangChain: PDF Chat App (GUI) | ChatGPT for Your PDF FILES LangFlow : Build Chatbots without Writing Code LangChain: Giving Memory to LLMs BEST OPEN Alternative to OPENAI's EMBEDDINGs for Retrieval QA: LangChain LangChain: Run Language Models Locally - Hugging Face Models ⛓ Slash API Costs: Mastering Caching for LLM Applications ⛓ Avoid PROMPT INJECTION with Constitutional AI - LangChain LangChain by Chat with data ​ LangChain Beginner's Tutorial for Typescript / Javascript GPT-4 Tutorial: How to Chat With Multiple PDF Files (~1000 pages of Tesla's 10-K Annual Reports) GPT-4 & LangChain Tutorial: How to Chat With A 56-Page PDF Document (w/ Pinecone ) LangChain & Supabase Tutorial: How to Build a ChatGPT Chatbot For Your Website LangChain Agents: Build Personal Assistants For Your Data (Q&A with Harrison Chase and Mayo Oshin) Codebase Analysis ​ Codebase Analysis: Langchain Agents ⛓ icon marks a new addition [last update 2023-09-21]"
    },
    {
        "title": "YouTube videos | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/additional_resources/youtube",
        "text_content": "On this page YouTube videos ⛓ icon marks a new addition [last update 2023-09-21] Official LangChain YouTube channel ​ Introduction to LangChain with Harrison Chase, creator of LangChain ​ Building the Future with LLMs, LangChain , & Pinecone by Pinecone LangChain and Weaviate with Harrison Chase and Bob van Luijt - Weaviate Podcast #36 by Weaviate • Vector Database LangChain Demo + Q&A with Harrison Chase by Full Stack Deep Learning LangChain Agents: Build Personal Assistants For Your Data (Q&A with Harrison Chase and Mayo Oshin) by Chat with data Videos (sorted by views) ​ Using ChatGPT with YOUR OWN Data. This is magical. (LangChain OpenAI API) by TechLead First look - ChatGPT + WolframAlpha ( GPT-3.5 and Wolfram|Alpha via LangChain by James Weaver) by Dr Alan D. Thompson LangChain explained - The hottest new Python framework by AssemblyAI Chatbot with INFINITE MEMORY using OpenAI & Pinecone - GPT-3 , Embeddings , ADA , Vector DB , Semantic by David Shapiro ~ AI LangChain for LLMs is... basically just an Ansible playbook by David Shapiro ~ AI Build your own LLM Apps with LangChain & GPT-Index by 1littlecoder BabyAGI - New System of Autonomous AI Agents with LangChain by 1littlecoder Run BabyAGI with Langchain Agents (with Python Code) by 1littlecoder How to Use Langchain With Zapier | Write and Send Email with GPT-3 | OpenAI API Tutorial by StarMorph AI Use Your Locally Stored Files To Get Response From GPT - OpenAI | Langchain | Python by Shweta Lodha Langchain JS | How to Use GPT-3, GPT-4 to Reference your own Data | OpenAI Embeddings Intro by StarMorph AI The easiest way to work with large language models | Learn LangChain in 10min by Sophia Yang 4 Autonomous AI Agents: “Westworld” simulation BabyAGI , AutoGPT , Camel , LangChain by Sophia Yang AI CAN SEARCH THE INTERNET? Langchain Agents + OpenAI ChatGPT by tylerwhatsgood Query Your Data with GPT-4 | Embeddings, Vector Databases | Langchain JS Knowledgebase by StarMorph AI Weaviate + LangChain for LLM apps presented by Erika Cardenas by Weaviate • Vector Database Langchain Overview — How to Use Langchain & ChatGPT by Python In Office Langchain Overview - How to Use Langchain & ChatGPT by Python In Office LangChain Tutorials by Edrick : LangChain, Chroma DB, OpenAI Beginner Guide | ChatGPT with your PDF LangChain 101: The Complete Beginner's Guide Custom langchain Agent & Tools with memory. Turn any Python function into langchain tool with Gpt 3 by echohive Building AI LLM Apps with LangChain (and more?) - LIVE STREAM by Nicholas Renotte ChatGPT with any YouTube video using langchain and chromadb by echohive How to Talk to a PDF using LangChain and ChatGPT by Automata Learning Lab Langchain Document Loaders Part 1: Unstructured Files by Merk LangChain - Prompt Templates (what all the best prompt engineers use) by Nick Daigler LangChain. Crear aplicaciones Python impulsadas por GPT by Jesús Conde Easiest Way to Use GPT In Your Products | LangChain Basics Tutorial by Rachel Woods BabyAGI + GPT-4 Langchain Agent with Internet Access by tylerwhatsgood Learning LLM Agents. How does it actually work? LangChain, AutoGPT & OpenAI by Arnoldas Kemeklis Get Started with LangChain in Node.js by Developers Digest LangChain + OpenAI tutorial: Building a Q&A system w/ own text data by Samuel Chan Langchain + Zapier Agent by Merk Connecting the Internet with ChatGPT (LLMs) using Langchain And Answers Your Questions by Kamalraj M M Build More Powerful LLM Applications for Business’s with LangChain (Beginners Guide) by No Code Blackbox LangFlow LLM Agent Demo for 🦜🔗LangChain by Cobus Greyling Chatbot Factory: Streamline Python Chatbot Creation with LLMs and Langchain by Finxter LangChain Tutorial - ChatGPT mit eigenen Daten by Coding Crashkurse Chat with a CSV | LangChain Agents Tutorial (Beginners) by GoDataProf Introdução ao Langchain - #Cortes - Live DataHackers by Prof. João Gabriel Lima LangChain: Level up ChatGPT !? | LangChain Tutorial Part 1 by Code Affinity KI schreibt krasses Youtube Skript 😲😳 | LangChain Tutorial Deutsch by SimpleKI Chat with Audio: Langchain, Chroma DB , OpenAI, and Assembly AI by AI Anytime QA over documents with Auto vector index selection with Langchain router chains by echohive Build your own custom LLM application with Bubble.io & Langchain (No Code & Beginner friendly) by No Code Blackbox Simple App to Question Your Docs: Leveraging Streamlit , Hugging Face Spaces , LangChain, and Claude ! by Chris Alexiuk LANGCHAIN AI- ConstitutionalChainAI + Databutton AI ASSISTANT Web App by Avra LANGCHAIN AI AUTONOMOUS AGENT WEB APP - 👶 BABY AGI 🤖 with EMAIL AUTOMATION using DATABUTTON by Avra The Future of Data Analysis: Using A.I. Models in Data Analysis (LangChain) by Absent Data Memory in LangChain | Deep dive (python) by Eden Marco 9 LangChain UseCases | Beginner's Guide | 2023 by Data Science Basics Use Large Language Models in Jupyter Notebook | LangChain | Agents & Indexes by Abhinaw Tiwari How to Talk to Your Langchain Agent | 11 Labs + Whisper by VRSEN LangChain Deep Dive: 5 FUN AI App Ideas To Build Quickly and Easily by James NoCode LangChain 101: Models by Mckay Wrigley LangChain with JavaScript Tutorial #1 | Setup & Using LLMs by Leon van Zyl LangChain Overview & Tutorial for Beginners: Build Powerful AI Apps Quickly & Easily (ZERO CODE) by James NoCode LangChain In Action: Real-World Use Case With Step-by-Step Tutorial by Rabbitmetrics Summarizing and Querying Multiple Papers with LangChain by Automata Learning Lab Using Langchain (and Replit ) through Tana , ask Google / Wikipedia / Wolfram Alpha to fill out a table by Stian Håklev Langchain PDF App (GUI) | Create a ChatGPT For Your PDF in Python by Alejandro AO - Software & Ai Auto-GPT with LangChain 🔥 | Create Your Own Personal AI Assistant by Data Science Basics Create Your OWN Slack AI Assistant with Python & LangChain by Dave Ebbelaar How to Create LOCAL Chatbots with GPT4All and LangChain [Full Guide] by Liam Ottley Build a Multilingual PDF Search App with LangChain, Cohere and Bubble by Menlo Park Lab Building a LangChain Agent (code-free!) Using Bubble and Flowise by Menlo Park Lab Build a LangChain-based Semantic PDF Search App with No-Code Tools Bubble and Flowise by Menlo Park Lab LangChain Memory Tutorial | Building a ChatGPT Clone in Python by Alejandro AO - Software & Ai ChatGPT For Your DATA | Chat with Multiple Documents Using LangChain by Data Science Basics Llama Index : Chat with Documentation using URL Loader by Merk Using OpenAI, LangChain, and Gradio to Build Custom GenAI Applications by David Hundley LangChain, Chroma DB, OpenAI Beginner Guide | ChatGPT with your PDF Build AI chatbot with custom knowledge base using OpenAI API and GPT Index by Irina Nik Build Your Own Auto-GPT Apps with LangChain (Python Tutorial) by Dave Ebbelaar Chat with Multiple PDFs | LangChain App Tutorial in Python (Free LLMs and Embeddings) by Alejandro AO - Software & Ai Chat with a CSV | LangChain Agents Tutorial (Beginners) by Alejandro AO - Software & Ai Create Your Own ChatGPT with PDF Data in 5 Minutes (LangChain Tutorial) by Liam Ottley Build a Custom Chatbot with OpenAI: GPT-Index & LangChain | Step-by-Step Tutorial by Fabrikod Flowise is an open-source no-code UI visual tool to build 🦜🔗LangChain applications by Cobus Greyling LangChain & GPT 4 For Data Analysis: The Pandas Dataframe Agent by Rabbitmetrics GirlfriendGPT - AI girlfriend with LangChain by Toolfinder AI How to build with Langchain 10x easier | ⛓️ LangFlow & Flowise by AI Jason Getting Started With LangChain In 20 Minutes- Build Celebrity Search Application by Krish Naik ⛓ Vector Embeddings Tutorial – Code Your Own AI Assistant with GPT-4 API + LangChain + NLP by FreeCodeCamp.org ⛓ Fully LOCAL Llama 2 Q&A with LangChain by 1littlecoder ⛓ Fully LOCAL Llama 2 Langchain on CPU by 1littlecoder ⛓ Build LangChain Audio Apps with Python in 5 Minutes by AssemblyAI ⛓ Voiceflow & Flowise : Want to Beat Competition? New Tutorial with Real AI Chatbot by AI SIMP ⛓ THIS Is How You Build Production-Ready AI Apps ( LangSmith Tutorial) by Dave Ebbelaar ⛓ Build POWERFUL LLM Bots EASILY with Your Own Data - Embedchain - Langchain 2.0? (Tutorial) by WorldofAI ⛓ Code Llama powered Gradio App for Coding: Runs on CPU by AI Anytime ⛓ LangChain Complete Course in One Video | Develop LangChain (AI) Based Solutions for Your Business by UBprogrammer ⛓ How to Run LLaMA Locally on CPU or GPU | Python & Langchain & CTransformers Guide by Code With Prince ⛓ PyData Heidelberg #11 - TimeSeries Forecasting & LLM Langchain by PyData ⛓ Prompt Engineering in Web Development | Using LangChain and Templates with OpenAI by Akamai Developer ⛓ Retrieval-Augmented Generation (RAG) using LangChain and Pinecone - The RAG Special Episode by Generative AI and Data Science On AWS ⛓ LLAMA2 70b-chat Multiple Documents Chatbot with Langchain & Streamlit |All OPEN SOURCE|Replicate API by DataInsightEdge ⛓ Chatting with 44K Fashion Products: LangChain Opportunities and Pitfalls by Rabbitmetrics ⛓ Structured Data Extraction from ChatGPT with LangChain by MG ⛓ Chat with Multiple PDFs using Llama 2 , Pinecone and LangChain (Free LLMs and Embeddings) by Muhammad Moin ⛓ Integrate Audio into LangChain.js apps in 5 Minutes by AssemblyAI ⛓ ChatGPT for your data with Local LLM by Jacob Jedryszek ⛓ Training Chatgpt with your personal data using langchain step by step in detail by NextGen Machines ⛓ Use ANY language in LangSmith with REST by Nerding I/O ⛓ How to Leverage the Full Potential of LLMs for Your Business with Langchain - Leon Ruddat by PyData ⛓ ChatCSV App: Chat with CSV files using LangChain and Llama 2 by Muhammad Moin Prompt Engineering and LangChain by Venelin Valkov ​ Getting Started with LangChain: Load Custom Data, Run OpenAI Models, Embeddings and ChatGPT Loaders, Indexes & Vectorstores in LangChain: Question Answering on PDF files with ChatGPT LangChain Models: ChatGPT , Flan Alpaca , OpenAI Embeddings , Prompt Templates & Streaming LangChain Chains: Use ChatGPT to Build Conversational Agents, Summaries and Q&A on Text With LLMs Analyze Custom CSV Data with GPT-4 using Langchain Build ChatGPT Chatbots with LangChain Memory: Understanding and Implementing Memory in Conversations ⛓ icon marks a new addition [last update 2023-09-21]"
    },
    {
        "title": "Get started | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/get_started",
        "text_content": ""
    },
    {
        "title": "Installation | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/get_started/installation",
        "text_content": "Get started Installation On this page Installation Official release ​ To install LangChain run: Pip Conda pip install langchain conda install langchain -c conda-forge This will install the bare minimum requirements of LangChain.\nA lot of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately. From source ​ If you want to install from source, you can do so by cloning the repo and be sure that the directory is PATH/TO/REPO/langchain/libs/langchain running: pip install -e . LangChain community ​ The langchain-community package contains third-party integrations. It is automatically installed by langchain , but can also be used separately. Install with: pip install langchain-community LangChain core ​ The langchain-core package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by langchain , but can also be used separately. Install with: pip install langchain-core LangChain experimental ​ The langchain-experimental package holds experimental LangChain code, intended for research and experimental uses.\nInstall with: pip install langchain-experimental LangServe ​ LangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with: pip install \"langserve[all]\" for both client and server dependencies. Or pip install \"langserve[client]\" for client code, and pip install \"langserve[server]\" for server code. LangChain CLI ​ The LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with: pip install langchain-cli LangSmith SDK ​ The LangSmith SDK is automatically installed by LangChain.\nIf not using LangChain, install with: pip install langsmith Previous Introduction Next Quickstart"
    },
    {
        "title": "Quickstart | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/get_started/quickstart",
        "text_content": "Get started Quickstart On this page Quickstart In this quickstart we'll show you how to: Get setup with LangChain, LangSmith and LangServe Use the most basic and common components of LangChain: prompt templates, models, and output parsers Use LangChain Expression Language, the protocol that LangChain is built on and which facilitates component chaining Build a simple application with LangChain Trace your application with LangSmith Serve your application with LangServe That's a fair amount to cover! Let's dive in. Setup ​ Installation ​ To install LangChain run: Pip Conda pip install langchain conda install langchain -c conda-forge For more details, see our Installation guide . Environment ​ Using LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs. First we'll need to install their Python package: pip install openai Accessing the API requires an API key, which you can get by creating an account and heading here . Once we have a key we'll want to set it as an environment variable by running: export OPENAI_API_KEY = \"...\" If you'd prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class: from langchain . chat_models import ChatOpenAI llm = ChatOpenAI ( openai_api_key = \"...\" ) LangSmith ​ Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\nThe best way to do this is with LangSmith . Note that LangSmith is not needed, but it is helpful.\nIf you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces: export LANGCHAIN_TRACING_V2 = \"true\" export LANGCHAIN_API_KEY = \"...\" LangServe ​ LangServe helps developers deploy LangChain chains as a REST API. You do not need to use LangServe to use LangChain, but in this guide we'll show how you can deploy your app with LangServe. Install with: pip install \"langserve[all]\" Building with LangChain ​ LangChain provides many modules that can be used to build language model applications.\nModules can be used as standalones in simple applications and they can be composed for more complex use cases.\nComposition is powered by LangChain Expression Language (LCEL), which defines a unified Runnable interface that many modules implement, making it possible to seamlessly chain components. The simplest and most common chain contains three things: LLM/Chat Model: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them. Prompt Template: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial. Output Parser: These translate the raw response from the language model to a more workable format, making it easy to use the output downstream. In this guide we'll cover those three components individually, and then go over how to combine them.\nUnderstanding these concepts will set you up well for being able to use and customize LangChain applications.\nMost LangChain applications allow you to configure the model and/or the prompt, so knowing how to take advantage of this will be a big enabler. LLM / Chat Model ​ There are two types of language models: LLM : underlying model takes a string as input and returns a string ChatModel : underlying model takes a list of messages as input and returns a message Strings are simple, but what exactly are messages? The base message interface is defined by BaseMessage , which has two required attributes: content : The content of the message. Usually a string. role : The entity from which the BaseMessage is coming. LangChain provides several objects to easily distinguish between different roles: HumanMessage : A BaseMessage coming from a human/user. AIMessage : A BaseMessage coming from an AI/assistant. SystemMessage : A BaseMessage coming from the system. FunctionMessage / ToolMessage : A BaseMessage containing the output of a function or tool call. If none of those roles sound right, there is also a ChatMessage class where you can specify the role manually. LangChain provides a common interface that's shared by both LLM s and ChatModel s.\nHowever it's useful to understand the difference in order to most effectively construct prompts for a given language model. The simplest way to call an LLM or ChatModel is using .invoke() , the universal synchronous call method for all LangChain Expression Language (LCEL) objects: LLM.invoke : Takes in a string, returns a string. ChatModel.invoke : Takes in a list of BaseMessage , returns a BaseMessage . The input types for these methods are actually more general than this, but for simplicity here we can assume LLMs only take strings and Chat models only takes lists of messages.\nCheck out the \"Go deeper\" section below to learn more about model invocation. Let's see how to work with these different types of models and these different types of inputs.\nFirst, let's import an LLM and a ChatModel. from langchain . llms import OpenAI from langchain . chat_models import ChatOpenAI llm = OpenAI ( ) chat_model = ChatOpenAI ( ) LLM and ChatModel objects are effectively configuration objects.\nYou can initialize them with parameters like temperature and others, and pass them around. from langchain . schema import HumanMessage text = \"What would be a good company name for a company that makes colorful socks?\" messages = [ HumanMessage ( content = text ) ] llm . invoke ( text ) # >> Feetful of Fun chat_model . invoke ( messages ) # >> AIMessage(content=\"Socks O'Color\") Go deeper LLM.invoke and ChatModel.invoke actually both support as input any of Union[str, List[BaseMessage], PromptValue] . PromptValue is an object that defines its own custom logic for returning its inputs either as a string or as messages. LLM s have logic for coercing any of these into a string, and ChatModel s have logic for coercing any of these to messages.\nThe fact that LLM and ChatModel accept the same inputs means that you can directly swap them for one another in most chains without breaking anything,\nthough it's of course important to think about how inputs are being coerced and how that may affect model performance.\nTo dive deeper on models head to the Language models section. Prompt templates ​ Most LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand. In the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it would be great if the user only had to provide the description of a company/product without worrying about giving the model instructions. PromptTemplates help with exactly this!\nThey bundle up all the logic for going from user input into a fully formatted prompt.\nThis can start off very simple - for example, a prompt to produce the above string would just be: from langchain . prompts import PromptTemplate prompt = PromptTemplate . from_template ( \"What is a good name for a company that makes {product}?\" ) prompt . format ( product = \"colorful socks\" ) What is a good name for a company that makes colorful socks? However, the advantages of using these over raw string formatting are several.\nYou can \"partial\" out variables - e.g. you can format only some of the variables at a time.\nYou can compose them together, easily combining different templates into a single prompt.\nFor explanations of these functionalities, see the section on prompts for more detail. PromptTemplate s can also be used to produce a list of messages.\nIn this case, the prompt not only contains information about the content, but also each message (its role, its position in the list, etc.).\nHere, what happens most often is a ChatPromptTemplate is a list of ChatMessageTemplates .\nEach ChatMessageTemplate contains instructions for how to format that ChatMessage - its role, and then also its content.\nLet's take a look at this below: from langchain . prompts . chat import ChatPromptTemplate template = \"You are a helpful assistant that translates {input_language} to {output_language}.\" human_template = \"{text}\" chat_prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , template ) , ( \"human\" , human_template ) , ] ) chat_prompt . format_messages ( input_language = \"English\" , output_language = \"French\" , text = \"I love programming.\" ) [ SystemMessage(content=\"You are a helpful assistant that translates English to French.\", additional_kwargs={}), HumanMessage(content=\"I love programming.\") ] ChatPromptTemplates can also be constructed in other ways - see the section on prompts for more detail. Output parsers ​ OutputParser s convert the raw output of a language model into a format that can be used downstream.\nThere are a few main types of OutputParser s, including: Convert text from LLM into structured information (e.g. JSON) Convert a ChatMessage into just a string Convert the extra information returned from a call besides the message (like OpenAI function invocation) into a string. For full information on this, see the section on output parsers . In this getting started guide, we will write our own output parser - one that converts a comma separated list into a list. from langchain . schema import BaseOutputParser class CommaSeparatedListOutputParser ( BaseOutputParser ) : \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\" def parse ( self , text : str ) : \"\"\"Parse the output of an LLM call.\"\"\" return text . strip ( ) . split ( \", \" ) CommaSeparatedListOutputParser ( ) . parse ( \"hi, bye\" ) # >> ['hi', 'bye'] Composing with LCEL ​ We can now combine all these into one chain.\nThis chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to a language model, and then pass the output through an (optional) output parser.\nThis is a convenient way to bundle up a modular piece of logic.\nLet's see it in action! from typing import List from langchain . chat_models import ChatOpenAI from langchain . prompts import ChatPromptTemplate from langchain . schema import BaseOutputParser class CommaSeparatedListOutputParser ( BaseOutputParser [ List [ str ] ] ) : \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\" def parse ( self , text : str ) - > List [ str ] : \"\"\"Parse the output of an LLM call.\"\"\" return text . strip ( ) . split ( \", \" ) template = \"\"\"You are a helpful assistant who generates comma separated lists. A user will pass in a category, and you should generate 5 objects in that category in a comma separated list. ONLY return a comma separated list, and nothing more.\"\"\" human_template = \"{text}\" chat_prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , template ) , ( \"human\" , human_template ) , ] ) chain = chat_prompt | ChatOpenAI ( ) | CommaSeparatedListOutputParser ( ) chain . invoke ( { \"text\" : \"colors\" } ) # >> ['red', 'blue', 'green', 'yellow', 'orange'] Note that we are using the | syntax to join these components together.\nThis | syntax is powered by the LangChain Expression Language (LCEL) and relies on the universal Runnable interface that all of these objects implement.\nTo learn more about LCEL, read the documentation here . Tracing with LangSmith ​ Assuming we've set our environment variables as shown in the beginning, all of the model and chain calls we've been making will have been automatically logged to LangSmith.\nOnce there, we can use LangSmith to debug and annotate our application traces, then turn them into datasets for evaluating future iterations of the application. Check out what the trace for the above chain would look like: https://smith.langchain.com/public/09370280-4330-4eb4-a7e8-c91817f6aa13/r For more on LangSmith head here . Serving with LangServe ​ Now that we've built an application, we need to serve it. That's where LangServe comes in.\nLangServe helps developers deploy LCEL chains as a REST API.\nThe library is integrated with FastAPI and uses pydantic for data validation. Server ​ To create a server for our application we'll make a serve.py file with three things: The definition of our chain (same as above) Our FastAPI app A definition of a route from which to serve the chain, which is done with langserve.add_routes #!/usr/bin/env python from typing import List from fastapi import FastAPI from langchain . prompts import ChatPromptTemplate from langchain . chat_models import ChatOpenAI from langchain . schema import BaseOutputParser from langserve import add_routes # 1. Chain definition class CommaSeparatedListOutputParser ( BaseOutputParser [ List [ str ] ] ) : \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\" def parse ( self , text : str ) - > List [ str ] : \"\"\"Parse the output of an LLM call.\"\"\" return text . strip ( ) . split ( \", \" ) template = \"\"\"You are a helpful assistant who generates comma separated lists. A user will pass in a category, and you should generate 5 objects in that category in a comma separated list. ONLY return a comma separated list, and nothing more.\"\"\" human_template = \"{text}\" chat_prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , template ) , ( \"human\" , human_template ) , ] ) category_chain = chat_prompt | ChatOpenAI ( ) | CommaSeparatedListOutputParser ( ) # 2. App definition app = FastAPI ( title = \"LangChain Server\" , version = \"1.0\" , description = \"A simple API server using LangChain's Runnable interfaces\" , ) # 3. Adding chain route add_routes ( app , category_chain , path = \"/category_chain\" , ) if __name__ == \"__main__\" : import uvicorn uvicorn . run ( app , host = \"localhost\" , port = 8000 ) And that's it! If we execute this file: python serve.py we should see our chain being served at localhost:8000. Playground ​ Every LangServe service comes with a simple built-in UI for configuring and invoking the application with streaming output and visibility into intermediate steps.\nHead to http://localhost:8000/category_chain/playground/ to try it out! Client ​ Now let's set up a client for programmatically interacting with our service. We can easily do this with the langserve.RemoteRunnable .\nUsing this, we can interact with the served chain as if it were running client-side. from langserve import RemoteRunnable remote_chain = RemoteRunnable ( \"http://localhost:8000/category_chain/\" ) remote_chain . invoke ( { \"text\" : \"colors\" } ) # >> ['red', 'blue', 'green', 'yellow', 'orange'] To learn more about the many other features of LangServe head here . Next steps ​ We've touched on how to build an application with LangChain, how to trace it with LangSmith, and how to serve it with LangServe.\nThere are a lot more features in all three of these than we can cover here.\nTo continue on your journey: Read up on LangChain Expression Language (LCEL) to learn how to chain these components together Dive deeper into LLMs, prompts, and output parsers and learn the other key components Explore common end-to-end use cases and template applications Read up on LangSmith , the platform for debugging, testing, monitoring and more Learn more about serving your applications with LangServe Previous Installation Next Security"
    },
    {
        "title": "Security | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/security",
        "text_content": "Get started Security On this page Security LangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources. Best Practices ​ When building such applications developers should remember to follow good security practices: Limit Permissions : Scope permissions specifically to the application's need. Granting broad or excessive permissions can introduce significant security vulnerabilities. To avoid such vulnerabilities, consider using read-only credentials, disallowing access to sensitive resources, using sandboxing techniques (such as running inside a container), etc. as appropriate for your application. Anticipate Potential Misuse : Just as humans can err, so can Large Language Models (LLMs). Always assume that any system access or credentials may be used in any way allowed by the permissions they are assigned. For example, if a pair of database credentials allows deleting data, it’s safest to assume that any LLM able to use those credentials may in fact delete data. Defense in Depth : No security technique is perfect. Fine-tuning and good chain design can reduce, but not eliminate, the odds that a Large Language Model (LLM) may make a mistake. It’s best to combine multiple layered security approaches rather than relying on any single layer of defense to ensure security. For example: use both read-only permissions and sandboxing to ensure that LLMs are only able to access data that is explicitly meant for them to use. Risks of not doing so include, but are not limited to: Data corruption or loss. Unauthorized access to confidential information. Compromised performance or availability of critical resources. Example scenarios with mitigation strategies: A user may ask an agent with access to the file system to delete files that should not be deleted or read the content of files that contain sensitive information. To mitigate, limit the agent to only use a specific directory and only allow it to read or write files that are safe to read or write. Consider further sandboxing the agent by running it in a container. A user may ask an agent with write access to an external API to write malicious data to the API, or delete data from that API. To mitigate, give the agent read-only API keys, or limit it to only use endpoints that are already resistant to such misuse. A user may ask an agent with access to a database to drop a table or mutate the schema. To mitigate, scope the credentials to only the tables that the agent needs to access and consider issuing READ-ONLY credentials. If you're building applications that access external resources like file systems, APIs\nor databases, consider speaking with your company's security team to determine how to best\ndesign and secure your applications. Reporting a Vulnerability ​ Please report security vulnerabilities by email to security@langchain.dev. This will ensure the issue is promptly triaged and acted upon as needed. Enterprise solutions ​ LangChain may offer enterprise solutions for customers who have additional security\nrequirements. Please contact us at sales@langchain.dev . Previous Quickstart Next LangChain Expression Language (LCEL)"
    },
    {
        "title": "LangChain Expression Language (LCEL) | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/",
        "text_content": "LangChain Expression Language LangChain Expression Language (LCEL) LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.\nLCEL was designed from day 1 to support putting prototypes in production, with no code changes , from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL: Streaming support When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens. Async support Any chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server. Optimized parallel execution Whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency. Retries and fallbacks Configure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost. Access intermediate results For more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every LangServe server. Input and output schemas Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe. Seamless LangSmith tracing integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability. Seamless LangServe deployment integration Any chain created with LCEL can be easily deployed using LangServe . Previous Security Next Get started"
    },
    {
        "title": "Get started | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/get_started",
        "text_content": "LangChain Expression Language Get started On this page Get started LCEL makes it easy to build complex chains from basic components, and\nsupports out of the box functionality such as streaming, parallelism,\nand logging. Basic example: prompt + model + output parser ​ The most basic and common use case is chaining a prompt template and a\nmodel together. To see how this works, let’s create a chain that takes a\ntopic and generates a joke: from langchain . chat_models import ChatOpenAI from langchain . prompts import ChatPromptTemplate from langchain_core . output_parsers import StrOutputParser prompt = ChatPromptTemplate . from_template ( \"tell me a short joke about {topic}\" ) model = ChatOpenAI ( ) output_parser = StrOutputParser ( ) chain = prompt | model | output_parser chain . invoke ( { \"topic\" : \"ice cream\" } ) \"Why did the ice cream go to therapy?\\n\\nBecause it had too many toppings and couldn't find its cone-fidence!\" Notice this line of this code, where we piece together then different\ncomponents into a single chain using LCEL: chain = prompt | model | output_parser The | symbol is similar to a unix pipe\noperator , which chains\ntogether the different components feeds the output from one component as\ninput into the next component. In this chain the user input is passed to the prompt template, then the\nprompt template output is passed to the model, then the model output is\npassed to the output parser. Let’s take a look at each component\nindividually to really understand what’s going on. 1. Prompt ​ prompt is a BasePromptTemplate , which means it takes in a dictionary\nof template variables and produces a PromptValue . A PromptValue is a\nwrapper around a completed prompt that can be passed to either an LLM (which takes a string as input) or ChatModel (which takes a sequence\nof messages as input). It can work with either language model type\nbecause it defines logic both for producing BaseMessage s and for\nproducing a string. prompt_value = prompt . invoke ( { \"topic\" : \"ice cream\" } ) prompt_value ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')]) prompt_value . to_messages ( ) [HumanMessage(content='tell me a short joke about ice cream')] prompt_value . to_string ( ) 'Human: tell me a short joke about ice cream' 2. Model ​ The PromptValue is then passed to model . In this case our model is\na ChatModel , meaning it will output a BaseMessage . message = model . invoke ( prompt_value ) message AIMessage(content=\"Why did the ice cream go to therapy? \\n\\nBecause it had too many toppings and couldn't find its cone-fidence!\") If our model was an LLM , it would output a string. from langchain . llms import OpenAI llm = OpenAI ( model = \"gpt-3.5-turbo-instruct\" ) llm . invoke ( prompt_value ) '\\n\\nRobot: Why did the ice cream go to therapy? Because it had a rocky road.' 3. Output parser ​ And lastly we pass our model output to the output_parser , which is a BaseOutputParser meaning it takes either a string or a BaseMessage as input. The StrOutputParser specifically simple converts any input\ninto a string. output_parser . invoke ( message ) \"Why did the ice cream go to therapy? \\n\\nBecause it had too many toppings and couldn't find its cone-fidence!\" 4. Entire Pipeline ​ To follow the steps along: We pass in user input on the desired topic as {\"topic\": \"ice cream\"} The prompt component takes the user input, which is then used to\nconstruct a PromptValue after using the topic to construct the\nprompt. The model component takes the generated prompt, and passes into\nthe OpenAI LLM model for evaluation. The generated output from the\nmodel is a ChatMessage object. Finally, the output_parser component takes in a ChatMessage , and\ntransforms this into a Python string, which is returned from the\ninvoke method. Note that if you’re curious about the output of any components, you can\nalways test out a smaller version of the chain such as prompt or prompt | model to see the intermediate results: input = { \"topic\" : \"ice cream\" } prompt . invoke ( input ) # > ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')]) ( prompt | model ) . invoke ( input ) # > AIMessage(content=\"Why did the ice cream go to therapy?\\nBecause it had too many toppings and couldn't cone-trol itself!\") RAG Search Example ​ For our next example, we want to run a retrieval-augmented generation\nchain to add some context when responding to questions. # Requires: # pip install langchain docarray tiktoken from langchain . chat_models import ChatOpenAI from langchain . embeddings import OpenAIEmbeddings from langchain . prompts import ChatPromptTemplate from langchain . vectorstores import DocArrayInMemorySearch from langchain_core . output_parsers import StrOutputParser from langchain_core . runnables import RunnableParallel , RunnablePassthrough vectorstore = DocArrayInMemorySearch . from_texts ( [ \"harrison worked at kensho\" , \"bears like to eat honey\" ] , embedding = OpenAIEmbeddings ( ) , ) retriever = vectorstore . as_retriever ( ) template = \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\" prompt = ChatPromptTemplate . from_template ( template ) model = ChatOpenAI ( ) output_parser = StrOutputParser ( ) setup_and_retrieval = RunnableParallel ( { \"context\" : retriever , \"question\" : RunnablePassthrough ( ) } ) chain = setup_and_retrieval | prompt | model | output_parser chain . invoke ( \"where did harrison work?\" ) In this case, the composed chain is: chain = setup_and_retrieval | prompt | model | output_parser To explain this, we first can see that the prompt template above takes\nin context and question as values to be substituted in the prompt.\nBefore building the prompt template, we want to retrieve relevant\ndocuments to the search and include them as part of the context. As a preliminary step, we’ve setup the retriever using an in memory\nstore, which can retrieve documents based on a query. This is a runnable\ncomponent as well that can be chained together with other components,\nbut you can also try to run it separately: retriever . invoke ( \"where did harrison work?\" ) We then use the RunnableParallel to prepare the expected inputs into\nthe prompt by using the entries for the retrieved documents as well as\nthe original user question, using the retriever for document search, and\nRunnablePassthrough to pass the user’s question: setup_and_retrieval = RunnableParallel ( { \"context\" : retriever , \"question\" : RunnablePassthrough ( ) } ) To review, the complete chain is: setup_and_retrieval = RunnableParallel ( { \"context\" : retriever , \"question\" : RunnablePassthrough ( ) } ) chain = setup_and_retrieval | prompt | model | output_parser With the flow being: The first steps create a RunnableParallel object with two entries.\nThe first entry, context will include the document results fetched\nby the retriever. The second entry, question will contain the\nuser’s original question. To pass on the question, we use RunnablePassthrough to copy this entry. Feed the dictionary from the step above to the prompt component.\nIt then takes the user input which is question as well as the\nretrieved document which is context to construct a prompt and\noutput a PromptValue. The model component takes the generated prompt, and passes into\nthe OpenAI LLM model for evaluation. The generated output from the\nmodel is a ChatMessage object. Finally, the output_parser component takes in a ChatMessage , and\ntransforms this into a Python string, which is returned from the\ninvoke method. Next steps ​ We recommend reading our Why use LCEL section next to see a side-by-side comparison of the code needed to\nproduce common functionality with and without LCEL. Previous LangChain Expression Language (LCEL) Next Why use LCEL"
    },
    {
        "title": "Why use LCEL | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/why",
        "text_content": "LangChain Expression Language Why use LCEL On this page Why use LCEL We recommend reading the LCEL Get\nstarted section first. LCEL makes it easy to build complex chains from basic components. It\ndoes this by providing: 1. A unified interface : Every LCEL object\nimplements the Runnable interface, which defines a common set of\ninvocation methods ( invoke , batch , stream , ainvoke , …). This\nmakes it possible for chains of LCEL objects to also automatically\nsupport these invocations. That is, every chain of LCEL objects is\nitself an LCEL object. 2. Composition primitives : LCEL provides a\nnumber of primitives that make it easy to compose chains, parallelize\ncomponents, add fallbacks, dynamically configure chain internal, and\nmore. To better understand the value of LCEL, it’s helpful to see it in action\nand think about how we might recreate similar functionality without it.\nIn this walkthrough we’ll do just that with our basic\nexample from the\nget started section. We’ll take our simple prompt + model chain, which\nunder the hood already defines a lot of functionality, and see what it\nwould take to recreate all of it. from langchain . chat_models import ChatOpenAI from langchain . prompts import ChatPromptTemplate from langchain_core . output_parsers import StrOutputParser prompt = ChatPromptTemplate . from_template ( \"Tell me a short joke about {topic}\" ) model = ChatOpenAI ( model = \"gpt-3.5-turbo\" ) output_parser = StrOutputParser ( ) chain = prompt | model | output_parser Invoke ​ In the simplest case, we just want to pass in a topic string and get\nback a joke string: Without LCEL ​ from typing import List import openai prompt_template = \"Tell me a short joke about {topic}\" client = openai . OpenAI ( ) def call_chat_model ( messages : List [ dict ] ) - > str : response = client . chat . completions . create ( model = \"gpt-3.5-turbo\" , messages = messages , ) return response . choices [ 0 ] . message . content def invoke_chain ( topic : str ) - > str : prompt_value = prompt_template . format ( topic = topic ) messages = [ { \"role\" : \"user\" , \"content\" : prompt_value } ] return call_chat_model ( messages ) invoke_chain ( \"ice cream\" ) LCEL ​ from langchain_core . runnables import RunnablePassthrough prompt = ChatPromptTemplate . from_template ( \"Tell me a short joke about {topic}\" ) output_parser = StrOutputParser ( ) model = ChatOpenAI ( model = \"gpt-3.5-turbo\" ) chain = ( { \"topic\" : RunnablePassthrough ( ) } | prompt | model | output_parser ) chain . invoke ( \"ice cream\" ) Stream ​ If we want to stream results instead, we’ll need to change our function: Without LCEL ​ from typing import Iterator def stream_chat_model ( messages : List [ dict ] ) - > Iterator [ str ] : stream = client . chat . completions . create ( model = \"gpt-3.5-turbo\" , messages = messages , stream = True , ) for response in stream : content = response . choices [ 0 ] . delta . content if content is not None : yield content def stream_chain ( topic : str ) - > Iterator [ str ] : prompt_value = prompt . format ( topic = topic ) return stream_chat_model ( [ { \"role\" : \"user\" , \"content\" : prompt_value } ] ) for chunk in stream_chain ( \"ice cream\" ) : print ( chunk , end = \"\" , flush = True ) LCEL ​ for chunk in chain . stream ( \"ice cream\" ) : print ( chunk , end = \"\" , flush = True ) Batch ​ If we want to run on a batch of inputs in parallel, we’ll again need a\nnew function: Without LCEL ​ from concurrent . futures import ThreadPoolExecutor def batch_chain ( topics : list ) - > list : with ThreadPoolExecutor ( max_workers = 5 ) as executor : return list ( executor . map ( invoke_chain , topics ) ) batch_chain ( [ \"ice cream\" , \"spaghetti\" , \"dumplings\" ] ) LCEL ​ chain . batch ( [ \"ice cream\" , \"spaghetti\" , \"dumplings\" ] ) Async ​ If we need an asynchronous version: Without LCEL ​ async_client = openai . AsyncOpenAI ( ) async def acall_chat_model ( messages : List [ dict ] ) - > str : response = await async_client . chat . completions . create ( model = \"gpt-3.5-turbo\" , messages = messages , ) return response . choices [ 0 ] . message . content async def ainvoke_chain ( topic : str ) - > str : prompt_value = prompt_template . format ( topic = topic ) messages = [ { \"role\" : \"user\" , \"content\" : prompt_value } ] return await acall_chat_model ( messages ) await ainvoke_chain ( \"ice cream\" ) LCEL ​ chain . ainvoke ( \"ice cream\" ) LLM instead of chat model ​ If we want to use a completion endpoint instead of a chat endpoint: Without LCEL ​ def call_llm ( prompt_value : str ) - > str : response = client . completions . create ( model = \"gpt-3.5-turbo-instruct\" , prompt = prompt_value , ) return response . choices [ 0 ] . text def invoke_llm_chain ( topic : str ) - > str : prompt_value = prompt_template . format ( topic = topic ) return call_llm ( prompt_value ) invoke_llm_chain ( \"ice cream\" ) LCEL ​ from langchain . llms import OpenAI llm = OpenAI ( model = \"gpt-3.5-turbo-instruct\" ) llm_chain = ( { \"topic\" : RunnablePassthrough ( ) } | prompt | llm | output_parser ) llm_chain . invoke ( \"ice cream\" ) Different model provider ​ If we want to use Anthropic instead of OpenAI: Without LCEL ​ import anthropic anthropic_template = f\"Human:\\n\\n { prompt_template } \\n\\nAssistant:\" anthropic_client = anthropic . Anthropic ( ) def call_anthropic ( prompt_value : str ) - > str : response = anthropic_client . completions . create ( model = \"claude-2\" , prompt = prompt_value , max_tokens_to_sample = 256 , ) return response . completion def invoke_anthropic_chain ( topic : str ) - > str : prompt_value = anthropic_template . format ( topic = topic ) return call_anthropic ( prompt_value ) invoke_anthropic_chain ( \"ice cream\" ) LCEL ​ from langchain . chat_models import ChatAnthropic anthropic = ChatAnthropic ( model = \"claude-2\" ) anthropic_chain = ( { \"topic\" : RunnablePassthrough ( ) } | prompt | anthropic | output_parser ) anthropic_chain . invoke ( \"ice cream\" ) Runtime configurability ​ If we wanted to make the choice of chat model or LLM configurable at\nruntime: Without LCEL ​ def invoke_configurable_chain ( topic : str , * , model : str = \"chat_openai\" ) - > str : if model == \"chat_openai\" : return invoke_chain ( topic ) elif model == \"openai\" : return invoke_llm_chain ( topic ) elif model == \"anthropic\" : return invoke_anthropic_chain ( topic ) else : raise ValueError ( f\"Received invalid model ' { model } '.\" \" Expected one of chat_openai, openai, anthropic\" ) def stream_configurable_chain ( topic : str , * , model : str = \"chat_openai\" ) - > Iterator [ str ] : if model == \"chat_openai\" : return stream_chain ( topic ) elif model == \"openai\" : # Note we haven't implemented this yet. return stream_llm_chain ( topic ) elif model == \"anthropic\" : # Note we haven't implemented this yet return stream_anthropic_chain ( topic ) else : raise ValueError ( f\"Received invalid model ' { model } '.\" \" Expected one of chat_openai, openai, anthropic\" ) def batch_configurable_chain ( topics : List [ str ] , * , model : str = \"chat_openai\" ) - > List [ str ] : # You get the idea . . . async def abatch_configurable_chain ( topics : List [ str ] , * , model : str = \"chat_openai\" ) - > List [ str ] : . . . invoke_configurable_chain ( \"ice cream\" , model = \"openai\" ) stream = stream_configurable_chain ( \"ice_cream\" , model = \"anthropic\" ) for chunk in stream : print ( chunk , end = \"\" , flush = True ) # batch_configurable_chain([\"ice cream\", \"spaghetti\", \"dumplings\"]) # await ainvoke_configurable_chain(\"ice cream\") With LCEL ​ from langchain_core . runnables import ConfigurableField configurable_model = model . configurable_alternatives ( ConfigurableField ( id = \"model\" ) , default_key = \"chat_openai\" , openai = llm , anthropic = anthropic , ) configurable_chain = ( { \"topic\" : RunnablePassthrough ( ) } | prompt | configurable_model | output_parser ) configurable_chain . invoke ( \"ice cream\" , config = { \"model\" : \"openai\" } ) stream = configurable_chain . stream ( \"ice cream\" , config = { \"model\" : \"anthropic\" } ) for chunk in stream : print ( chunk , end = \"\" , flush = True ) configurable_chain . batch ( [ \"ice cream\" , \"spaghetti\" , \"dumplings\" ] ) # await configurable_chain.ainvoke(\"ice cream\") Logging ​ If we want to log our intermediate results: Without LCEL ​ We’ll print intermediate steps for illustrative purposes def invoke_anthropic_chain_with_logging ( topic : str ) - > str : print ( f\"Input: { topic } \" ) prompt_value = anthropic_template . format ( topic = topic ) print ( f\"Formatted prompt: { prompt_value } \" ) output = call_anthropic ( prompt_value ) print ( f\"Output: { output } \" ) return output invoke_anthropic_chain_with_logging ( \"ice cream\" ) LCEL ​ Every component has built-in integrations with LangSmith. If we set the\nfollowing two environment variables, all chain traces are logged to\nLangSmith. import os os . environ [ \"LANGCHAIN_API_KEY\" ] = \"...\" os . environ [ \"LANGCHAIN_TRACING_V2\" ] = \"true\" anthropic_chain . invoke ( \"ice cream\" ) Here’s what our LangSmith trace looks like: https://smith.langchain.com/public/e4de52f8-bcd9-4732-b950-deee4b04e313/r Fallbacks ​ If we wanted to add fallback logic, in case one model API is down: Without LCEL ​ def invoke_chain_with_fallback ( topic : str ) - > str : try : return invoke_chain ( topic ) except Exception : return invoke_anthropic_chain ( topic ) async def ainvoke_chain_with_fallback ( topic : str ) - > str : try : return await ainvoke_chain ( topic ) except Exception : # Note: we haven't actually implemented this. return ainvoke_anthropic_chain ( topic ) async def batch_chain_with_fallback ( topics : List [ str ] ) - > str : try : return batch_chain ( topics ) except Exception : # Note: we haven't actually implemented this. return batch_anthropic_chain ( topics ) invoke_chain_with_fallback ( \"ice cream\" ) # await ainvoke_chain_with_fallback(\"ice cream\") batch_chain_with_fallback ( [ \"ice cream\" , \"spaghetti\" , \"dumplings\" ] ) ) LCEL ​ fallback_chain = chain . with_fallbacks ( [ anthropic_chain ] ) fallback_chain . invoke ( \"ice cream\" ) # await fallback_chain.ainvoke(\"ice cream\") fallback_chain . batch ( [ \"ice cream\" , \"spaghetti\" , \"dumplings\" ] ) Full code comparison ​ Even in this simple case, our LCEL chain succinctly packs in a lot of\nfunctionality. As chains become more complex, this becomes especially\nvaluable. Without LCEL ​ from concurrent . futures import ThreadPoolExecutor from typing import Iterator , List , Tuple import anthropic import openai prompt_template = \"Tell me a short joke about {topic}\" anthropic_template = f\"Human:\\n\\n { prompt_template } \\n\\nAssistant:\" client = openai . OpenAI ( ) async_client = openai . AsyncOpenAI ( ) anthropic_client = anthropic . Anthropic ( ) def call_chat_model ( messages : List [ dict ] ) - > str : response = client . chat . completions . create ( model = \"gpt-3.5-turbo\" , messages = messages , ) return response . choices [ 0 ] . message . content def invoke_chain ( topic : str ) - > str : print ( f\"Input: { topic } \" ) prompt_value = prompt_template . format ( topic = topic ) print ( f\"Formatted prompt: { prompt_value } \" ) messages = [ { \"role\" : \"user\" , \"content\" : prompt_value } ] output = call_chat_model ( messages ) print ( f\"Output: { output } \" ) return output def stream_chat_model ( messages : List [ dict ] ) - > Iterator [ str ] : stream = client . chat . completions . create ( model = \"gpt-3.5-turbo\" , messages = messages , stream = True , ) for response in stream : content = response . choices [ 0 ] . delta . content if content is not None : yield content def stream_chain ( topic : str ) - > Iterator [ str ] : print ( f\"Input: { topic } \" ) prompt_value = prompt . format ( topic = topic ) print ( f\"Formatted prompt: { prompt_value } \" ) stream = stream_chat_model ( [ { \"role\" : \"user\" , \"content\" : prompt_value } ] ) for chunk in stream : print ( f\"Token: { chunk } \" , end = \"\" ) yield chunk def batch_chain ( topics : list ) - > list : with ThreadPoolExecutor ( max_workers = 5 ) as executor : return list ( executor . map ( invoke_chain , topics ) ) def call_llm ( prompt_value : str ) - > str : response = client . completions . create ( model = \"gpt-3.5-turbo-instruct\" , prompt = prompt_value , ) return response . choices [ 0 ] . text def invoke_llm_chain ( topic : str ) - > str : print ( f\"Input: { topic } \" ) prompt_value = promtp_template . format ( topic = topic ) print ( f\"Formatted prompt: { prompt_value } \" ) output = call_llm ( prompt_value ) print ( f\"Output: { output } \" ) return output def call_anthropic ( prompt_value : str ) - > str : response = anthropic_client . completions . create ( model = \"claude-2\" , prompt = prompt_value , max_tokens_to_sample = 256 , ) return response . completion def invoke_anthropic_chain ( topic : str ) - > str : print ( f\"Input: { topic } \" ) prompt_value = anthropic_template . format ( topic = topic ) print ( f\"Formatted prompt: { prompt_value } \" ) output = call_anthropic ( prompt_value ) print ( f\"Output: { output } \" ) return output async def ainvoke_anthropic_chain ( topic : str ) - > str : . . . def stream_anthropic_chain ( topic : str ) - > Iterator [ str ] : . . . def batch_anthropic_chain ( topics : List [ str ] ) - > List [ str ] : . . . def invoke_configurable_chain ( topic : str , * , model : str = \"chat_openai\" ) - > str : if model == \"chat_openai\" : return invoke_chain ( topic ) elif model == \"openai\" : return invoke_llm_chain ( topic ) elif model == \"anthropic\" : return invoke_anthropic_chain ( topic ) else : raise ValueError ( f\"Received invalid model ' { model } '.\" \" Expected one of chat_openai, openai, anthropic\" ) def stream_configurable_chain ( topic : str , * , model : str = \"chat_openai\" ) - > Iterator [ str ] : if model == \"chat_openai\" : return stream_chain ( topic ) elif model == \"openai\" : # Note we haven't implemented this yet. return stream_llm_chain ( topic ) elif model == \"anthropic\" : # Note we haven't implemented this yet return stream_anthropic_chain ( topic ) else : raise ValueError ( f\"Received invalid model ' { model } '.\" \" Expected one of chat_openai, openai, anthropic\" ) def batch_configurable_chain ( topics : List [ str ] , * , model : str = \"chat_openai\" ) - > List [ str ] : . . . async def abatch_configurable_chain ( topics : List [ str ] , * , model : str = \"chat_openai\" ) - > List [ str ] : . . . def invoke_chain_with_fallback ( topic : str ) - > str : try : return invoke_chain ( topic ) except Exception : return invoke_anthropic_chain ( topic ) async def ainvoke_chain_with_fallback ( topic : str ) - > str : try : return await ainvoke_chain ( topic ) except Exception : return ainvoke_anthropic_chain ( topic ) async def batch_chain_with_fallback ( topics : List [ str ] ) - > str : try : return batch_chain ( topics ) except Exception : return batch_anthropic_chain ( topics ) LCEL ​ import os from langchain . chat_models import ChatAnthropic , ChatOpenAI from langchain . llms import OpenAI from langchain_core . output_parsers import StrOutputParser from langchain_core . prompts import ChatPromptTemplate from langchain_core . runnables import RunnablePassthrough os . environ [ \"LANGCHAIN_API_KEY\" ] = \"...\" os . environ [ \"LANGCHAIN_TRACING_V2\" ] = \"true\" prompt = ChatPromptTemplate . from_template ( \"Tell me a short joke about {topic}\" ) chat_openai = ChatOpenAI ( model = \"gpt-3.5-turbo\" ) openai = OpenAI ( model = \"gpt-3.5-turbo-instruct\" ) anthropic = ChatAnthropic ( model = \"claude-2\" ) model = ( chat_openai . with_fallbacks ( [ anthropic ] ) . configurable_alternatives ( ConfigurableField ( id = \"model\" ) , default_key = \"chat_openai\" , openai = openai , anthropic = anthropic , ) ) chain = ( { \"topic\" : RunnablePassthrough ( ) } | prompt | model | StrOutputParser ( ) ) Next steps ​ To continue learning about LCEL, we recommend: - Reading up on the full\nLCEL Interface , which we’ve only\npartially covered here. - Exploring the How-to section to learn about\nadditional composition primitives that LCEL provides. - Looking through\nthe Cookbook section to see LCEL\nin action for common use cases. A good next use case to look at would be Retrieval-augmented\ngeneration . Previous Get started Next Interface"
    },
    {
        "title": "Interface | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/interface",
        "text_content": "LangChain Expression Language Interface On this page Interface To make it as easy as possible to create custom chains, we’ve\nimplemented a “Runnable” protocol. The Runnable protocol is implemented for most components.\nThis is a standard interface, which makes it easy to define custom\nchains as well as invoke them in a standard way. The standard interface\nincludes: stream : stream back chunks of the response invoke : call the chain on an input batch : call the chain on a list of inputs These also have corresponding async methods: astream : stream back chunks of the response async ainvoke : call the chain on an input async abatch : call the chain on a list of inputs async astream_log : stream back\nintermediate steps as they happen, in addition to the final response The input type and output type varies by component: Component Input Type Output Type Prompt Dictionary PromptValue ChatModel Single string, list of chat messages or a PromptValue ChatMessage LLM Single string, list of chat messages or a PromptValue String OutputParser The output of an LLM or ChatModel Depends on the parser Retriever Single string List of Documents Tool Single string or dictionary, depending on the tool Depends on the tool All runnables expose input and output schemas to inspect the inputs\nand outputs: - input_schema : an input Pydantic model\nauto-generated from the structure of the Runnable - output_schema : an output Pydantic model\nauto-generated from the structure of the Runnable Let’s take a look at these methods. To do so, we’ll create a super\nsimple PromptTemplate + ChatModel chain. from langchain . chat_models import ChatOpenAI from langchain . prompts import ChatPromptTemplate model = ChatOpenAI ( ) prompt = ChatPromptTemplate . from_template ( \"tell me a joke about {topic}\" ) chain = prompt | model Input Schema ​ A description of the inputs accepted by a Runnable. This is a Pydantic\nmodel dynamically generated from the structure of any Runnable. You can\ncall .schema() on it to obtain a JSONSchema representation. # The input schema of the chain is the input schema of its first part, the prompt. chain . input_schema . schema ( ) {'title': 'PromptInput', 'type': 'object', 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}} prompt . input_schema . schema ( ) {'title': 'PromptInput', 'type': 'object', 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}} model . input_schema . schema ( ) {'title': 'ChatOpenAIInput', 'anyOf': [{'type': 'string'}, {'$ref': '#/definitions/StringPromptValue'}, {'$ref': '#/definitions/ChatPromptValueConcrete'}, {'type': 'array', 'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'}, {'$ref': '#/definitions/HumanMessage'}, {'$ref': '#/definitions/ChatMessage'}, {'$ref': '#/definitions/SystemMessage'}, {'$ref': '#/definitions/FunctionMessage'}]}}], 'definitions': {'StringPromptValue': {'title': 'StringPromptValue', 'description': 'String prompt value.', 'type': 'object', 'properties': {'text': {'title': 'Text', 'type': 'string'}, 'type': {'title': 'Type', 'default': 'StringPromptValue', 'enum': ['StringPromptValue'], 'type': 'string'}}, 'required': ['text']}, 'AIMessage': {'title': 'AIMessage', 'description': 'A Message from an AI.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'type': 'string'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'ai', 'enum': ['ai'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'HumanMessage': {'title': 'HumanMessage', 'description': 'A Message from a human.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'type': 'string'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'human', 'enum': ['human'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'ChatMessage': {'title': 'ChatMessage', 'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).', 'type': 'object', 'properties': {'content': {'title': 'Content', 'type': 'string'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'chat', 'enum': ['chat'], 'type': 'string'}, 'role': {'title': 'Role', 'type': 'string'}}, 'required': ['content', 'role']}, 'SystemMessage': {'title': 'SystemMessage', 'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'type': 'string'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'system', 'enum': ['system'], 'type': 'string'}}, 'required': ['content']}, 'FunctionMessage': {'title': 'FunctionMessage', 'description': 'A Message for passing the result of executing a function back to a model.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'type': 'string'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'function', 'enum': ['function'], 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['content', 'name']}, 'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete', 'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.', 'type': 'object', 'properties': {'messages': {'title': 'Messages', 'type': 'array', 'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'}, {'$ref': '#/definitions/HumanMessage'}, {'$ref': '#/definitions/ChatMessage'}, {'$ref': '#/definitions/SystemMessage'}, {'$ref': '#/definitions/FunctionMessage'}]}}, 'type': {'title': 'Type', 'default': 'ChatPromptValueConcrete', 'enum': ['ChatPromptValueConcrete'], 'type': 'string'}}, 'required': ['messages']}}} Output Schema ​ A description of the outputs produced by a Runnable. This is a Pydantic\nmodel dynamically generated from the structure of any Runnable. You can\ncall .schema() on it to obtain a JSONSchema representation. # The output schema of the chain is the output schema of its last part, in this case a ChatModel, which outputs a ChatMessage chain . output_schema . schema ( ) {'title': 'ChatOpenAIOutput', 'anyOf': [{'$ref': '#/definitions/HumanMessage'}, {'$ref': '#/definitions/AIMessage'}, {'$ref': '#/definitions/ChatMessage'}, {'$ref': '#/definitions/FunctionMessage'}, {'$ref': '#/definitions/SystemMessage'}], 'definitions': {'HumanMessage': {'title': 'HumanMessage', 'description': 'A Message from a human.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'type': 'string'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'human', 'enum': ['human'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'AIMessage': {'title': 'AIMessage', 'description': 'A Message from an AI.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'type': 'string'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'ai', 'enum': ['ai'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'ChatMessage': {'title': 'ChatMessage', 'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).', 'type': 'object', 'properties': {'content': {'title': 'Content', 'type': 'string'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'chat', 'enum': ['chat'], 'type': 'string'}, 'role': {'title': 'Role', 'type': 'string'}}, 'required': ['content', 'role']}, 'FunctionMessage': {'title': 'FunctionMessage', 'description': 'A Message for passing the result of executing a function back to a model.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'type': 'string'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'function', 'enum': ['function'], 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['content', 'name']}, 'SystemMessage': {'title': 'SystemMessage', 'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.', 'type': 'object', 'properties': {'content': {'title': 'Content', 'type': 'string'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'system', 'enum': ['system'], 'type': 'string'}}, 'required': ['content']}}} Stream ​ for s in chain . stream ( { \"topic\" : \"bears\" } ) : print ( s . content , end = \"\" , flush = True ) Why don't bears wear shoes? Because they already have bear feet! Invoke ​ chain . invoke ( { \"topic\" : \"bears\" } ) AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\") Batch ​ chain . batch ( [ { \"topic\" : \"bears\" } , { \"topic\" : \"cats\" } ] ) [AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\"), AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\")] You can set the number of concurrent requests by using the max_concurrency parameter chain . batch ( [ { \"topic\" : \"bears\" } , { \"topic\" : \"cats\" } ] , config = { \"max_concurrency\" : 5 } ) [AIMessage(content=\"Why don't bears wear shoes? \\n\\nBecause they have bear feet!\"), AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\")] Async Stream ​ async for s in chain . astream ( { \"topic\" : \"bears\" } ) : print ( s . content , end = \"\" , flush = True ) Sure, here's a bear-themed joke for you: Why don't bears wear shoes? Because they already have bear feet! Async Invoke ​ await chain . ainvoke ( { \"topic\" : \"bears\" } ) AIMessage(content=\"Why don't bears wear shoes? \\n\\nBecause they have bear feet!\") Async Batch ​ await chain . abatch ( [ { \"topic\" : \"bears\" } ] ) [AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\")] Async Stream Intermediate Steps ​ All runnables also have a method .astream_log() which is used to\nstream (as they happen) all or part of the intermediate steps of your\nchain/sequence. This is useful to show progress to the user, to use intermediate\nresults, or to debug your chain. You can stream all steps (default) or include/exclude steps by name,\ntags or metadata. This method yields JSONPatch ops that when\napplied in the same order as received build up the RunState. class LogEntry ( TypedDict ) : id : str \"\"\"ID of the sub-run.\"\"\" name : str \"\"\"Name of the object being run.\"\"\" type : str \"\"\"Type of the object being run, eg. prompt, chain, llm, etc.\"\"\" tags : List [ str ] \"\"\"List of tags for the run.\"\"\" metadata : Dict [ str , Any ] \"\"\"Key-value pairs of metadata for the run.\"\"\" start_time : str \"\"\"ISO-8601 timestamp of when the run started.\"\"\" streamed_output_str : List [ str ] \"\"\"List of LLM tokens streamed by this run, if applicable.\"\"\" final_output : Optional [ Any ] \"\"\"Final output of this run. Only available after the run has finished successfully.\"\"\" end_time : Optional [ str ] \"\"\"ISO-8601 timestamp of when the run ended. Only available after the run has finished.\"\"\" class RunState ( TypedDict ) : id : str \"\"\"ID of the run.\"\"\" streamed_output : List [ Any ] \"\"\"List of output chunks streamed by Runnable.stream()\"\"\" final_output : Optional [ Any ] \"\"\"Final output of the run, usually the result of aggregating (`+`) streamed_output. Only available after the run has finished successfully.\"\"\" logs : Dict [ str , LogEntry ] \"\"\"Map of run names to sub-runs. If filters were supplied, this list will contain only the runs that matched the filters.\"\"\" Streaming JSONPatch chunks ​ This is useful eg. to stream the JSONPatch in an HTTP server, and then\napply the ops on the client to rebuild the run state there. See LangServe for tooling to\nmake it easier to build a webserver from any Runnable. from langchain . embeddings import OpenAIEmbeddings from langchain . vectorstores import FAISS from langchain_core . output_parsers import StrOutputParser from langchain_core . runnables import RunnablePassthrough template = \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\" prompt = ChatPromptTemplate . from_template ( template ) vectorstore = FAISS . from_texts ( [ \"harrison worked at kensho\" ] , embedding = OpenAIEmbeddings ( ) ) retriever = vectorstore . as_retriever ( ) retrieval_chain = ( { \"context\" : retriever . with_config ( run_name = \"Docs\" ) , \"question\" : RunnablePassthrough ( ) , } | prompt | model | StrOutputParser ( ) ) async for chunk in retrieval_chain . astream_log ( \"where did harrison work?\" , include_names = [ \"Docs\" ] ) : print ( \"-\" * 40 ) print ( chunk ) ---------------------------------------- RunLogPatch({'op': 'replace', 'path': '', 'value': {'final_output': None, 'id': 'e2f2cc72-eb63-4d20-8326-237367482efb', 'logs': {}, 'streamed_output': []}}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/logs/Docs', 'value': {'end_time': None, 'final_output': None, 'id': '8da492cc-4492-4e74-b8b0-9e60e8693390', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:50:13.526', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/logs/Docs/final_output', 'value': {'documents': [Document(page_content='harrison worked at kensho')]}}, {'op': 'add', 'path': '/logs/Docs/end_time', 'value': '2023-10-19T17:50:13.713'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'H'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'arrison'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' worked'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' at'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Kens'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ho'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''}) ---------------------------------------- RunLogPatch({'op': 'replace', 'path': '/final_output', 'value': {'output': 'Harrison worked at Kensho.'}}) Streaming the incremental RunState ​ You can simply pass diff=False to get incremental values of RunState . You get more verbose output with more repetitive parts. async for chunk in retrieval_chain . astream_log ( \"where did harrison work?\" , include_names = [ \"Docs\" ] , diff = False ) : print ( \"-\" * 70 ) print ( chunk ) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {}, 'streamed_output': []}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': None, 'final_output': None, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': []}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': []}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': ['']}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': ['', 'H']}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': ['', 'H', 'arrison']}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': ['', 'H', 'arrison', ' worked']}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': ['', 'H', 'arrison', ' worked', ' at']}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens']}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho']}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho', '.']}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho', '.', '']}) ---------------------------------------------------------------------- RunLog({'final_output': {'output': 'Harrison worked at Kensho.'}, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho', '.', '']}) Parallelism ​ Let’s take a look at how LangChain Expression Language supports parallel\nrequests. For example, when using a RunnableParallel (often written as\na dictionary) it executes each element in parallel. from langchain_core . runnables import RunnableParallel chain1 = ChatPromptTemplate . from_template ( \"tell me a joke about {topic}\" ) | model chain2 = ( ChatPromptTemplate . from_template ( \"write a short (2 line) poem about {topic}\" ) | model ) combined = RunnableParallel ( joke = chain1 , poem = chain2 ) % % time chain1 . invoke ( { \"topic\" : \"bears\" } ) CPU times: user 54.3 ms, sys: 0 ns, total: 54.3 ms Wall time: 2.29 s AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\") % % time chain2 . invoke ( { \"topic\" : \"bears\" } ) CPU times: user 7.8 ms, sys: 0 ns, total: 7.8 ms Wall time: 1.43 s AIMessage(content=\"In wild embrace,\\nNature's strength roams with grace.\") % % time combined . invoke ( { \"topic\" : \"bears\" } ) CPU times: user 167 ms, sys: 921 µs, total: 168 ms Wall time: 1.56 s {'joke': AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\"), 'poem': AIMessage(content=\"Fierce and wild, nature's might,\\nBears roam the woods, shadows of the night.\")} Parallelism on batches ​ Parallelism can be combined with other runnables. Let’s try to use\nparallelism with batches. % % time chain1 . batch ( [ { \"topic\" : \"bears\" } , { \"topic\" : \"cats\" } ] ) CPU times: user 159 ms, sys: 3.66 ms, total: 163 ms Wall time: 1.34 s [AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\"), AIMessage(content=\"Sure, here's a cat joke for you:\\n\\nWhy don't cats play poker in the wild?\\n\\nBecause there are too many cheetahs!\")] % % time chain2 . batch ( [ { \"topic\" : \"bears\" } , { \"topic\" : \"cats\" } ] ) CPU times: user 165 ms, sys: 0 ns, total: 165 ms Wall time: 1.73 s [AIMessage(content=\"Silent giants roam,\\nNature's strength, love's emblem shown.\"), AIMessage(content='Whiskers aglow, paws tiptoe,\\nGraceful hunters, hearts aglow.')] % % time combined . batch ( [ { \"topic\" : \"bears\" } , { \"topic\" : \"cats\" } ] ) CPU times: user 507 ms, sys: 125 ms, total: 632 ms Wall time: 1.49 s [{'joke': AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\"), 'poem': AIMessage(content=\"Majestic bears roam,\\nNature's wild guardians of home.\")}, {'joke': AIMessage(content=\"Sure, here's a cat joke for you:\\n\\nWhy did the cat sit on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!\"), 'poem': AIMessage(content='Whiskers twitch, eyes gleam,\\nGraceful creatures, feline dream.')}] Previous Why use LCEL Next How to"
    },
    {
        "title": "How to | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/how_to/",
        "text_content": "LangChain Expression Language How to How to 📄️ RunnableParallel: Manipulating data manipulating-inputs-output} 📄️ RunnablePassthrough: Passing data through passing-data-through} 📄️ RunnableLambda: Run Custom Functions run-custom-functions} 📄️ RunnableBranch: Dynamically route logic based on input dynamically-route-logic-based-on-input} 📄️ Bind runtime args Sometimes we want to invoke a Runnable within a Runnable sequence with 📄️ Configure chain internals at runtime Oftentimes you may want to experiment with, or even expose to the end 📄️ Add fallbacks There are many possible points of failure in an LLM application, whether 📄️ Stream custom generator functions You can use generator functions (ie. functions that use the yield 📄️ Add message history (memory) The RunnableWithMessageHistory let’s us add message history to certain Previous Interface Next RunnableParallel: Manipulating data"
    },
    {
        "title": "Cookbook | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/cookbook/",
        "text_content": "LangChain Expression Language Cookbook Cookbook Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start. 📄️ Prompt + LLM The most common and valuable composition is taking: 📄️ RAG Let’s look at adding in a retrieval step to a prompt and LLM, which adds 📄️ Multiple chains Runnables can easily be used to string together multiple Chains 📄️ Querying a SQL DB We can replicate our SQLDatabaseChain with Runnables. 📄️ Agents You can pass a Runnable into an agent. 📄️ Code writing Example of how to use LCEL to write Python code. 📄️ Routing by semantic similarity With LCEL you can easily add [custom routing 📄️ Adding memory This shows how to add memory to an arbitrary chain. Right now, you can 📄️ Adding moderation This shows how to add in moderation (or other safeguards) around your 📄️ Managing prompt size Agents dynamically call tools. The results of those tool calls are added 📄️ Using tools You can use any Tools with Runnables easily. Previous Add message history (memory) Next Prompt + LLM"
    },
    {
        "title": "Modules | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/",
        "text_content": "Modules On this page Modules LangChain provides standard, extendable interfaces and external integrations for the following main modules: Model I/O ​ Interface with language models Retrieval ​ Interface with application-specific data Agents ​ Let chains choose which tools to use given high-level directives Additional ​ Chains ​ Common, building block compositions Memory ​ Persist application state between runs of a chain Callbacks ​ Log and stream intermediate steps of any chain Previous LangChain Expression Language (LCEL) Next Model I/O"
    },
    {
        "title": "Model I/O | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/model_io/",
        "text_content": "Modules Model I/O On this page Model I/O The core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model. Prompts : Templatize, dynamically select, and manage model inputs Chat models : Models that are backed by a language model but take a list of Chat Messages as input and return a Chat Message LLMs : Models that take a text string as input and return a text string Output parsers : Extract information from model outputs LLMs vs Chat models ​ LLMs and chat models are subtly but importantly different. LLMs in LangChain refer to pure text completion models.\nThe APIs they wrap take a string prompt as input and output a string completion. OpenAI's GPT-3 is implemented as an LLM.\nChat models are often backed by LLMs but tuned specifically for having conversations.\nAnd, crucially, their provider APIs use a different interface than pure text completion models. Instead of a single string,\nthey take a list of chat messages as input. Usually these messages are labeled with the speaker (usually one of \"System\",\n\"AI\", and \"Human\"). And they return an AI chat message as output. GPT-4 and Anthropic's Claude-2 are both implemented as chat models. Previous Modules Next Model I/O"
    },
    {
        "title": "Retrieval | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/data_connection/",
        "text_content": "Modules Retrieval Retrieval Many LLM applications require user-specific data that is not part of the model's training set.\nThe primary way of accomplishing this is through Retrieval Augmented Generation (RAG).\nIn this process, external data is retrieved and then passed to the LLM when doing the generation step. LangChain provides all the building blocks for RAG applications - from simple to complex.\nThis section of the documentation covers everything related to the retrieval step - e.g. the fetching of the data.\nAlthough this sounds simple, it can be subtly complex.\nThis encompasses several key modules. Document loaders Document loaders load documents from many different sources.\nLangChain provides over 100 different document loaders as well as integrations with other major providers in the space,\nlike AirByte and Unstructured.\nLangChain provides integrations to load all types of documents (HTML, PDF, code) from all types of locations (private S3 buckets, public websites). Document transformers A key part of retrieval is fetching only the relevant parts of documents.\nThis involves several transformation steps to prepare the documents for retrieval.\nOne of the primary ones here is splitting (or chunking) a large document into smaller chunks.\nLangChain provides several transformation algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc). Text embedding models Another key part of retrieval is creating embeddings for documents.\nEmbeddings capture the semantic meaning of the text, allowing you to quickly and\nefficiently find other pieces of a text that are similar.\nLangChain provides integrations with over 25 different embedding providers and methods,\nfrom open-source to proprietary API,\nallowing you to choose the one best suited for your needs.\nLangChain provides a standard interface, allowing you to easily swap between models. Vector stores With the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings.\nLangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones,\nallowing you to choose the one best suited for your needs.\nLangChain exposes a standard interface, allowing you to easily swap between vector stores. Retrievers Once the data is in the database, you still need to retrieve it.\nLangChain supports many different retrieval algorithms and is one of the places where we add the most value.\nLangChain supports basic methods that are easy to get started - namely simple semantic search.\nHowever, we have also added a collection of algorithms on top of this to increase performance.\nThese include: Parent Document Retriever : This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context. Self Query Retriever : User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the semantic part of a query from other metadata filters present in the query. Ensemble Retriever : Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this. And more! Indexing The LangChain Indexing API syncs your data from any source into a vector store,\nhelping you: Avoid writing duplicated content into the vector store Avoid re-writing unchanged content Avoid re-computing embeddings over unchanged content All of which should save you time and money, as well as improve your vector search results. Previous XML parser Next Document loaders"
    },
    {
        "title": "Agents | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/agents/",
        "text_content": "Modules Agents On this page Agents The core idea of agents is to use a language model to choose a sequence\nof actions to take. In chains, a sequence of actions is hardcoded (in\ncode). In agents, a language model is used as a reasoning engine to\ndetermine which actions to take and in which order. Concepts ​ There are several key components here: Agent ​ This is the chain responsible for deciding what step to take next. This\nis powered by a language model and a prompt. The inputs to this chain\nare: Tools: Descriptions of available tools User input: The high level objective Intermediate steps: Any (action, tool output) pairs previously\nexecuted in order to achieve the user input The output is the next action(s) to take or the final response to send\nto the user ( AgentAction s or AgentFinish ). An action specifies a\ntool and the input to that tool. Different agents have different prompting styles for reasoning,\ndifferent ways of encoding inputs, and different ways of parsing the\noutput. For a full list of built-in agents see agent\ntypes . You can also easily build\ncustom agents , which we show how to do in the Get started section\nbelow. Tools ​ Tools are functions that an agent can invoke. There are two important\ndesign considerations around tools: Giving the agent access to the right tools Describing the tools in a way that is most helpful to the agent Without thinking through both, you won’t be able to build a working\nagent. If you don’t give the agent access to a correct set of tools, it\nwill never be able to accomplish the objectives you give it. If you\ndon’t describe the tools well, the agent won’t know how to use them\nproperly. LangChain provides a wide set of built-in tools, but also makes it easy\nto define your own (including custom descriptions). For a full list of\nbuilt-in tools, see the tools integrations\nsection Toolkits ​ For many common tasks, an agent will need a set of related tools. For\nthis LangChain provides the concept of toolkits - groups of around 3-5\ntools needed to accomplish specific objectives. For example, the GitHub\ntoolkit has a tool for searching through GitHub issues, a tool for\nreading a file, a tool for commenting, etc. LangChain provides a wide set of toolkits to get started. For a full\nlist of built-in toolkits, see the toolkits integrations\nsection AgentExecutor ​ The agent executor is the runtime for an agent. This is what actually\ncalls the agent, executes the actions it chooses, passes the action\noutputs back to the agent, and repeats. In pseudocode, this looks\nroughly like: next_action = agent . get_action ( . . . ) while next_action != AgentFinish : observation = run ( next_action ) next_action = agent . get_action ( . . . , next_action , observation ) return next_action While this may seem simple, there are several complexities this runtime\nhandles for you, including: Handling cases where the agent selects a non-existent tool Handling cases where the tool errors Handling cases where the agent produces output that cannot be parsed\ninto a tool invocation Logging and observability at all levels (agent decisions, tool\ncalls) to stdout and/or to LangSmith . Other types of agent runtimes ​ The AgentExecutor class is the main agent runtime supported by\nLangChain. However, there are other, more experimental runtimes we also\nsupport. These include: Plan-and-execute\nAgent Baby AGI Auto GPT You can also always create your own custom execution logic, which we\nshow how to do below. Get started ​ To best understand the agent framework, lets build an agent from scratch\nusing LangChain Expression Language (LCEL). We’ll need to build the\nagent itself, define custom tools, and run the agent and tools in a\ncustom loop. At the end we’ll show how to use the standard LangChain AgentExecutor to make execution easier. Some important terminology (and schema) to know: AgentAction : This is a dataclass that represents the action an\nagent should take. It has a tool property (which is the name of\nthe tool that should be invoked) and a tool_input property (the\ninput to that tool) AgentFinish : This is a dataclass that signifies that the agent has\nfinished and should return to the user. It has a return_values parameter, which is a dictionary to return. It often only has one\nkey - output - that is a string, and so often it is just this key\nthat is returned. intermediate_steps : These represent previous agent actions and\ncorresponding outputs that are passed around. These are important to\npass to future iteration so the agent knows what work it has already\ndone. This is typed as a List[Tuple[AgentAction, Any]] . Note that\nobservation is currently left as type Any to be maximally\nflexible. In practice, this is often a string. Setup: LangSmith ​ By definition, agents take a self-determined, input-dependent sequence\nof steps before returning a user-facing output. This makes debugging\nthese systems particularly tricky, and observability particularly\nimportant. LangSmith is especially useful for such\ncases. When building with LangChain, any built-in agent or custom agent built\nwith LCEL will automatically be traced in LangSmith. And if we use the AgentExecutor , we’ll get full tracing of not only the agent planning\nsteps but also the tool inputs and outputs. To set up LangSmith we just need set the following environment\nvariables: export LANGCHAIN_TRACING_V2 = \"true\" export LANGCHAIN_API_KEY = \"<your-api-key>\" Define the agent ​ We first need to create our agent. This is the chain responsible for\ndetermining what action to take next. In this example, we will use OpenAI Function Calling to create this\nagent. This is generally the most reliable way to create agents. For this guide, we will construct a custom agent that has access to a\ncustom tool. We are choosing this example because for most real world\nuse cases you will NEED to customize either the agent or the tools.\nWe’ll create a simple tool that computes the length of a word. This is\nuseful because it’s actually something LLMs can mess up due to\ntokenization. We will first create it WITHOUT memory, but we will then\nshow how to add memory in. Memory is needed to enable conversation. First, let’s load the language model we’re going to use to control the\nagent. from langchain . chat_models import ChatOpenAI llm = ChatOpenAI ( model = \"gpt-3.5-turbo\" , temperature = 0 ) We can see that it struggles to count the letters in the string “educa”. llm . invoke ( \"how many letters in the word educa?\" ) AIMessage(content='There are 6 letters in the word \"educa\".') Next, let’s define some tools to use. Let’s write a really simple Python\nfunction to calculate the length of a word that is passed in. from langchain . agents import tool @tool def get_word_length ( word : str ) - > int : \"\"\"Returns the length of a word.\"\"\" return len ( word ) tools = [ get_word_length ] Now let us create the prompt. Because OpenAI Function Calling is\nfinetuned for tool usage, we hardly need any instructions on how to\nreason, or how to output format. We will just have two input variables: input and agent_scratchpad . input should be a string containing\nthe user objective. agent_scratchpad should be a sequence of messages\nthat contains the previous agent tool invocations and the corresponding\ntool outputs. from langchain . prompts import ChatPromptTemplate , MessagesPlaceholder prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , \"You are very powerful assistant, but bad at calculating lengths of words.\" , ) , ( \"user\" , \"{input}\" ) , MessagesPlaceholder ( variable_name = \"agent_scratchpad\" ) , ] ) How does the agent know what tools it can use? In this case we’re\nrelying on OpenAI function calling LLMs, which take functions as a\nseparate argument and have been specifically trained to know when to\ninvoke those functions. To pass in our tools to the agent, we just need to format them to the\nOpenAI function format and pass them to our model. (By bind -ing the\nfunctions, we’re making sure that they’re passed in each time the model\nis invoked.) from langchain . tools . render import format_tool_to_openai_function llm_with_tools = llm . bind ( functions = [ format_tool_to_openai_function ( t ) for t in tools ] ) Putting those pieces together, we can now create the agent. We will\nimport two last utility functions: a component for formatting\nintermediate steps (agent action, tool output pairs) to input messages\nthat can be sent to the model, and a component for converting the output\nmessage into an agent action/agent finish. from langchain . agents . format_scratchpad import format_to_openai_function_messages from langchain . agents . output_parsers import OpenAIFunctionsAgentOutputParser agent = ( { \"input\" : lambda x : x [ \"input\" ] , \"agent_scratchpad\" : lambda x : format_to_openai_function_messages ( x [ \"intermediate_steps\" ] ) , } | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser ( ) ) Now that we have our agent, let’s play around with it! Let’s pass in a\nsimple question and empty intermediate steps and see what it returns: agent . invoke ( { \"input\" : \"how many letters in the word educa?\" , \"intermediate_steps\" : [ ] } ) AgentActionMessageLog(tool='get_word_length', tool_input={'word': 'educa'}, log=\"\\nInvoking: `get_word_length` with `{'word': 'educa'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"word\": \"educa\"\\n}', 'name': 'get_word_length'}})]) We can see that it responds with an AgentAction to take (it’s actually\nan AgentActionMessageLog - a subclass of AgentAction which also\ntracks the full message log). If we’ve set up LangSmith, we’ll see a trace that let’s us inspect the\ninput and output to each step in the sequence: https://smith.langchain.com/public/04110122-01a8-413c-8cd0-b4df6eefa4b7/r Define the runtime ​ So this is just the first step - now we need to write a runtime for\nthis. The simplest one is just one that continuously loops, calling the\nagent, then taking the action, and repeating until an AgentFinish is\nreturned. Let’s code that up below: from langchain_core . agents import AgentFinish user_input = \"how many letters in the word educa?\" intermediate_steps = [ ] while True : output = agent . invoke ( { \"input\" : user_input , \"intermediate_steps\" : intermediate_steps , } ) if isinstance ( output , AgentFinish ) : final_result = output . return_values [ \"output\" ] break else : print ( f\"TOOL NAME: { output . tool } \" ) print ( f\"TOOL INPUT: { output . tool_input } \" ) tool = { \"get_word_length\" : get_word_length } [ output . tool ] observation = tool . run ( output . tool_input ) intermediate_steps . append ( ( output , observation ) ) print ( final_result ) TOOL NAME: get_word_length TOOL INPUT: {'word': 'educa'} There are 5 letters in the word \"educa\". Woo! It’s working. Using AgentExecutor ​ To simplify this a bit, we can import and use the AgentExecutor class.\nThis bundles up all of the above and adds in error handling, early\nstopping, tracing, and other quality-of-life improvements that reduce\nsafeguards you need to write. from langchain . agents import AgentExecutor agent_executor = AgentExecutor ( agent = agent , tools = tools , verbose = True ) Now let’s test it out! agent_executor . invoke ( { \"input\" : \"how many letters in the word educa?\" } ) > Entering new AgentExecutor chain... Invoking: `get_word_length` with `{'word': 'educa'}` 5There are 5 letters in the word \"educa\". > Finished chain. {'input': 'how many letters in the word educa?', 'output': 'There are 5 letters in the word \"educa\".'} And looking at the trace, we can see that all of our agent calls and\ntool invocations are automatically logged: https://smith.langchain.com/public/957b7e26-bef8-4b5b-9ca3-4b4f1c96d501/r Adding memory ​ This is great - we have an agent! However, this agent is stateless - it\ndoesn’t remember anything about previous interactions. This means you\ncan’t ask follow up questions easily. Let’s fix that by adding in\nmemory. In order to do this, we need to do two things: Add a place for memory variables to go in the prompt Keep track of the chat history First, let’s add a place for memory in the prompt. We do this by adding\na placeholder for messages with the key \"chat_history\" . Notice that we\nput this ABOVE the new user input (to follow the conversation flow). from langchain . prompts import MessagesPlaceholder MEMORY_KEY = \"chat_history\" prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , \"You are very powerful assistant, but bad at calculating lengths of words.\" , ) , MessagesPlaceholder ( variable_name = MEMORY_KEY ) , ( \"user\" , \"{input}\" ) , MessagesPlaceholder ( variable_name = \"agent_scratchpad\" ) , ] ) We can then set up a list to track the chat history from langchain_core . messages import AIMessage , HumanMessage chat_history = [ ] We can then put it all together! agent = ( { \"input\" : lambda x : x [ \"input\" ] , \"agent_scratchpad\" : lambda x : format_to_openai_function_messages ( x [ \"intermediate_steps\" ] ) , \"chat_history\" : lambda x : x [ \"chat_history\" ] , } | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser ( ) ) agent_executor = AgentExecutor ( agent = agent , tools = tools , verbose = True ) When running, we now need to track the inputs and outputs as chat\nhistory input1 = \"how many letters in the word educa?\" result = agent_executor . invoke ( { \"input\" : input1 , \"chat_history\" : chat_history } ) chat_history . extend ( [ HumanMessage ( content = input1 ) , AIMessage ( content = result [ \"output\" ] ) , ] ) agent_executor . invoke ( { \"input\" : \"is that a real word?\" , \"chat_history\" : chat_history } ) > Entering new AgentExecutor chain... Invoking: `get_word_length` with `{'word': 'educa'}` 5There are 5 letters in the word \"educa\". > Finished chain. > Entering new AgentExecutor chain... No, \"educa\" is not a real word in English. > Finished chain. {'input': 'is that a real word?', 'chat_history': [HumanMessage(content='how many letters in the word educa?'), AIMessage(content='There are 5 letters in the word \"educa\".')], 'output': 'No, \"educa\" is not a real word in English.'} Here’s the LangSmith trace: https://smith.langchain.com/public/1e1b7e07-3220-4a6c-8a1e-f04182a755b3/r Next Steps ​ Awesome! You’ve now run your first end-to-end agent. To dive deeper, you\ncan: Check out all the different agent\ntypes supported Learn all the controls for AgentExecutor Explore the how-to’s of tools and all\nthe tool integrations See a full list of all the off-the-shelf toolkits we provide Previous Indexing Next Agent Types"
    },
    {
        "title": "Chains | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/chains/",
        "text_content": "Modules More Chains On this page Chains Using an LLM in isolation is fine for simple applications, but more\ncomplex applications require chaining LLMs - either with each other or\nwith other components. LangChain provides two high-level frameworks for “chaining” components.\nThe legacy approach is to use the Chain interface. The updated\napproach is to use the LangChain Expression Language\n(LCEL) . When building new applications we\nrecommend using LCEL for chain composition. But there are a number of\nuseful, built-in Chain ’s that we continue to support, so we document\nboth frameworks here. As we’ll touch on below, Chain ’s can also\nthemselves be used in LCEL, so the two are not mutually exclusive. LCEL ​ The most visible part of LCEL is that it provides an intuitive and\nreadable syntax for composition. But more importantly, it also provides\nfirst-class support for: streaming , async calls , batching , parallelization , retries, fallbacks , tracing, and more. As a simple and common example, we can see what it’s like to combine a\nprompt, model and output parser: from langchain . chat_models import ChatAnthropic from langchain . prompts import ChatPromptTemplate from langchain . schema import StrOutputParser model = ChatAnthropic ( ) prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , \"You're a very knowledgeable historian who provides accurate and eloquent answers to historical questions.\" , ) , ( \"human\" , \"{question}\" ) , ] ) runnable = prompt | model | StrOutputParser ( ) for chunk in runnable . stream ( { \"question\" : \"How did Mansa Musa accumulate his wealth?\" } ) : print ( chunk , end = \"\" , flush = True ) Mansa Musa was the emperor of the Mali Empire in West Africa during the 14th century. He accumulated immense wealth through several means: - Gold mining - Mali contained very rich gold deposits, especially in the region of Bambuk. Gold mining and gold trade was a major source of wealth for the empire. - Control of trade routes - Mali dominated the trans-Saharan trade routes connecting West Africa to North Africa and beyond. By taxing the goods that passed through its territory, Mali profited greatly. - Tributary states - Many lands surrounding Mali paid tribute to the empire. This came in the form of gold, slaves, and other valuable resources. - Agriculture - Mali also had extensive agricultural lands irrigated by the Niger River. Surplus food produced could be sold or traded. - Royal monopolies - The emperor claimed monopoly rights over the production and sale of certain goods like salt from the Taghaza mines. This added to his personal wealth. - Inheritance - As an emperor, Mansa Musa inherited a wealthy state. His predecessors had already consolidated lands and accumulated riches which fell to Musa. So in summary, mining, trade, taxes, For more head to the LCEL section . [ Legacy ] Chain interface ​ Chain ’s are the legacy interface for “chained” applications. We\ndefine a Chain very generically as a sequence of calls to components,\nwhich can include other chains. The base interface is simple: class Chain ( BaseModel , ABC ) : \"\"\"Base interface that all chains should implement.\"\"\" memory : BaseMemory callbacks : Callbacks def __call__ ( self , inputs : Any , return_only_outputs : bool = False , callbacks : Callbacks = None , ) - > Dict [ str , Any ] : . . . We can recreate the LCEL runnable we made above using the built-in LLMChain : from langchain . chains import LLMChain chain = LLMChain ( llm = model , prompt = prompt , output_parser = StrOutputParser ( ) ) chain . run ( question = \"How did Mansa Musa accumulate his wealth?\" ) \" Mansa Musa was the emperor of the Mali Empire in West Africa in the early 14th century. He accumulated his vast wealth through several means:\\n\\n- Gold mining - Mali contained very rich gold deposits, especially in the southern part of the empire. Gold mining and trade was a major source of wealth.\\n\\n- Control of trade routes - Mali dominated the trans-Saharan trade routes connecting West Africa to North Africa and beyond. By taxing and controlling this lucrative trade, Mansa Musa reaped great riches.\\n\\n- Tributes from conquered lands - The Mali Empire expanded significantly under Mansa Musa's rule. As new lands were conquered, they paid tribute to the mansa in the form of gold, salt, and slaves.\\n\\n- Inheritance - Mansa Musa inherited a wealthy empire from his predecessor. He continued to build the wealth of Mali through the factors above.\\n\\n- Sound fiscal management - Musa is considered to have managed the empire and its finances very effectively, including keeping taxes reasonable and promoting a robust economy. This allowed him to accumulate and maintain wealth.\\n\\nSo in summary, conquest, trade, taxes, mining, and inheritance all contributed to Mansa Musa growing the M\" For more specifics check out: - How-to for walkthroughs of different chain features - Foundational to get acquainted\nwith core building block chains - Document to learn how to incorporate\ndocuments into chains Previous Agents Next How to"
    },
    {
        "title": "🦜️🏓 LangServe | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/langserve",
        "text_content": "LangServe On this page 🦜️🏓 LangServe 🚩 We will be releasing a hosted version of LangServe for one-click deployments of LangChain applications. Sign up here to get on the waitlist. Overview ​ LangServe helps developers deploy LangChain runnables and chains as a REST API. This library is integrated with FastAPI and uses pydantic for data validation. In addition, it provides a client that can be used to call into runnables deployed on a server.\nA javascript client is available in LangChainJS . Features ​ Input and Output schemas automatically inferred from your LangChain object, and enforced on every API call, with rich error messages API docs page with JSONSchema and Swagger (insert example link) Efficient /invoke/ , /batch/ and /stream/ endpoints with support for many concurrent requests on a single server /stream_log/ endpoint for streaming all (or some) intermediate steps from your chain/agent Playground page at /playground/ with streaming output and intermediate steps Built-in (optional) tracing to LangSmith , just add your API key (see Instructions ]) All built with battle-tested open-source Python libraries like FastAPI, Pydantic, uvloop and asyncio. Use the client SDK to call a LangServe server as if it was a Runnable running locally (or call the HTTP API directly) LangServe Hub Limitations ​ Client callbacks are not yet supported for events that originate on the server OpenAPI docs will not be generated when using Pydantic V2. Fast API does not support mixing pydantic v1 and v2 namespaces . See section below for more details. Hosted LangServe ​ We will be releasing a hosted version of LangServe for one-click deployments of LangChain applications. Sign up here to get on the waitlist. Security ​ Vulnerability in Versions 0.0.13 - 0.0.15 -- playground endpoint allows accessing arbitrary files on server. Resolved in 0.0.16 . Installation ​ For both client and server: pip install \"langserve[all]\" or pip install \"langserve[client]\" for client code, and pip install \"langserve[server]\" for server code. LangChain CLI 🛠️ ​ Use the LangChain CLI to bootstrap a LangServe project quickly. To use the langchain CLI make sure that you have a recent version of langchain-cli installed. You can install it with pip install -U langchain-cli . langchain app new ../path/to/directory Examples ​ Get your LangServe instance started quickly with LangChain Templates . For more examples, see the templates index or the examples directory. Server ​ Here's a server that deploys an OpenAI chat model, an Anthropic chat model, and a chain that uses\nthe Anthropic model to tell a joke about a topic. #!/usr/bin/env python from fastapi import FastAPI from langchain . prompts import ChatPromptTemplate from langchain . chat_models import ChatAnthropic , ChatOpenAI from langserve import add_routes app = FastAPI ( title = \"LangChain Server\" , version = \"1.0\" , description = \"A simple api server using Langchain's Runnable interfaces\" , ) add_routes ( app , ChatOpenAI ( ) , path = \"/openai\" , ) add_routes ( app , ChatAnthropic ( ) , path = \"/anthropic\" , ) model = ChatAnthropic ( ) prompt = ChatPromptTemplate . from_template ( \"tell me a joke about {topic}\" ) add_routes ( app , prompt | model , path = \"/joke\" , ) if __name__ == \"__main__\" : import uvicorn uvicorn . run ( app , host = \"localhost\" , port = 8000 ) Docs ​ If you've deployed the server above, you can view the generated OpenAPI docs using: ⚠️ If using pydantic v2, docs will not be generated for invoke , batch , stream , stream_log . See Pydantic section below for more details. curl localhost:8000/docs make sure to add the /docs suffix. ⚠️ Index page / is not defined by design , so curl localhost:8000 or visiting the URL\nwill return a 404. If you want content at / define an endpoint @app.get(\"/\") . Client ​ Python SDK from langchain . schema import SystemMessage , HumanMessage from langchain . prompts import ChatPromptTemplate from langchain . schema . runnable import RunnableMap from langserve import RemoteRunnable openai = RemoteRunnable ( \"http://localhost:8000/openai/\" ) anthropic = RemoteRunnable ( \"http://localhost:8000/anthropic/\" ) joke_chain = RemoteRunnable ( \"http://localhost:8000/joke/\" ) joke_chain . invoke ( { \"topic\" : \"parrots\" } ) # or async await joke_chain . ainvoke ( { \"topic\" : \"parrots\" } ) prompt = [ SystemMessage ( content = 'Act like either a cat or a parrot.' ) , HumanMessage ( content = 'Hello!' ) ] # Supports astream async for msg in anthropic . astream ( prompt ) : print ( msg , end = \"\" , flush = True ) prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , \"Tell me a long story about {topic}\" ) ] ) # Can define custom chains chain = prompt | RunnableMap ( { \"openai\" : openai , \"anthropic\" : anthropic , } ) chain . batch ( [ { \"topic\" : \"parrots\" } , { \"topic\" : \"cats\" } ] ) In TypeScript (requires LangChain.js version 0.0.166 or later): import { RemoteRunnable } from \"langchain/runnables/remote\" ; const chain = new RemoteRunnable ( { url : ` http://localhost:8000/joke/ ` , } ) ; const result = await chain . invoke ( { topic : \"cats\" , } ) ; Python using requests : import requests response = requests . post ( \"http://localhost:8000/joke/invoke\" , json = { 'input' : { 'topic' : 'cats' } } ) response . json ( ) You can also use curl : curl --location --request POST 'http://localhost:8000/joke/invoke' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"input\": { \"topic\": \"cats\" } }' Endpoints ​ The following code: . . . add_routes ( app , runnable , path = \"/my_runnable\" , ) adds of these endpoints to the server: POST /my_runnable/invoke - invoke the runnable on a single input POST /my_runnable/batch - invoke the runnable on a batch of inputs POST /my_runnable/stream - invoke on a single input and stream the output POST /my_runnable/stream_log - invoke on a single input and stream the output, including output of intermediate steps as it's generated GET /my_runnable/input_schema - json schema for input to the runnable GET /my_runnable/output_schema - json schema for output of the runnable GET /my_runnable/config_schema - json schema for config of the runnable These endpoints match the LangChain Expression Language interface -- please reference this documentation for more details. Playground ​ You can find a playground page for your runnable at /my_runnable/playground/ . This exposes a simple UI to configure and invoke your runnable with streaming output and intermediate steps. Widgets ​ The playground supports widgets and can be used to test your runnable with different inputs. In addition, for configurable runnables, the playground will allow you to configure the runnable and share a link with the configuration: Sharing ​ Legacy Chains ​ LangServe works with both Runnables (constructed via LangChain Expression Language ) and legacy chains (inheriting from Chain ).\nHowever, some of the input schemas for legacy chains may be incomplete/incorrect, leading to errors.\nThis can be fixed by updating the input_schema property of those chains in LangChain.\nIf you encounter any errors, please open an issue on THIS repo, and we will work to address it. Deployment ​ Deploy to Azure ​ You can deploy to Azure using Azure Container Apps (Serverless): az containerapp up --name [container-app-name] --source . --resource-group [resource-group-name] --environment  [environment-name] --ingress external --target-port 8001 --env-vars=OPENAI_API_KEY=your_key You can find more info here Deploy to GCP ​ You can deploy to GCP Cloud Run using the following command: gcloud run deploy [your-service-name] --source . --port 8001 --allow-unauthenticated --region us-central1 --set-env-vars=OPENAI_API_KEY=your_key Community Contributed ​ Deploy to Railway ​ Example Repo Pydantic ​ LangServe provides support for Pydantic 2 with some limitations. OpenAPI docs will not be generated for invoke/batch/stream/stream_log when using Pydantic V2. Fast API does not support [mixing pydantic v1 and v2 namespaces] . LangChain uses the v1 namespace in Pydantic v2. Please read the following guidelines to ensure compatibility with LangChain Except for these limitations, we expect the API endpoints, the playground and any other features to work as expected. Advanced ​ Handling Authentication ​ If you need to add authentication to your server,\nplease reference FastAPI's security documentation and middleware documentation . Files ​ LLM applications often deal with files. There are different architectures\nthat can be made to implement file processing; at a high level: The file may be uploaded to the server via a dedicated endpoint and processed using a separate endpoint The file may be uploaded by either value (bytes of file) or reference (e.g., s3 url to file content) The processing endpoint may be blocking or non-blocking If significant processing is required, the processing may be offloaded to a dedicated process pool You should determine what is the appropriate architecture for your application. Currently, to upload files by value to a runnable, use base64 encoding for the\nfile ( multipart/form-data is not supported yet). Here's an example that shows\nhow to use base64 encoding to send a file to a remote runnable. Remember, you can always upload files by reference (e.g., s3 url) or upload them as\nmultipart/form-data to a dedicated endpoint. Custom Input and Output Types ​ Input and Output types are defined on all runnables. You can access them via the input_schema and output_schema properties. LangServe uses these types for validation and documentation. If you want to override the default inferred types, you can use the with_types method. Here's a toy example to illustrate the idea: from typing import Any from fastapi import FastAPI from langchain . schema . runnable import RunnableLambda app = FastAPI ( ) def func ( x : Any ) - > int : \"\"\"Mistyped function that should accept an int but accepts anything.\"\"\" return x + 1 runnable = RunnableLambda ( func ) . with_types ( input_schema = int , ) add_routes ( app , runnable ) Custom User Types ​ Inherit from CustomUserType if you want the data to de-serialize into a\npydantic model rather than the equivalent dict representation. At the moment, this type only works server side and is used\nto specify desired decoding behavior. If inheriting from this type\nthe server will keep the decoded type as a pydantic model instead\nof converting it into a dict. from fastapi import FastAPI from langchain . schema . runnable import RunnableLambda from langserve import add_routes from langserve . schema import CustomUserType app = FastAPI ( ) class Foo ( CustomUserType ) : bar : int def func ( foo : Foo ) - > int : \"\"\"Sample function that expects a Foo type which is a pydantic model\"\"\" assert isinstance ( foo , Foo ) return foo . bar # Note that the input and output type are automatically inferred! # You do not need to specify them. # runnable = RunnableLambda(func).with_types( # <-- Not needed in this case #     input_schema=Foo, #     output_schema=int, # add_routes ( app , RunnableLambda ( func ) , path = \"/foo\" ) Playground Widgets ​ The playground allows you to define custom widgets for your runnable from the backend. A widget is specified at the field level and shipped as part of the JSON schema of the input type A widget must contain a key called type with the value being one of a well known list of widgets Other widget keys will be associated with values that describe paths in a JSON object General schema: type JsonPath = number | string | ( number | string ) [ ] ; type NameSpacedPath = { title : string ; path : JsonPath } ; // Using title to mimick json schema, but can use namespace type OneOfPath = { oneOf : JsonPath [ ] } ; type Widget = { type : string // Some well known type (e.g., base64file, chat etc.) [ key : string ] : JsonPath | NameSpacedPath | OneOfPath ; } ; Available Widgets ​ There are only two widgets that the user can specify manually right now: File Upload Widget Chat History Widget See below more information about these widgets. All other widgets on the playground UI are created and managed automatically by the UI\nbased on the config schema of the Runnable. When you create Configurable Runnables,\nthe playground should create appropriate widgets for you to control the behavior. File Upload Widget ​ Allows creation of a file upload input in the UI playground for files\nthat are uploaded as base64 encoded strings. Here's the full example . Snippet: try : from pydantic . v1 import Field except ImportError : from pydantic import Field from langserve import CustomUserType # ATTENTION: Inherit from CustomUserType instead of BaseModel otherwise #            the server will decode it into a dict instead of a pydantic model. class FileProcessingRequest ( CustomUserType ) : \"\"\"Request including a base64 encoded file.\"\"\" # The extra field is used to specify a widget for the playground UI. file : str = Field ( . . . , extra = { \"widget\" : { \"type\" : \"base64file\" } } ) num_chars : int = 100 Example widget: Chat Widget ​ Look at widget example . To define a chat widget, make sure that you pass \"type\": \"chat\". \"input\" is JSONPath to the field in the Request that has the new input message. \"output\" is JSONPath to the field in the Response that has new output message(s). Don't specify these fields if the entire input or output should be used as they are (e.g., if the output is a list of chat messages.) Here's a snippet: class ChatHistory ( CustomUserType ) : chat_history : List [ Tuple [ str , str ] ] = Field ( . . . , examples = [ [ ( \"human input\" , \"ai response\" ) ] ] , extra = { \"widget\" : { \"type\" : \"chat\" , \"input\" : \"question\" , \"output\" : \"answer\" } } , ) question : str def _format_to_messages ( input : ChatHistory ) - > List [ BaseMessage ] : \"\"\"Format the input to a list of messages.\"\"\" history = input . chat_history user_input = input . question messages = [ ] for human , ai in history : messages . append ( HumanMessage ( content = human ) ) messages . append ( AIMessage ( content = ai ) ) messages . append ( HumanMessage ( content = user_input ) ) return messages model = ChatOpenAI ( ) chat_model = RunnableParallel ( { \"answer\" : ( RunnableLambda ( _format_to_messages ) | model ) } ) add_routes ( app , chat_model . with_types ( input_type = ChatHistory ) , config_keys = [ \"configurable\" ] , path = \"/chat\" , ) Example widget: Enabling / Disabling Endpoints (LangServe >=0.0.33) ​ You can enable / disable which endpoints are exposed. Use enabled_endpoints if you want to make sure to never get a new endpoint when upgrading langserve to a newer verison. Enable: The code below will only enable invoke , batch and the corresponding config_hash endpoint variants. add_routes ( app , chain , enabled_endpoints = [ \"invoke\" , \"batch\" , \"config_hashes\" ] ) Disable: The code below will disable the playground for the chain add_routes ( app , chain , disabled_endpoints = [ \"playground\" ] ) Previous Token counting Next LangSmith"
    },
    {
        "title": "LangSmith | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/langsmith/",
        "text_content": "LangSmith LangSmith LangSmith helps you trace and evaluate your language model applications and intelligent agents to help you\nmove from prototype to production. Check out the interactive walkthrough to get started. For more information, please refer to the LangSmith documentation . For tutorials and other end-to-end examples demonstrating ways to integrate LangSmith in your workflow,\ncheck out the LangSmith Cookbook . Some of the guides therein include: Leveraging user feedback in your JS application ( link ). Building an automated feedback pipeline ( link ). How to evaluate and audit your RAG workflows ( link ). How to fine-tune a LLM on real usage data ( link ). How to use the LangChain Hub to version your prompts ( link ) Previous 🦜️🏓 LangServe Next LangSmith"
    },
    {
        "title": "Templates | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates",
        "text_content": "Templates On this page Templates Highlighting a few different categories of templates ⭐ Popular ​ These are some of the more popular templates to get started with. Retrieval Augmented Generation Chatbot : Build a chatbot over your data. Defaults to OpenAI and Pinecone. Extraction with OpenAI Functions : Do extraction of structured data from unstructured data. Uses OpenAI function calling. Local Retrieval Augmented Generation : Build a chatbot over your data. Uses only local tooling: Ollama, GPT4all, Chroma. OpenAI Functions Agent : Build a chatbot that can take actions. Uses OpenAI function calling and Tavily. XML Agent : Build a chatbot that can take actions. Uses Anthropic and You.com. 📥 Advanced Retrieval ​ These templates cover advanced retrieval techniques, which can be used for chat and QA over databases or documents. Reranking : This retrieval technique uses Cohere's reranking endpoint to rerank documents from an initial retrieval step. Anthropic Iterative Search : This retrieval technique uses iterative prompting to determine what to retrieve and whether the retriever documents are good enough. Parent Document Retrieval using Neo4j or MongoDB : This retrieval technique stores embeddings for smaller chunks, but then returns larger chunks to pass to the model for generation. Semi-Structured RAG : The template shows how to do retrieval over semi-structured data (e.g. data that involves both text and tables). Temporal RAG : The template shows how to do hybrid search over data with a time-based component using Timescale Vector . 🔍Advanced Retrieval - Query Transformation ​ A selection of advanced retrieval methods that involve transforming the original user query, which can improve retrieval quality. Hypothetical Document Embeddings : A retrieval technique that generates a hypothetical document for a given query, and then uses the embedding of that document to do semantic search. Paper . Rewrite-Retrieve-Read : A retrieval technique that rewrites a given query before passing it to a search engine. Paper . Step-back QA Prompting : A retrieval technique that generates a \"step-back\" question and then retrieves documents relevant to both that question and the original question. Paper . RAG-Fusion : A retrieval technique that generates multiple queries and then reranks the retrieved documents using reciprocal rank fusion. Article . Multi-Query Retriever : This retrieval technique uses an LLM to generate multiple queries and then fetches documents for all queries. 🧠Advanced Retrieval - Query Construction ​ A selection of advanced retrieval methods that involve constructing a query in a separate DSL from natural language, which enable natural languge chat over various structured databases. Elastic Query Generator : Generate elastic search queries from natural language. Neo4j Cypher Generation : Generate cypher statements from natural language. Available with a \"full text\" option as well. Supabase Self Query : Parse a natural language query into a semantic query as well as a metadata filter for Supabase. 🦙 OSS Models ​ These templates use OSS models, which enable privacy for sensitive data. Local Retrieval Augmented Generation : Build a chatbot over your data. Uses only local tooling: Ollama, GPT4all, Chroma. SQL Question Answering (Replicate) : Question answering over a SQL database, using Llama2 hosted on Replicate . SQL Question Answering (LlamaCpp) : Question answering over a SQL database, using Llama2 through LlamaCpp . SQL Question Answering (Ollama) : Question answering over a SQL database, using Llama2 through Ollama . ⛏️ Extraction ​ These templates extract data in a structured format based upon a user-specified schema. Extraction Using OpenAI Functions : Extract information from text using OpenAI Function Calling. Extraction Using Anthropic Functions : Extract information from text using a LangChain wrapper around the Anthropic endpoints intended to simulate function calling. Extract BioTech Plate Data : Extract microplate data from messy Excel spreadsheets into a more normalized format. ⛏️Summarization and tagging ​ These templates summarize or categorize documents and text. Summarization using Anthropic : Uses Anthropic's Claude2 to summarize long documents. 🤖 Agents ​ These templates build chatbots that can take actions, helping to automate tasks. OpenAI Functions Agent : Build a chatbot that can take actions. Uses OpenAI function calling and Tavily. XML Agent : Build a chatbot that can take actions. Uses Anthropic and You.com. 🚨 Safety and evaluation ​ These templates enable moderation or evaluation of LLM outputs. Guardrails Output Parser : Use guardrails-ai to validate LLM output. Chatbot Feedback : Use LangSmith to evaluate chatbot responses. Next anthropic-iterative-search"
    },
    {
        "title": "LangSmith | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/langsmith",
        "text_content": "LangSmith LangSmith LangSmith helps you trace and evaluate your language model applications and intelligent agents to help you\nmove from prototype to production. Check out the interactive walkthrough to get started. For more information, please refer to the LangSmith documentation . For tutorials and other end-to-end examples demonstrating ways to integrate LangSmith in your workflow,\ncheck out the LangSmith Cookbook . Some of the guides therein include: Leveraging user feedback in your JS application ( link ). Building an automated feedback pipeline ( link ). How to evaluate and audit your RAG workflows ( link ). How to fine-tune a LLM on real usage data ( link ). How to use the LangChain Hub to version your prompts ( link ) Previous 🦜️🏓 LangServe Next LangSmith"
    },
    {
        "title": "How to | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/how_to",
        "text_content": "LangChain Expression Language How to How to 📄️ RunnableParallel: Manipulating data manipulating-inputs-output} 📄️ RunnablePassthrough: Passing data through passing-data-through} 📄️ RunnableLambda: Run Custom Functions run-custom-functions} 📄️ RunnableBranch: Dynamically route logic based on input dynamically-route-logic-based-on-input} 📄️ Bind runtime args Sometimes we want to invoke a Runnable within a Runnable sequence with 📄️ Configure chain internals at runtime Oftentimes you may want to experiment with, or even expose to the end 📄️ Add fallbacks There are many possible points of failure in an LLM application, whether 📄️ Stream custom generator functions You can use generator functions (ie. functions that use the yield 📄️ Add message history (memory) The RunnableWithMessageHistory let’s us add message history to certain Previous Interface Next RunnableParallel: Manipulating data"
    },
    {
        "title": "Cookbook | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/cookbook",
        "text_content": "LangChain Expression Language Cookbook Cookbook Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start. 📄️ Prompt + LLM The most common and valuable composition is taking: 📄️ RAG Let’s look at adding in a retrieval step to a prompt and LLM, which adds 📄️ Multiple chains Runnables can easily be used to string together multiple Chains 📄️ Querying a SQL DB We can replicate our SQLDatabaseChain with Runnables. 📄️ Agents You can pass a Runnable into an agent. 📄️ Code writing Example of how to use LCEL to write Python code. 📄️ Routing by semantic similarity With LCEL you can easily add [custom routing 📄️ Adding memory This shows how to add memory to an arbitrary chain. Right now, you can 📄️ Adding moderation This shows how to add in moderation (or other safeguards) around your 📄️ Managing prompt size Agents dynamically call tools. The results of those tool calls are added 📄️ Using tools You can use any Tools with Runnables easily. Previous Add message history (memory) Next Prompt + LLM"
    },
    {
        "title": "Retrieval-augmented generation (RAG) | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/question_answering/",
        "text_content": "Retrieval-augmented generation (RAG) On this page Retrieval-augmented generation (RAG) Open In Colab Overview ​ What is RAG? ​ RAG is a technique for augmenting LLM knowledge with additional, often\nprivate or real-time, data. LLMs can reason about wide-ranging topics, but their knowledge is\nlimited to the public data up to a specific point in time that they were\ntrained on. If you want to build AI applications that can reason about\nprivate data or data introduced after a model’s cutoff date, you need to\naugment the knowledge of the model with the specific information it\nneeds. The process of bringing the appropriate information and inserting\nit into the model prompt is known as Retrieval Augmented Generation\n(RAG). What’s in this guide? ​ LangChain has a number of components specifically designed to help build\nRAG applications. To familiarize ourselves with these, we’ll build a\nsimple question-answering application over a text data source.\nSpecifically, we’ll build a QA bot over the LLM Powered Autonomous\nAgents blog post\nby Lilian Weng. Along the way we’ll go over a typical QA architecture,\ndiscuss the relevant LangChain components, and highlight additional\nresources for more advanced QA techniques. We’ll also see how LangSmith\ncan help us trace and understand our application. LangSmith will become\nincreasingly helpful as our application grows in complexity. Note Here we focus on RAG for unstructured data. Two RAG use cases\nwhich we cover elsewhere are: - QA over structured\ndata (e.g., SQL) - QA over\ncode (e.g.,\nPython) Architecture ​ A typical RAG application has two main components: Indexing : a pipeline for ingesting data from a source and indexing\nit. This usually happen offline. Retrieval and generation : the actual RAG chain, which takes the user\nquery at run time and retrieves the relevant data from the index, then\npasses that to the model. The most common full sequence from raw data to answer looks like: Indexing ​ Load : First we need to load our data. We’ll use DocumentLoaders for this. Split : Text\nsplitters break large Documents into smaller chunks. This is useful both for\nindexing data and for passing it in to a model, since large chunks\nare harder to search over and won’t fit in a model’s finite context\nwindow. Store : We need somewhere to store and index our splits, so that\nthey can later be searched over. This is often done using a VectorStore and Embeddings model. Retrieval and generation ​ Retrieve : Given a user input, relevant splits are retrieved from\nstorage using a Retriever . Generate : A ChatModel / LLM produces an answer using a\nprompt that includes the question and the retrieved data Setup ​ Dependencies ​ We’ll use an OpenAI chat model and embeddings and a Chroma vector store\nin this walkthrough, but everything shown here works with any ChatModel or LLM , Embeddings , and VectorStore or Retriever . We’ll use the following packages: !pip install - U langchain openai chromadb langchainhub bs4 We need to set environment variable OPENAI_API_KEY , which can be done\ndirectly or loaded from a .env file like so: import getpass import os os . environ [ \"OPENAI_API_KEY\" ] = getpass . getpass ( ) # import dotenv # dotenv.load_dotenv() LangSmith ​ Many of the applications you build with LangChain will contain multiple\nsteps with multiple invocations of LLM calls. As these applications get\nmore and more complex, it becomes crucial to be able to inspect what\nexactly is going on inside your chain or agent. The best way to do this\nis with LangSmith . Note that LangSmith is not needed, but it is helpful. If you do want to\nuse LangSmith, after you sign up at the link above, make sure to set\nyour environment variables to start logging traces: os . environ [ \"LANGCHAIN_TRACING_V2\" ] = \"true\" os . environ [ \"LANGCHAIN_API_KEY\" ] = getpass . getpass ( ) Quickstart ​ Suppose we want to build a QA app over the LLM Powered Autonomous\nAgents blog post\nby Lilian Weng. We can create a simple pipeline for this in ~20 lines of\ncode: import bs4 from langchain import hub from langchain . chat_models import ChatOpenAI from langchain . document_loaders import WebBaseLoader from langchain . embeddings import OpenAIEmbeddings from langchain . schema import StrOutputParser from langchain . text_splitter import RecursiveCharacterTextSplitter from langchain . vectorstores import Chroma from langchain_core . runnables import RunnablePassthrough loader = WebBaseLoader ( web_paths = ( \"https://lilianweng.github.io/posts/2023-06-23-agent/\" , ) , bs_kwargs = dict ( parse_only = bs4 . SoupStrainer ( class_ = ( \"post-content\" , \"post-title\" , \"post-header\" ) ) ) , ) docs = loader . load ( ) text_splitter = RecursiveCharacterTextSplitter ( chunk_size = 1000 , chunk_overlap = 200 ) splits = text_splitter . split_documents ( docs ) vectorstore = Chroma . from_documents ( documents = splits , embedding = OpenAIEmbeddings ( ) ) retriever = vectorstore . as_retriever ( ) prompt = hub . pull ( \"rlm/rag-prompt\" ) llm = ChatOpenAI ( model_name = \"gpt-3.5-turbo\" , temperature = 0 ) def format_docs ( docs ) : return \"\\n\\n\" . join ( doc . page_content for doc in docs ) rag_chain = ( { \"context\" : retriever | format_docs , \"question\" : RunnablePassthrough ( ) } | prompt | llm | StrOutputParser ( ) ) rag_chain . invoke ( \"What is Task Decomposition?\" ) 'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through prompting techniques like Chain of Thought or Tree of Thoughts, or by using task-specific instructions or human inputs. Task decomposition helps agents plan ahead and manage complicated tasks more effectively.' # cleanup vectorstore . delete_collection ( ) Check out the LangSmith\ntrace Detailed walkthrough ​ Let’s go through the above code step-by-step to really understand what’s\ngoing on. Step 1. Load ​ We need to first load the blog post contents. We can use DocumentLoader s for this, which are objects that load in data from a\nsource as Documents . A Document is an object with page_content (str) and metadata (dict) attributes. In this case we’ll use the WebBaseLoader , which uses urllib and BeautifulSoup to load and parse the passed in web urls, returning one Document per url. We can customize the html - > text parsing by\npassing in parameters to the BeautifulSoup parser via bs_kwargs (see BeautifulSoup\ndocs ).\nIn this case only HTML tags with class “post-content”, “post-title”, or\n“post-header” are relevant, so we’ll remove all others. from langchain . document_loaders import WebBaseLoader loader = WebBaseLoader ( web_paths = ( \"https://lilianweng.github.io/posts/2023-06-23-agent/\" , ) , bs_kwargs = { \"parse_only\" : bs4 . SoupStrainer ( class_ = ( \"post-content\" , \"post-title\" , \"post-header\" ) ) } , ) docs = loader . load ( ) len ( docs [ 0 ] . page_content ) 42824 print ( docs [ 0 ] . page_content [ : 500 ] ) LLM Powered Autonomous Agents Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview# In Go deeper ​ DocumentLoader : Object that load data from a source as Documents . - Docs : Further\ndocumentation on how to use DocumentLoader s. - Integrations : Find the relevant DocumentLoader integration (of the > 160 of them) for your use case. Step 2. Split ​ Our loaded document is over 42k characters long. This is too long to fit\nin the context window of many models. And even for those models that\ncould fit the full post in their context window, empirically models\nstruggle to find the relevant context in very long prompts. So we’ll split the Document into chunks for embedding and vector\nstorage. This should help us retrieve only the most relevant bits of the\nblog post at run time. In this case we’ll split our documents into chunks of 1000 characters\nwith 200 characters of overlap between chunks. The overlap helps\nmitigate the possibility of separating a statement from important\ncontext related to it. We use the RecursiveCharacterTextSplitter ,\nwhich will (recursively) split the document using common separators\n(like new lines) until each chunk is the appropriate size. from langchain . text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter ( chunk_size = 1000 , chunk_overlap = 200 , add_start_index = True ) all_splits = text_splitter . split_documents ( docs ) len ( all_splits ) 66 len ( all_splits [ 0 ] . page_content ) 969 all_splits [ 10 ] . metadata {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 7056} Go deeper ​ DocumentSplitter : Object that splits a list of Document s into\nsmaller chunks. Subclass of DocumentTransformer s. - Explore Context-aware splitters , which keep the location (“context”) of each\nsplit in the original Document : - Markdown\nfiles - Code (py or js) - Scientific papers DocumentTransformer : Object that performs a transformation on a list\nof Document s. - Docs : Further\ndocumentation on how to use DocumentTransformer s - Integrations Step 3. Store ​ Now that we’ve got 66 text chunks in memory, we need to store and index\nthem so that we can search them later in our RAG app. The most common\nway to do this is to embed the contents of each document split and\nupload those embeddings to a vector store. Then, when we want to search over our splits, we take the search query,\nembed it as well, and perform some sort of “similarity” search to\nidentify the stored splits with the most similar embeddings to our query\nembedding. The simplest similarity measure is cosine similarity — we\nmeasure the cosine of the angle between each pair of embeddings (which\nare just very high dimensional vectors). We can embed and store all of our document splits in a single command\nusing the Chroma vector store and OpenAIEmbeddings model. from langchain . embeddings import OpenAIEmbeddings from langchain . vectorstores import Chroma vectorstore = Chroma . from_documents ( documents = all_splits , embedding = OpenAIEmbeddings ( ) ) Go deeper ​ Embeddings : Wrapper around a text embedding model, used for converting\ntext to embeddings. - Docs : Further\ndocumentation on the interface. - Integrations : Browse the > 30\ntext embedding integrations VectorStore : Wrapper around a vector database, used for storing and\nquerying embeddings. - Docs : Further\ndocumentation on the interface. - Integrations : Browse the > 40 VectorStore integrations. This completes the Indexing portion of the pipeline. At this point\nwe have an query-able vector store containing the chunked contents of\nour blog post. Given a user question, we should ideally be able to\nreturn the snippets of the blog post that answer the question: Step 4. Retrieve ​ Now let’s write the actual application logic. We want to create a simple\napplication that let’s the user ask a question, searches for documents\nrelevant to that question, passes the retrieved documents and initial\nquestion to a model, and finally returns an answer. LangChain defines a Retriever interface which wraps an index that can\nreturn relevant documents given a string query. All retrievers implement\na common method get_relevant_documents() (and its asynchronous variant aget_relevant_documents() ). The most common type of Retriever is the VectorStoreRetriever , which\nuses the similarity search capabilities of a vector store to facillitate\nretrieval. Any VectorStore can easily be turned into a Retriever : retriever = vectorstore . as_retriever ( search_type = \"similarity\" , search_kwargs = { \"k\" : 6 } ) retrieved_docs = retriever . get_relevant_documents ( \"What are the approaches to Task Decomposition?\" ) len ( retrieved_docs ) 6 print ( retrieved_docs [ 0 ] . page_content ) Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote. Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs. Go deeper ​ Vector stores are commonly used for retrieval, but there are plenty of\nother ways to do retrieval. Retriever : An object that returns Document s given a text query - Docs : Further documentation\non the interface and built-in retrieval techniques. Some of which\ninclude: - MultiQueryRetriever generates variants of the input\nquestion to improve retrieval hit rate. - MultiVectorRetriever (diagram below)\ninstead generates variants of the\nembeddings , also\nin order to improve retrieval hit rate. - Max marginal relevance selects for relevance and\ndiversity among the retrieved documents to avoid passing in duplicate context. -\nDocuments can be filtered during vector store retrieval using metadata filters . - Integrations : Integrations with\nretrieval services. Step 5. Generate ​ Let’s put it all together into a chain that takes a question, retrieves\nrelevant documents, constructs a prompt, passes that to a model, and\nparses the output. We’ll use the gpt-3.5-turbo OpenAI chat model, but any LangChain LLM or ChatModel could be substituted in. from langchain . chat_models import ChatOpenAI llm = ChatOpenAI ( model_name = \"gpt-3.5-turbo\" , temperature = 0 ) We’ll use a prompt for RAG that is checked into the LangChain prompt hub\n( here ). from langchain import hub prompt = hub . pull ( \"rlm/rag-prompt\" ) print ( prompt . invoke ( { \"context\" : \"filler context\" , \"question\" : \"filler question\" } ) . to_string ( ) ) Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Question: filler question Context: filler context Answer: We’ll use the LCEL\nRunnable protocol to define the chain, allowing us to - pipe together components\nand functions in a transparent way - automatically trace our chain in\nLangSmith - get streaming, async, and batched calling out of the box from langchain . schema import StrOutputParser from langchain_core . runnables import RunnablePassthrough def format_docs ( docs ) : return \"\\n\\n\" . join ( doc . page_content for doc in docs ) rag_chain = ( { \"context\" : retriever | format_docs , \"question\" : RunnablePassthrough ( ) } | prompt | llm | StrOutputParser ( ) ) for chunk in rag_chain . stream ( \"What is Task Decomposition?\" ) : print ( chunk , end = \"\" , flush = True ) Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through methods like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the task into manageable subtasks and exploring multiple reasoning possibilities at each step. Task decomposition can be performed by AI models with prompting, task-specific instructions, or human inputs. Check out the LangSmith\ntrace Go deeper ​ Choosing LLMs ​ ChatModel : An LLM-backed chat model wrapper. Takes in a sequence of\nmessages and returns a message. - Docs - Integrations : Explore over 25 ChatModel integrations. LLM : A text-in-text-out LLM. Takes in a string and returns a string. - Docs - Integrations : Explore over 75 LLM integrations. See a guide on RAG with locally-running models here . Customizing the prompt ​ As shown above, we can load prompts (e.g., this RAG\nprompt ) from the prompt\nhub. The prompt can also be easily customized: from langchain . prompts import PromptTemplate template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum and keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. {context} Question: {question} Helpful Answer:\"\"\" rag_prompt_custom = PromptTemplate . from_template ( template ) rag_chain = ( { \"context\" : retriever | format_docs , \"question\" : RunnablePassthrough ( ) } | rag_prompt_custom | llm | StrOutputParser ( ) ) rag_chain . invoke ( \"What is Task Decomposition?\" ) 'Task decomposition is the process of breaking down a complex task into smaller and simpler steps. It can be done through techniques like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the problem into multiple thought steps and generating multiple thoughts per step. Task decomposition helps in enhancing model performance and understanding the thinking process of the model. Thanks for asking!' Check out the LangSmith\ntrace Adding sources ​ With LCEL it’s easy to return the retrieved documents or certain source\nmetadata from the documents: from operator import itemgetter from langchain_core . runnables import RunnableParallel rag_chain_from_docs = ( { \"context\" : lambda input : format_docs ( input [ \"documents\" ] ) , \"question\" : itemgetter ( \"question\" ) , } | rag_prompt_custom | llm | StrOutputParser ( ) ) rag_chain_with_source = RunnableParallel ( { \"documents\" : retriever , \"question\" : RunnablePassthrough ( ) } ) | { \"documents\" : lambda input : [ doc . metadata for doc in input [ \"documents\" ] ] , \"answer\" : rag_chain_from_docs , } rag_chain_with_source . invoke ( \"What is Task Decomposition\" ) {'documents': [{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17804}, {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17414}, {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}, {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}], 'answer': 'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!'} Check out the LangSmith\ntrace Adding memory ​ Suppose we want to create a stateful application that remembers past\nuser inputs. There are two main things we need to do to support this. 1.\nAdd a messages placeholder to our chain which allows us to pass in\nhistorical messages 2. Add a chain that takes the latest user query and\nreformulates it in the context of the chat history into a standalone\nquestion that can be passed to our retriever. Let’s start with 2. We can build a “condense question” chain that looks\nsomething like this: from langchain . prompts import ChatPromptTemplate , MessagesPlaceholder condense_q_system_prompt = \"\"\"Given a chat history and the latest user question \\ which might reference the chat history, formulate a standalone question \\ which can be understood without the chat history. Do NOT answer the question, \\ just reformulate it if needed and otherwise return it as is.\"\"\" condense_q_prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , condense_q_system_prompt ) , MessagesPlaceholder ( variable_name = \"chat_history\" ) , ( \"human\" , \"{question}\" ) , ] ) condense_q_chain = condense_q_prompt | llm | StrOutputParser ( ) from langchain_core . messages import AIMessage , HumanMessage condense_q_chain . invoke ( { \"chat_history\" : [ HumanMessage ( content = \"What does LLM stand for?\" ) , AIMessage ( content = \"Large language model\" ) , ] , \"question\" : \"What is meant by large\" , } ) 'What is the definition of \"large\" in the context of a language model?' condense_q_chain . invoke ( { \"chat_history\" : [ HumanMessage ( content = \"What does LLM stand for?\" ) , AIMessage ( content = \"Large language model\" ) , ] , \"question\" : \"How do transformers work\" , } ) 'How do transformer models function?' And now we can build our full QA chain. Notice we add some routing\nfunctionality to only run the “condense question chain” when our chat\nhistory isn’t empty. qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\ Use the following pieces of retrieved context to answer the question. \\ If you don't know the answer, just say that you don't know. \\ Use three sentences maximum and keep the answer concise.\\ {context}\"\"\" qa_prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , qa_system_prompt ) , MessagesPlaceholder ( variable_name = \"chat_history\" ) , ( \"human\" , \"{question}\" ) , ] ) def condense_question ( input : dict ) : if input . get ( \"chat_history\" ) : return condense_q_chain else : return input [ \"question\" ] rag_chain = ( RunnablePassthrough . assign ( context = condense_question | retriever | format_docs ) | qa_prompt | llm ) chat_history = [ ] question = \"What is Task Decomposition?\" ai_msg = rag_chain . invoke ( { \"question\" : question , \"chat_history\" : chat_history } ) chat_history . extend ( [ HumanMessage ( content = question ) , ai_msg ] ) second_question = \"What are common ways of doing it?\" rag_chain . invoke ( { \"question\" : second_question , \"chat_history\" : chat_history } ) AIMessage(content='Common ways of task decomposition include:\\n\\n1. Using Chain of Thought (CoT): CoT is a prompting technique that instructs the model to \"think step by step\" and decompose complex tasks into smaller and simpler steps. It utilizes more test-time computation and sheds light on the model\\'s thinking process.\\n\\n2. Prompting with LLM: Language Model (LLM) can be used to prompt the model with simple instructions like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" This allows the model to generate a sequence of subtasks or thought steps.\\n\\n3. Task-specific instructions: For certain tasks, task-specific instructions can be provided to guide the model in decomposing the task. For example, for writing a novel, the instruction \"Write a story outline\" can be given to break down the task into manageable steps.\\n\\n4. Human inputs: In some cases, human inputs can be used to assist in task decomposition. Humans can provide their expertise and knowledge to identify and break down complex tasks into smaller subtasks.') Check out the LangSmith\ntrace Here we’ve gone over how to add chain logic for incorporating historical\noutputs. But how do we actually store and retrieve historical outputs\nfor different sessions? For that check out the LCEL How to add message\nhistory (memory) page. Next steps ​ That’s a lot of content we’ve covered in a short amount of time. There’s\nplenty of nuances, features, integrations, etc to explore in each of the\nabove sections. Aside from the sources mentioned above, good next steps\ninclude: Reading up on more advanced retrieval techniques in the Retrievers section. Learning about the LangChain Indexing\nAPI , which helps repeatedly\nsync data sources and vector stores without redundant computation or\nstorage. Exploring RAG LangChain\nTemplates , which are\nreference applications that can easily be deployed with LangServe . Learning about evaluating RAG applications with\nLangSmith . Previous SQL Next RAG over code"
    },
    {
        "title": "Chatbots | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/chatbots/",
        "text_content": "Chatbots On this page Chatbots Open In Colab Use case ​ Chatbots are one of the central LLM use-cases. The core features of\nchatbots are that they can have long-running conversations and have\naccess to information that users want to know about. Aside from basic prompting and LLMs, memory and retrieval are the core\ncomponents of a chatbot. Memory allows a chatbot to remember past\ninteractions, and retrieval provides a chatbot with up-to-date,\ndomain-specific information. Overview ​ The chat model interface is based around messages rather than raw text.\nSeveral components are important to consider for chat: chat model : See here for a list of chat\nmodel integrations and here for\ndocumentation on the chat model interface in LangChain. You can use LLMs (see here ) for chatbots as\nwell, but chat models have a more conversational tone and natively\nsupport a message interface. prompt template : Prompt templates make it easy to assemble prompts\nthat combine default messages, user input, chat history, and\n(optionally) additional retrieved context. memory : See here for in-depth\ndocumentation on memory types retriever (optional): See\nhere for in-depth\ndocumentation on retrieval systems. These are useful if you want to\nbuild a chatbot with domain-specific knowledge. Quickstart ​ Here’s a quick preview of how we can create chatbot interfaces. First\nlet’s install some dependencies and set the required credentials: !pip install langchain openai # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv() With a plain chat model, we can get chat completions by passing one or\nmore messages to the model. The chat model will respond with a message. from langchain . chat_models import ChatOpenAI from langchain . schema import HumanMessage , SystemMessage chat = ChatOpenAI ( ) chat ( [ HumanMessage ( content = \"Translate this sentence from English to French: I love programming.\" ) ] ) AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False) And if we pass in a list of messages: messages = [ SystemMessage ( content = \"You are a helpful assistant that translates English to French.\" ) , HumanMessage ( content = \"I love programming.\" ) , ] chat ( messages ) AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False) We can then wrap our chat model in a ConversationChain , which has\nbuilt-in memory for remembering past user inputs and model outputs. from langchain . chains import ConversationChain conversation = ConversationChain ( llm = chat ) conversation . run ( \"Translate this sentence from English to French: I love programming.\" ) 'Je adore la programmation.' conversation . run ( \"Translate it to German.\" ) 'Ich liebe Programmieren.' Memory ​ As we mentioned above, the core component of chatbots is the memory\nsystem. One of the simplest and most commonly used forms of memory is ConversationBufferMemory : This memory allows for storing of messages in a buffer When called in a chain, it returns all of the messages it has stored LangChain comes with many other types of memory, too. See\nhere for in-depth documentation on memory types. For now let’s take a quick look at ConversationBufferMemory. We can\nmanually add a few chat messages to the memory like so: from langchain . memory import ConversationBufferMemory memory = ConversationBufferMemory ( ) memory . chat_memory . add_user_message ( \"hi!\" ) memory . chat_memory . add_ai_message ( \"whats up?\" ) And now we can load from our memory. The key method exposed by all Memory classes is load_memory_variables . This takes in any initial\nchain input and returns a list of memory variables which are added to\nthe chain input. Since this simple memory type doesn’t actually take into account the\nchain input when loading memory, we can pass in an empty input for now: memory . load_memory_variables ( { } ) {'history': 'Human: hi!\\nAI: whats up?'} We can also keep a sliding window of the most recent k interactions\nusing ConversationBufferWindowMemory . from langchain . memory import ConversationBufferWindowMemory memory = ConversationBufferWindowMemory ( k = 1 ) memory . save_context ( { \"input\" : \"hi\" } , { \"output\" : \"whats up\" } ) memory . save_context ( { \"input\" : \"not much you\" } , { \"output\" : \"not much\" } ) memory . load_memory_variables ( { } ) {'history': 'Human: not much you\\nAI: not much'} ConversationSummaryMemory is an extension of this theme. It creates a summary of the conversation over time. This memory is most useful for longer conversations where the full\nmessage history would consume many tokens. from langchain . llms import OpenAI from langchain . memory import ConversationSummaryMemory llm = OpenAI ( temperature = 0 ) memory = ConversationSummaryMemory ( llm = llm ) memory . save_context ( { \"input\" : \"hi\" } , { \"output\" : \"whats up\" } ) memory . save_context ( { \"input\" : \"im working on better docs for chatbots\" } , { \"output\" : \"oh, that sounds like a lot of work\" } , ) memory . save_context ( { \"input\" : \"yes, but it's worth the effort\" } , { \"output\" : \"agreed, good docs are important!\" } , ) memory . load_memory_variables ( { } ) {'history': '\\nThe human greets the AI, to which the AI responds. The human then mentions they are working on better docs for chatbots, to which the AI responds that it sounds like a lot of work. The human agrees that it is worth the effort, and the AI agrees that good docs are important.'} ConversationSummaryBufferMemory extends this a bit further: It uses token length rather than number of interactions to determine\nwhen to flush interactions. from langchain . memory import ConversationSummaryBufferMemory memory = ConversationSummaryBufferMemory ( llm = llm , max_token_limit = 10 ) memory . save_context ( { \"input\" : \"hi\" } , { \"output\" : \"whats up\" } ) memory . save_context ( { \"input\" : \"not much you\" } , { \"output\" : \"not much\" } ) Conversation ​ We can unpack what goes under the hood with ConversationChain . We can specify our memory, ConversationSummaryMemory and we can\nspecify the prompt. from langchain . chains import LLMChain from langchain . prompts import ( ChatPromptTemplate , HumanMessagePromptTemplate , MessagesPlaceholder , SystemMessagePromptTemplate , ) # LLM llm = ChatOpenAI ( ) # Prompt prompt = ChatPromptTemplate ( messages = [ SystemMessagePromptTemplate . from_template ( \"You are a nice chatbot having a conversation with a human.\" ) , # The `variable_name` here is what must align with memory MessagesPlaceholder ( variable_name = \"chat_history\" ) , HumanMessagePromptTemplate . from_template ( \"{question}\" ) , ] ) # Notice that we `return_messages=True` to fit into the MessagesPlaceholder # Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name memory = ConversationBufferMemory ( memory_key = \"chat_history\" , return_messages = True ) conversation = LLMChain ( llm = llm , prompt = prompt , verbose = True , memory = memory ) # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation ( { \"question\" : \"hi\" } ) > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi > Finished chain. {'question': 'hi', 'chat_history': [HumanMessage(content='hi', additional_kwargs={}, example=False), AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False)], 'text': 'Hello! How can I assist you today?'} conversation ( { \"question\" : \"Translate this sentence from English to French: I love programming.\" } ) > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. > Finished chain. {'question': 'Translate this sentence from English to French: I love programming.', 'chat_history': [HumanMessage(content='hi', additional_kwargs={}, example=False), AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False), HumanMessage(content='Translate this sentence from English to French: I love programming.', additional_kwargs={}, example=False), AIMessage(content='Sure! The translation of \"I love programming\" from English to French is \"J\\'adore programmer.\"', additional_kwargs={}, example=False)], 'text': 'Sure! The translation of \"I love programming\" from English to French is \"J\\'adore programmer.\"'} conversation ( { \"question\" : \"Now translate the sentence to German.\" } ) > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. AI: Sure! The translation of \"I love programming\" from English to French is \"J'adore programmer.\" Human: Now translate the sentence to German. > Finished chain. {'question': 'Now translate the sentence to German.', 'chat_history': [HumanMessage(content='hi', additional_kwargs={}, example=False), AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False), HumanMessage(content='Translate this sentence from English to French: I love programming.', additional_kwargs={}, example=False), AIMessage(content='Sure! The translation of \"I love programming\" from English to French is \"J\\'adore programmer.\"', additional_kwargs={}, example=False), HumanMessage(content='Now translate the sentence to German.', additional_kwargs={}, example=False), AIMessage(content='Certainly! The translation of \"I love programming\" from English to German is \"Ich liebe das Programmieren.\"', additional_kwargs={}, example=False)], 'text': 'Certainly! The translation of \"I love programming\" from English to German is \"Ich liebe das Programmieren.\"'} We can see the chat history preserved in the prompt using the LangSmith\ntrace . Chat Retrieval ​ Now, suppose we want to chat with\ndocuments or some other source of knowledge. This is popular use case, combining chat with document\nretrieval . It allows us to chat with specific information that the model was not\ntrained on. !pip install tiktoken chromadb Load a blog post. from langchain . document_loaders import WebBaseLoader loader = WebBaseLoader ( \"https://lilianweng.github.io/posts/2023-06-23-agent/\" ) data = loader . load ( ) Split and store this in a vector. from langchain . text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter ( chunk_size = 500 , chunk_overlap = 0 ) all_splits = text_splitter . split_documents ( data ) from langchain . embeddings import OpenAIEmbeddings from langchain . vectorstores import Chroma vectorstore = Chroma . from_documents ( documents = all_splits , embedding = OpenAIEmbeddings ( ) ) Create our memory, as before, but’s let’s use ConversationSummaryMemory . memory = ConversationSummaryMemory ( llm = llm , memory_key = \"chat_history\" , return_messages = True ) from langchain . chains import ConversationalRetrievalChain from langchain . chat_models import ChatOpenAI llm = ChatOpenAI ( ) retriever = vectorstore . as_retriever ( ) qa = ConversationalRetrievalChain . from_llm ( llm , retriever = retriever , memory = memory ) qa ( \"How do agents use Task decomposition?\" ) {'question': 'How do agents use Task decomposition?', 'chat_history': [SystemMessage(content='', additional_kwargs={})], 'answer': 'Agents can use task decomposition in several ways:\\n\\n1. Simple prompting: Agents can use Language Model based prompting to break down tasks into subgoals. For example, by providing prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\", the agent can generate a sequence of smaller steps that lead to the completion of the overall task.\\n\\n2. Task-specific instructions: Agents can be given task-specific instructions to guide their planning process. For example, if the task is to write a novel, the agent can be instructed to \"Write a story outline.\" This provides a high-level structure for the task and helps in breaking it down into smaller components.\\n\\n3. Human inputs: Agents can also take inputs from humans to decompose tasks. This can be done through direct communication or by leveraging human expertise. Humans can provide guidance and insights to help the agent break down complex tasks into manageable subgoals.\\n\\nOverall, task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.'} qa ( \"What are the various ways to implement memory to support it?\" ) {'question': 'What are the various ways to implement memory to support it?', 'chat_history': [SystemMessage(content='The human asks how agents use task decomposition. The AI explains that agents can use task decomposition in several ways, including simple prompting, task-specific instructions, and human inputs. Task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.', additional_kwargs={})], 'answer': 'There are several ways to implement memory to support task decomposition:\\n\\n1. Long-Term Memory Management: This involves storing and organizing information in a long-term memory system. The agent can retrieve past experiences, knowledge, and learned strategies to guide the task decomposition process.\\n\\n2. Internet Access: The agent can use internet access to search for relevant information and gather resources to aid in task decomposition. This allows the agent to access a vast amount of information and utilize it in the decomposition process.\\n\\n3. GPT-3.5 Powered Agents: The agent can delegate simple tasks to GPT-3.5 powered agents. These agents can perform specific tasks or provide assistance in task decomposition, allowing the main agent to focus on higher-level planning and decision-making.\\n\\n4. File Output: The agent can store the results of task decomposition in files or documents. This allows for easy retrieval and reference during the execution of the task.\\n\\nThese memory resources help the agent in organizing and managing information, making informed decisions, and effectively decomposing complex tasks into smaller, manageable subgoals.'} Again, we can use the LangSmith\ntrace to explore the prompt structure. Going deeper ​ Agents, such as the conversational retrieval\nagent ,\ncan be used for retrieval when necessary while also holding a\nconversation. Previous Interacting with APIs Next Extraction"
    },
    {
        "title": "SQL | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/qa_structured/sql/",
        "text_content": "QA over structured data SQL On this page SQL Open In Colab Use case ​ Enterprise data is often stored in SQL databases. LLMs make it possible to interact with SQL databases using natural\nlanguage. LangChain offers SQL Chains and Agents to build and run SQL queries\nbased on natural language prompts. These are compatible with any SQL dialect supported by SQLAlchemy (e.g.,\nMySQL, PostgreSQL, Oracle SQL, Databricks, SQLite). They enable use cases such as: Generating queries that will be run based on natural language\nquestions Creating chatbots that can answer questions based on database data Building custom dashboards based on insights a user wants to analyze Overview ​ LangChain provides tools to interact with SQL Databases: Build SQL queries based on natural language user questions Query a SQL database using chains for query creation and execution Interact with a SQL database using agents for robust and flexible\nquerying Quickstart ​ First, get required packages and set environment variables: ! pip install langchain langchain - experimental openai # Set env var OPENAI_API_KEY or load from a .env file # import dotenv # dotenv.load_dotenv() The below example will use a SQLite connection with Chinook database. Follow installation\nsteps to create Chinook.db in the same directory as this notebook: Save this\nfile to the directory as Chinook_Sqlite.sql Run sqlite3 Chinook.db Run .read Chinook_Sqlite.sql Test SELECT * FROM Artist LIMIT 10; Now, Chinhook.db is in our directory. Let’s create a SQLDatabaseChain to create and execute SQL queries. from langchain . llms import OpenAI from langchain . utilities import SQLDatabase from langchain_experimental . sql import SQLDatabaseChain db = SQLDatabase . from_uri ( \"sqlite:///Chinook.db\" ) llm = OpenAI ( temperature = 0 , verbose = True ) db_chain = SQLDatabaseChain . from_llm ( llm , db , verbose = True ) db_chain . run ( \"How many employees are there?\" ) > Entering new SQLDatabaseChain chain... How many employees are there? SQLQuery:SELECT COUNT(*) FROM \"Employee\"; SQLResult: [(8,)] Answer:There are 8 employees. > Finished chain. 'There are 8 employees.' Note that this both creates and executes the query. In the following sections, we will cover the 3 different use cases\nmentioned in the overview. Go deeper ​ You can load tabular data from other sources other than SQL Databases.\nFor example: - Loading a CSV\nfile - Loading a Pandas\nDataFrame Here\nyou can check full list of Document\nLoaders Case 1: Text-to-SQL query ​ from langchain . chains import create_sql_query_chain from langchain . chat_models import ChatOpenAI Let’s create the chain that will build the SQL Query: chain = create_sql_query_chain ( ChatOpenAI ( temperature = 0 ) , db ) response = chain . invoke ( { \"question\" : \"How many employees are there\" } ) print ( response ) SELECT COUNT(*) FROM Employee After building the SQL query based on a user question, we can execute\nthe query: db . run ( response ) '[(8,)]' As we can see, the SQL Query Builder chain only created the query,\nand we handled the query execution separately . Go deeper ​ Looking under the hood We can look at the LangSmith\ntrace to unpack this: Some papers have reported good\nperformance when prompting with: A CREATE TABLE description for each table, which include column\nnames, their types, etc Followed by three example rows in a SELECT statement create_sql_query_chain adopts this the best practice (see more in this blog ). Improvements The query builder can be improved in several ways, such as (but not\nlimited to): Customizing database description to your specific use case Hardcoding a few examples of questions and their corresponding SQL\nquery in the prompt Using a vector database to include dynamic examples that are\nrelevant to the specific user question All these examples involve customizing the chain’s prompt. For example, we can include a few examples in our prompt like so: from langchain . prompts import PromptTemplate TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Use the following format: Question: \"Question here\" SQLQuery: \"SQL Query to run\" SQLResult: \"Result of the SQLQuery\" Answer: \"Final answer here\" Only use the following tables: {table_info}. Some examples of SQL queries that correspond to questions are: {few_shot_examples} Question: {input}\"\"\" CUSTOM_PROMPT = PromptTemplate ( input_variables = [ \"input\" , \"few_shot_examples\" , \"table_info\" , \"dialect\" ] , template = TEMPLATE , ) We can also access this prompt in the\nLangChain prompt hub. This will work with your LangSmith API\nkey . from langchain import hub CUSTOM_PROMPT = hub . pull ( \"rlm/text-to-sql\" ) Case 2: Text-to-SQL query and execution ​ We can use SQLDatabaseChain from langchain_experimental to create\nand run SQL queries. from langchain . llms import OpenAI from langchain_experimental . sql import SQLDatabaseChain llm = OpenAI ( temperature = 0 , verbose = True ) db_chain = SQLDatabaseChain . from_llm ( llm , db , verbose = True ) db_chain . run ( \"How many employees are there?\" ) > Entering new SQLDatabaseChain chain... How many employees are there? SQLQuery:SELECT COUNT(*) FROM \"Employee\"; SQLResult: [(8,)] Answer:There are 8 employees. > Finished chain. 'There are 8 employees.' As we can see, we get the same result as the previous case. Here, the chain also handles the query execution and provides a\nfinal answer based on the user question and the query result. Be careful while using this approach as it is susceptible to SQL Injection : The chain is executing queries that are created by an LLM, and\nweren’t validated e.g. records may be created, modified or deleted unintentionally _ This is why we see the SQLDatabaseChain is inside langchain_experimental . Go deeper ​ Looking under the hood We can use the LangSmith\ntrace to see what is happening under the hood: As discussed above, first we create the query: text: ' SELECT COUNT(*) FROM \"Employee\";' Then, it executes the query and passes the results to an LLM for\nsynthesis. Improvements The performance of the SQLDatabaseChain can be enhanced in several\nways: Adding sample rows Specifying custom table\ninformation Using Query\nChecker self-correct invalid SQL using parameter use_query_checker=True Customizing the LLM\nPrompt include\nspecific instructions or relevant information, using parameter prompt=CUSTOM_PROMPT Get intermediate\nsteps access the SQL statement as well as the final result using parameter return_intermediate_steps=True Limit the number of\nrows a query will return using parameter top_k=5 You might find SQLDatabaseSequentialChain useful for cases in which the number of tables in the database is large. This Sequential Chain handles the process of: Determining which tables to use based on the user question Calling the normal SQL database chain using only relevant tables Adding Sample Rows Providing sample data can help the LLM construct correct queries when\nthe data format is not obvious. For example, we can tell LLM that artists are saved with their full\nnames by providing two rows from the Track table. db = SQLDatabase . from_uri ( \"sqlite:///Chinook.db\" , include_tables = [ \"Track\" ] , # we include only one table to save tokens in the prompt :) sample_rows_in_table_info = 2 , ) The sample rows are added to the prompt after each corresponding table’s\ncolumn information. We can use db.table_info and check which sample rows are included: print ( db . table_info ) CREATE TABLE \"Track\" ( \"TrackId\" INTEGER NOT NULL, \"Name\" NVARCHAR(200) NOT NULL, \"AlbumId\" INTEGER, \"MediaTypeId\" INTEGER NOT NULL, \"GenreId\" INTEGER, \"Composer\" NVARCHAR(220), \"Milliseconds\" INTEGER NOT NULL, \"Bytes\" INTEGER, \"UnitPrice\" NUMERIC(10, 2) NOT NULL, PRIMARY KEY (\"TrackId\"), FOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), FOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), FOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\") ) /* 2 rows from Track table: TrackId Name    AlbumId MediaTypeId GenreId Composer    Milliseconds    Bytes   UnitPrice 1   For Those About To Rock (We Salute You) 1   1   1   Angus Young, Malcolm Young, Brian Johnson   343719  11170334    0.99 2   Balls to the Wall   2   2   1   None    342562  5510424 0.99 */ Case 3: SQL agents ​ LangChain has an SQL Agent which provides a more flexible way of\ninteracting with SQL Databases than the SQLDatabaseChain . The main advantages of using the SQL Agent are: It can answer questions based on the databases’ schema as well as on\nthe databases’ content (like describing a specific table) It can recover from errors by running a generated query, catching\nthe traceback and regenerating it correctly To initialize the agent, we use create_sql_agent function. This agent contains the SQLDatabaseToolkit which contains tools to: Create and execute queries Check query syntax Retrieve table descriptions … and more from langchain . agents import create_sql_agent from langchain . agents . agent_toolkits import SQLDatabaseToolkit # from langchain.agents import AgentExecutor from langchain . agents . agent_types import AgentType db = SQLDatabase . from_uri ( \"sqlite:///Chinook.db\" ) agent_executor = create_sql_agent ( llm = OpenAI ( temperature = 0 ) , toolkit = SQLDatabaseToolkit ( db = db , llm = OpenAI ( temperature = 0 ) ) , verbose = True , agent_type = AgentType . ZERO_SHOT_REACT_DESCRIPTION , ) Agent task example #1 - Running queries ​ agent_executor . run ( \"List the total sales per country. Which country's customers spent the most?\" ) > Entering new AgentExecutor chain... Action: sql_db_list_tables Action Input: Observation: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Thought: I should query the schema of the Invoice and Customer tables. Action: sql_db_schema Action Input: Invoice, Customer Observation: CREATE TABLE \"Customer\" ( \"CustomerId\" INTEGER NOT NULL, \"FirstName\" NVARCHAR(40) NOT NULL, \"LastName\" NVARCHAR(20) NOT NULL, \"Company\" NVARCHAR(80), \"Address\" NVARCHAR(70), \"City\" NVARCHAR(40), \"State\" NVARCHAR(40), \"Country\" NVARCHAR(40), \"PostalCode\" NVARCHAR(10), \"Phone\" NVARCHAR(24), \"Fax\" NVARCHAR(24), \"Email\" NVARCHAR(60) NOT NULL, \"SupportRepId\" INTEGER, PRIMARY KEY (\"CustomerId\"), FOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\") ) /* 3 rows from Customer table: CustomerId  FirstName   LastName    Company Address City    State   Country PostalCode  Phone   Fax Email   SupportRepId 1   Luís    Gonçalves   Embraer - Empresa Brasileira de Aeronáutica S.A.    Av. Brigadeiro Faria Lima, 2170 São José dos Campos SP  Brazil  12227-000   +55 (12) 3923-5555  +55 (12) 3923-5566  luisg@embraer.com.br    3 2   Leonie  Köhler  None    Theodor-Heuss-Straße 34 Stuttgart   None    Germany 70174   +49 0711 2842222    None    leonekohler@surfeu.de   5 3   François    Tremblay    None    1498 rue Bélanger   Montréal    QC  Canada  H2G 1A7 +1 (514) 721-4711   None    ftremblay@gmail.com 3 */ CREATE TABLE \"Invoice\" ( \"InvoiceId\" INTEGER NOT NULL, \"CustomerId\" INTEGER NOT NULL, \"InvoiceDate\" DATETIME NOT NULL, \"BillingAddress\" NVARCHAR(70), \"BillingCity\" NVARCHAR(40), \"BillingState\" NVARCHAR(40), \"BillingCountry\" NVARCHAR(40), \"BillingPostalCode\" NVARCHAR(10), \"Total\" NUMERIC(10, 2) NOT NULL, PRIMARY KEY (\"InvoiceId\"), FOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\") ) /* 3 rows from Invoice table: InvoiceId   CustomerId  InvoiceDate BillingAddress  BillingCity BillingState    BillingCountry  BillingPostalCode   Total 1   2   2009-01-01 00:00:00 Theodor-Heuss-Straße 34 Stuttgart   None    Germany 70174   1.98 2   4   2009-01-02 00:00:00 Ullevålsveien 14    Oslo    None    Norway  0171    3.96 3   8   2009-01-03 00:00:00 Grétrystraat 63 Brussels    None    Belgium 1000    5.94 */ Thought: I should query the total sales per country. Action: sql_db_query Action Input: SELECT Country, SUM(Total) AS TotalSales FROM Invoice INNER JOIN Customer ON Invoice.CustomerId = Customer.CustomerId GROUP BY Country ORDER BY TotalSales DESC LIMIT 10 Observation: [('USA', 523.0600000000003), ('Canada', 303.9599999999999), ('France', 195.09999999999994), ('Brazil', 190.09999999999997), ('Germany', 156.48), ('United Kingdom', 112.85999999999999), ('Czech Republic', 90.24000000000001), ('Portugal', 77.23999999999998), ('India', 75.25999999999999), ('Chile', 46.62)] Thought: I now know the final answer Final Answer: The country with the highest total sales is the USA, with a total of $523.06. > Finished chain. 'The country with the highest total sales is the USA, with a total of $523.06.' Looking at the LangSmith\ntrace ,\nwe can see: The agent is using a ReAct style prompt First, it will look at the tables: Action: sql_db_list_tables using tool sql_db_list_tables Given the tables as an observation, it thinks and then\ndeterminates the next action : Observation: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Thought: I should query the schema of the Invoice and Customer tables. Action: sql_db_schema Action Input: Invoice, Customer It then formulates the query using the schema from tool sql_db_schema Thought: I should query the total sales per country. Action: sql_db_query Action Input: SELECT Country, SUM(Total) AS TotalSales FROM Invoice INNER JOIN Customer ON Invoice.CustomerId = Customer.CustomerId GROUP BY Country ORDER BY TotalSales DESC LIMIT 10 It finally executes the generated query using tool sql_db_query Agent task example #2 - Describing a Table ​ agent_executor . run ( \"Describe the playlisttrack table\" ) > Entering new AgentExecutor chain... Action: sql_db_list_tables Action Input: Observation: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Thought: The PlaylistTrack table is the most relevant to the question. Action: sql_db_schema Action Input: PlaylistTrack Observation: CREATE TABLE \"PlaylistTrack\" ( \"PlaylistId\" INTEGER NOT NULL, \"TrackId\" INTEGER NOT NULL, PRIMARY KEY (\"PlaylistId\", \"TrackId\"), FOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), FOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\") ) /* 3 rows from PlaylistTrack table: PlaylistId  TrackId 1   3402 1   3389 1   3390 */ Thought: I now know the final answer Final Answer: The PlaylistTrack table contains two columns, PlaylistId and TrackId, which are both integers and form a primary key. It also has two foreign keys, one to the Track table and one to the Playlist table. > Finished chain. 'The PlaylistTrack table contains two columns, PlaylistId and TrackId, which are both integers and form a primary key. It also has two foreign keys, one to the Track table and one to the Playlist table.' Extending the SQL Toolkit ​ Although the out-of-the-box SQL Toolkit contains the necessary tools to\nstart working on a database, it is often the case that some extra tools\nmay be useful for extending the agent’s capabilities. This is\nparticularly useful when trying to use domain specific knowledge in\nthe solution, in order to improve its overall performance. Some examples include: Including dynamic few shot examples Finding misspellings in proper nouns to use as column filters We can create separate tools which tackle these specific use cases and\ninclude them as a complement to the standard SQL Toolkit. Let’s see how\nto include these two custom tools. Including dynamic few-shot examples ​ In order to include dynamic few-shot examples, we need a custom Retriever Tool that handles the vector database in order to retrieve\nthe examples that are semantically similar to the user’s question. Let’s start by creating a dictionary with some examples: few_shots = { \"List all artists.\" : \"SELECT * FROM artists;\" , \"Find all albums for the artist 'AC/DC'.\" : \"SELECT * FROM albums WHERE ArtistId = (SELECT ArtistId FROM artists WHERE Name = 'AC/DC');\" , \"List all tracks in the 'Rock' genre.\" : \"SELECT * FROM tracks WHERE GenreId = (SELECT GenreId FROM genres WHERE Name = 'Rock');\" , \"Find the total duration of all tracks.\" : \"SELECT SUM(Milliseconds) FROM tracks;\" , \"List all customers from Canada.\" : \"SELECT * FROM customers WHERE Country = 'Canada';\" , \"How many tracks are there in the album with ID 5?\" : \"SELECT COUNT(*) FROM tracks WHERE AlbumId = 5;\" , \"Find the total number of invoices.\" : \"SELECT COUNT(*) FROM invoices;\" , \"List all tracks that are longer than 5 minutes.\" : \"SELECT * FROM tracks WHERE Milliseconds > 300000;\" , \"Who are the top 5 customers by total purchase?\" : \"SELECT CustomerId, SUM(Total) AS TotalPurchase FROM invoices GROUP BY CustomerId ORDER BY TotalPurchase DESC LIMIT 5;\" , \"Which albums are from the year 2000?\" : \"SELECT * FROM albums WHERE strftime('%Y', ReleaseDate) = '2000';\" , \"How many employees are there\" : 'SELECT COUNT(*) FROM \"employee\"' , } We can then create a retriever using the list of questions, assigning\nthe target SQL query as metadata: from langchain . embeddings . openai import OpenAIEmbeddings from langchain . schema import Document from langchain . vectorstores import FAISS embeddings = OpenAIEmbeddings ( ) few_shot_docs = [ Document ( page_content = question , metadata = { \"sql_query\" : few_shots [ question ] } ) for question in few_shots . keys ( ) ] vector_db = FAISS . from_documents ( few_shot_docs , embeddings ) retriever = vector_db . as_retriever ( ) Now we can create our own custom tool and append it as a new tool in the create_sql_agent function: from langchain . agents . agent_toolkits import create_retriever_tool tool_description = \"\"\" This tool will help you understand similar examples to adapt them to the user question. Input to this tool should be the user question. \"\"\" retriever_tool = create_retriever_tool ( retriever , name = \"sql_get_similar_examples\" , description = tool_description ) custom_tool_list = [ retriever_tool ] Now we can create the agent, adjusting the standard SQL Agent suffix to\nconsider our use case. Although the most straightforward way to handle\nthis would be to include it just in the tool description, this is often\nnot enough and we need to specify it in the agent prompt using the suffix argument in the constructor. from langchain . agents import AgentType , create_sql_agent from langchain . agents . agent_toolkits import SQLDatabaseToolkit from langchain . chat_models import ChatOpenAI from langchain . utilities import SQLDatabase db = SQLDatabase . from_uri ( \"sqlite:///Chinook.db\" ) llm = ChatOpenAI ( model_name = \"gpt-4\" , temperature = 0 ) toolkit = SQLDatabaseToolkit ( db = db , llm = llm ) custom_suffix = \"\"\" I should first get the similar examples I know. If the examples are enough to construct the query, I can build it. Otherwise, I can then look at the tables in the database to see what I can query. Then I should query the schema of the most relevant tables \"\"\" agent = create_sql_agent ( llm = llm , toolkit = toolkit , verbose = True , agent_type = AgentType . OPENAI_FUNCTIONS , extra_tools = custom_tool_list , suffix = custom_suffix , ) Let’s try it out: agent . run ( \"How many employees do we have?\" ) > Entering new AgentExecutor chain... Invoking: `sql_get_similar_examples` with `How many employees do we have?` [Document(page_content='How many employees are there', metadata={'sql_query': 'SELECT COUNT(*) FROM \"employee\"'}), Document(page_content='Find the total number of invoices.', metadata={'sql_query': 'SELECT COUNT(*) FROM invoices;'})] Invoking: `sql_db_query_checker` with `SELECT COUNT(*) FROM employee` responded: {content} SELECT COUNT(*) FROM employee Invoking: `sql_db_query` with `SELECT COUNT(*) FROM employee` [(8,)]We have 8 employees. > Finished chain. 'We have 8 employees.' As we can see, the agent first used the sql_get_similar_examples tool\nin order to retrieve similar examples. As the question was very similar\nto other few shot examples, the agent didn’t need to use any other\ntool from the standard Toolkit, thus saving time and tokens . Finding and correcting misspellings for proper nouns ​ In order to filter columns that contain proper nouns such as addresses,\nsong names or artists, we first need to double-check the spelling in\norder to filter the data correctly. We can achieve this by creating a vector store using all the distinct\nproper nouns that exist in the database. We can then have the agent\nquery that vector store each time the user includes a proper noun in\ntheir question, to find the correct spelling for that word. In this way,\nthe agent can make sure it understands which entity the user is\nreferring to before building the target query. Let’s follow a similar approach to the few shots, but without metadata:\njust embedding the proper nouns and then querying to get the most\nsimilar one to the misspelled user question. First we need the unique values for each entity we want, for which we\ndefine a function that parses the result into a list of elements: import ast import re def run_query_save_results ( db , query ) : res = db . run ( query ) res = [ el for sub in ast . literal_eval ( res ) for el in sub if el ] res = [ re . sub ( r\"\\b\\d+\\b\" , \"\" , string ) . strip ( ) for string in res ] return res artists = run_query_save_results ( db , \"SELECT Name FROM Artist\" ) albums = run_query_save_results ( db , \"SELECT Title FROM Album\" ) Now we can proceed with creating the custom retriever tool and the\nfinal agent: from langchain . agents . agent_toolkits import create_retriever_tool from langchain . embeddings . openai import OpenAIEmbeddings from langchain . vectorstores import FAISS texts = artists + albums embeddings = OpenAIEmbeddings ( ) vector_db = FAISS . from_texts ( texts , embeddings ) retriever = vector_db . as_retriever ( ) retriever_tool = create_retriever_tool ( retriever , name = \"name_search\" , description = \"use to learn how a piece of data is actually written, can be from names, surnames addresses etc\" , ) custom_tool_list = [ retriever_tool ] from langchain . agents import AgentType , create_sql_agent from langchain . agents . agent_toolkits import SQLDatabaseToolkit from langchain . chat_models import ChatOpenAI from langchain . utilities import SQLDatabase # db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\") llm = ChatOpenAI ( model_name = \"gpt-4\" , temperature = 0 ) toolkit = SQLDatabaseToolkit ( db = db , llm = llm ) custom_suffix = \"\"\" If a user asks for me to filter based on proper nouns, I should first check the spelling using the name_search tool. Otherwise, I can then look at the tables in the database to see what I can query. Then I should query the schema of the most relevant tables \"\"\" agent = create_sql_agent ( llm = llm , toolkit = toolkit , verbose = True , agent_type = AgentType . OPENAI_FUNCTIONS , extra_tools = custom_tool_list , suffix = custom_suffix , ) Let’s try it out: agent . run ( \"How many albums does alis in pains have?\" ) > Entering new AgentExecutor chain... Invoking: `name_search` with `alis in pains` [Document(page_content='House of Pain', metadata={}), Document(page_content='Alice In Chains', metadata={}), Document(page_content='Aisha Duo', metadata={}), Document(page_content='House Of Pain', metadata={})] Invoking: `sql_db_list_tables` with `` responded: {content} Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Invoking: `sql_db_schema` with `Album, Artist` responded: {content} CREATE TABLE \"Album\" ( \"AlbumId\" INTEGER NOT NULL, \"Title\" NVARCHAR(160) NOT NULL, \"ArtistId\" INTEGER NOT NULL, PRIMARY KEY (\"AlbumId\"), FOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\") ) /* 3 rows from Album table: AlbumId Title   ArtistId 1   For Those About To Rock We Salute You   1 2   Balls to the Wall   2 3   Restless and Wild   2 */ CREATE TABLE \"Artist\" ( \"ArtistId\" INTEGER NOT NULL, \"Name\" NVARCHAR(120), PRIMARY KEY (\"ArtistId\") ) /* 3 rows from Artist table: ArtistId    Name 1   AC/DC 2   Accept 3   Aerosmith */ Invoking: `sql_db_query_checker` with `SELECT COUNT(*) FROM Album JOIN Artist ON Album.ArtistId = Artist.ArtistId WHERE Artist.Name = 'Alice In Chains'` responded: {content} SELECT COUNT(*) FROM Album JOIN Artist ON Album.ArtistId = Artist.ArtistId WHERE Artist.Name = 'Alice In Chains' Invoking: `sql_db_query` with `SELECT COUNT(*) FROM Album JOIN Artist ON Album.ArtistId = Artist.ArtistId WHERE Artist.Name = 'Alice In Chains'` [(1,)]Alice In Chains has 1 album in the database. > Finished chain. 'Alice In Chains has 1 album in the database.' As we can see, the agent used the name_search tool in order to check\nhow to correctly query the database for this specific artist. Go deeper ​ To learn more about the SQL Agent and how it works we refer to the SQL\nAgent Toolkit documentation. You can also check Agents for other document types: - Pandas\nAgent - CSV\nAgent Elastic Search ​ Going beyond the above use-case, there are integrations with other\ndatabases. For example, we can interact with Elasticsearch analytics database. This chain builds search queries via the Elasticsearch DSL API (filters\nand aggregations). The Elasticsearch client must have permissions for index listing,\nmapping description and search queries. See here for instructions on how to run Elasticsearch locally. Make sure to install the Elasticsearch Python client before: pip install elasticsearch from elasticsearch import Elasticsearch from langchain . chains . elasticsearch_database import ElasticsearchDatabaseChain from langchain . chat_models import ChatOpenAI # Initialize Elasticsearch python client. # See https://elasticsearch-py.readthedocs.io/en/v8.8.2/api.html#elasticsearch.Elasticsearch ELASTIC_SEARCH_SERVER = \"https://elastic:pass@localhost:9200\" db = Elasticsearch ( ELASTIC_SEARCH_SERVER ) Uncomment the next cell to initially populate your db. # customers = [ #     {\"firstname\": \"Jennifer\", \"lastname\": \"Walters\"}, #     {\"firstname\": \"Monica\",\"lastname\":\"Rambeau\"}, #     {\"firstname\": \"Carol\",\"lastname\":\"Danvers\"}, #     {\"firstname\": \"Wanda\",\"lastname\":\"Maximoff\"}, #     {\"firstname\": \"Jennifer\",\"lastname\":\"Takeda\"}, # ] # for i, customer in enumerate(customers): #     db.create(index=\"customers\", document=customer, id=i) llm = ChatOpenAI ( model_name = \"gpt-4\" , temperature = 0 ) chain = ElasticsearchDatabaseChain . from_llm ( llm = llm , database = db , verbose = True ) question = \"What are the first names of all the customers?\" chain . run ( question ) We can customize the prompt. from langchain . prompts . prompt import PromptTemplate PROMPT_TEMPLATE = \"\"\"Given an input question, create a syntactically correct Elasticsearch query to run. Unless the user specifies in their question a specific number of examples they wish to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database. Unless told to do not query for all the columns from a specific index, only ask for a the few relevant columns given the question. Pay attention to use only the column names that you can see in the mapping description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which index. Return the query as valid json. Use the following format: Question: Question here ESQuery: Elasticsearch Query formatted as json \"\"\" PROMPT = PromptTemplate . from_template ( PROMPT_TEMPLATE , ) chain = ElasticsearchDatabaseChain . from_llm ( llm = llm , database = db , query_prompt = PROMPT ) Next Retrieval-augmented generation (RAG)"
    },
    {
        "title": "Providers | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/providers/",
        "text_content": ""
    },
    {
        "title": "Interacting with APIs | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/apis",
        "text_content": "Interacting with APIs On this page Interacting with APIs Open In Colab Use case ​ Suppose you want an LLM to interact with external APIs. This can be very useful for retrieving context for the LLM to utilize. And, more generally, it allows us to interact with APIs using natural\nlanguage! Overview ​ There are two primary ways to interface LLMs with external APIs: Functions : For example, OpenAI\nfunctions is one popular means of doing this. LLM-generated interface : Use an LLM with access to API\ndocumentation to create an interface. Quickstart ​ Many APIs are already compatible with OpenAI function calling. For example, Klarna has a YAML file that describes its API and allows OpenAI to interact\nwith it: https://www.klarna.com/us/shopping/public/openai/v0/api-docs/ Other options include: Speak for translation XKCD for comics We can supply the specification to get_openapi_chain directly in order\nto query the API with OpenAI functions: pip install langchain openai # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv() from langchain . chains . openai_functions . openapi import get_openapi_chain chain = get_openapi_chain ( \"https://www.klarna.com/us/shopping/public/openai/v0/api-docs/\" ) chain ( \"What are some options for a men's large blue button down shirt\" ) Attempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support. {'query': \"What are some options for a men's large blue button down shirt\", 'response': {'products': [{'name': 'Cubavera Four Pocket Guayabera Shirt', 'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3202055522/Clothing/Cubavera-Four-Pocket-Guayabera-Shirt/?utm_source=openai&ref-site=openai_plugin', 'price': '$13.50', 'attributes': ['Material:Polyester,Cotton', 'Target Group:Man', 'Color:Red,White,Blue,Black', 'Properties:Pockets', 'Pattern:Solid Color', 'Size (Small-Large):S,XL,L,M,XXL']}, {'name': 'Polo Ralph Lauren Plaid Short Sleeve Button-down Oxford Shirt', 'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3207163438/Clothing/Polo-Ralph-Lauren-Plaid-Short-Sleeve-Button-down-Oxford-Shirt/?utm_source=openai&ref-site=openai_plugin', 'price': '$52.20', 'attributes': ['Material:Cotton', 'Target Group:Man', 'Color:Red,Blue,Multicolor', 'Size (Small-Large):S,XL,L,M,XXL']}, {'name': 'Brixton Bowery Flannel Shirt', 'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3202331096/Clothing/Brixton-Bowery-Flannel-Shirt/?utm_source=openai&ref-site=openai_plugin', 'price': '$27.48', 'attributes': ['Material:Cotton', 'Target Group:Man', 'Color:Gray,Blue,Black,Orange', 'Properties:Pockets', 'Pattern:Checkered', 'Size (Small-Large):XL,3XL,4XL,5XL,L,M,XXL']}, {'name': 'Vineyard Vines Gingham On-The-Go brrr Classic Fit Shirt Crystal', 'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3201938510/Clothing/Vineyard-Vines-Gingham-On-The-Go-brrr-Classic-Fit-Shirt-Crystal/?utm_source=openai&ref-site=openai_plugin', 'price': '$80.64', 'attributes': ['Material:Cotton', 'Target Group:Man', 'Color:Blue', 'Size (Small-Large):XL,XS,L,M']}, {'name': \"Carhartt Men's Loose Fit Midweight Short Sleeve Plaid Shirt\", 'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3201826024/Clothing/Carhartt-Men-s-Loose-Fit-Midweight-Short-Sleeve-Plaid-Shirt/?utm_source=openai&ref-site=openai_plugin', 'price': '$17.99', 'attributes': ['Material:Cotton', 'Target Group:Man', 'Color:Red,Brown,Blue,Green', 'Properties:Pockets', 'Pattern:Checkered', 'Size (Small-Large):S,XL,L,M']}]}} Functions ​ We can unpack what is happening when we use the functions to call\nexternal APIs. Let’s look at the LangSmith\ntrace : See here that we call the OpenAI LLM with the provided API spec: https://www.klarna.com/us/shopping/public/openai/v0/api-docs/ The prompt then tells the LLM to use the API spec with input\nquestion: Use the provided APIs to respond to this user query: What are some options for a men's large blue button down shirt The LLM returns the parameters for the function call productsUsingGET , which is specified in the provided API\nspec : function_call: name: productsUsingGET arguments: |- { \"params\": { \"countryCode\": \"US\", \"q\": \"men's large blue button down shirt\", \"size\": 5, \"min_price\": 0, \"max_price\": 100 } } This Dict above split and the API is called\nhere . API Chain ​ We can also build our own interface to external APIs using the APIChain and provided API documentation. from langchain . chains import APIChain from langchain . chains . api import open_meteo_docs from langchain . llms import OpenAI llm = OpenAI ( temperature = 0 ) chain = APIChain . from_llm_and_api_docs ( llm , open_meteo_docs . OPEN_METEO_DOCS , verbose = True , limit_to_domains = [ \"https://api.open-meteo.com/\" ] , ) chain . run ( \"What is the weather like right now in Munich, Germany in degrees Fahrenheit?\" ) > Entering new APIChain chain... https://api.open-meteo.com/v1/forecast?latitude=48.1351&longitude=11.5820&hourly=temperature_2m&temperature_unit=fahrenheit&current_weather=true {\"latitude\":48.14,\"longitude\":11.58,\"generationtime_ms\":0.1710653305053711,\"utc_offset_seconds\":0,\"timezone\":\"GMT\",\"timezone_abbreviation\":\"GMT\",\"elevation\":521.0,\"current_weather_units\":{\"time\":\"iso8601\",\"interval\":\"seconds\",\"temperature\":\"°F\",\"windspeed\":\"km/h\",\"winddirection\":\"°\",\"is_day\":\"\",\"weathercode\":\"wmo code\"},\"current_weather\":{\"time\":\"2023-11-01T21:30\",\"interval\":900,\"temperature\":46.5,\"windspeed\":7.7,\"winddirection\":259,\"is_day\":0,\"weathercode\":3},\"hourly_units\":{\"time\":\"iso8601\",\"temperature_2m\":\"°F\"},\"hourly\":{\"time\":[\"2023-11-01T00:00\",\"2023-11-01T01:00\",\"2023-11-01T02:00\",\"2023-11-01T03:00\",\"2023-11-01T04:00\",\"2023-11-01T05:00\",\"2023-11-01T06:00\",\"2023-11-01T07:00\",\"2023-11-01T08:00\",\"2023-11-01T09:00\",\"2023-11-01T10:00\",\"2023-11-01T11:00\",\"2023-11-01T12:00\",\"2023-11-01T13:00\",\"2023-11-01T14:00\",\"2023-11-01T15:00\",\"2023-11-01T16:00\",\"2023-11-01T17:00\",\"2023-11-01T18:00\",\"2023-11-01T19:00\",\"2023-11-01T20:00\",\"2023-11-01T21:00\",\"2023-11-01T22:00\",\"2023-11-01T23:00\",\"2023-11-02T00:00\",\"2023-11-02T01:00\",\"2023-11-02T02:00\",\"2023-11-02T03:00\",\"2023-11-02T04:00\",\"2023-11-02T05:00\",\"2023-11-02T06:00\",\"2023-11-02T07:00\",\"2023-11-02T08:00\",\"2023-11-02T09:00\",\"2023-11-02T10:00\",\"2023-11-02T11:00\",\"2023-11-02T12:00\",\"2023-11-02T13:00\",\"2023-11-02T14:00\",\"2023-11-02T15:00\",\"2023-11-02T16:00\",\"2023-11-02T17:00\",\"2023-11-02T18:00\",\"2023-11-02T19:00\",\"2023-11-02T20:00\",\"2023-11-02T21:00\",\"2023-11-02T22:00\",\"2023-11-02T23:00\",\"2023-11-03T00:00\",\"2023-11-03T01:00\",\"2023-11-03T02:00\",\"2023-11-03T03:00\",\"2023-11-03T04:00\",\"2023-11-03T05:00\",\"2023-11-03T06:00\",\"2023-11-03T07:00\",\"2023-11-03T08:00\",\"2023-11-03T09:00\",\"2023-11-03T10:00\",\"2023-11-03T11:00\",\"2023-11-03T12:00\",\"2023-11-03T13:00\",\"2023-11-03T14:00\",\"2023-11-03T15:00\",\"2023-11-03T16:00\",\"2023-11-03T17:00\",\"2023-11-03T18:00\",\"2023-11-03T19:00\",\"2023-11-03T20:00\",\"2023-11-03T21:00\",\"2023-11-03T22:00\",\"2023-11-03T23:00\",\"2023-11-04T00:00\",\"2023-11-04T01:00\",\"2023-11-04T02:00\",\"2023-11-04T03:00\",\"2023-11-04T04:00\",\"2023-11-04T05:00\",\"2023-11-04T06:00\",\"2023-11-04T07:00\",\"2023-11-04T08:00\",\"2023-11-04T09:00\",\"2023-11-04T10:00\",\"2023-11-04T11:00\",\"2023-11-04T12:00\",\"2023-11-04T13:00\",\"2023-11-04T14:00\",\"2023-11-04T15:00\",\"2023-11-04T16:00\",\"2023-11-04T17:00\",\"2023-11-04T18:00\",\"2023-11-04T19:00\",\"2023-11-04T20:00\",\"2023-11-04T21:00\",\"2023-11-04T22:00\",\"2023-11-04T23:00\",\"2023-11-05T00:00\",\"2023-11-05T01:00\",\"2023-11-05T02:00\",\"2023-11-05T03:00\",\"2023-11-05T04:00\",\"2023-11-05T05:00\",\"2023-11-05T06:00\",\"2023-11-05T07:00\",\"2023-11-05T08:00\",\"2023-11-05T09:00\",\"2023-11-05T10:00\",\"2023-11-05T11:00\",\"2023-11-05T12:00\",\"2023-11-05T13:00\",\"2023-11-05T14:00\",\"2023-11-05T15:00\",\"2023-11-05T16:00\",\"2023-11-05T17:00\",\"2023-11-05T18:00\",\"2023-11-05T19:00\",\"2023-11-05T20:00\",\"2023-11-05T21:00\",\"2023-11-05T22:00\",\"2023-11-05T23:00\",\"2023-11-06T00:00\",\"2023-11-06T01:00\",\"2023-11-06T02:00\",\"2023-11-06T03:00\",\"2023-11-06T04:00\",\"2023-11-06T05:00\",\"2023-11-06T06:00\",\"2023-11-06T07:00\",\"2023-11-06T08:00\",\"2023-11-06T09:00\",\"2023-11-06T10:00\",\"2023-11-06T11:00\",\"2023-11-06T12:00\",\"2023-11-06T13:00\",\"2023-11-06T14:00\",\"2023-11-06T15:00\",\"2023-11-06T16:00\",\"2023-11-06T17:00\",\"2023-11-06T18:00\",\"2023-11-06T19:00\",\"2023-11-06T20:00\",\"2023-11-06T21:00\",\"2023-11-06T22:00\",\"2023-11-06T23:00\",\"2023-11-07T00:00\",\"2023-11-07T01:00\",\"2023-11-07T02:00\",\"2023-11-07T03:00\",\"2023-11-07T04:00\",\"2023-11-07T05:00\",\"2023-11-07T06:00\",\"2023-11-07T07:00\",\"2023-11-07T08:00\",\"2023-11-07T09:00\",\"2023-11-07T10:00\",\"2023-11-07T11:00\",\"2023-11-07T12:00\",\"2023-11-07T13:00\",\"2023-11-07T14:00\",\"2023-11-07T15:00\",\"2023-11-07T16:00\",\"2023-11-07T17:00\",\"2023-11-07T18:00\",\"2023-11-07T19:00\",\"2023-11-07T20:00\",\"2023-11-07T21:00\",\"2023-11-07T22:00\",\"2023-11-07T23:00\"],\"temperature_2m\":[47.9,46.9,47.1,46.6,45.8,45.2,43.4,43.5,46.8,51.5,55.0,56.3,58.1,57.9,57.0,56.6,54.4,52.1,49.1,48.3,47.7,46.9,46.2,45.8,44.4,42.4,41.7,41.7,42.0,42.7,43.6,44.3,45.9,48.0,49.1,50.7,52.2,52.6,51.9,50.3,48.1,47.4,47.1,46.9,46.2,45.7,45.6,45.6,45.7,45.3,45.1,44.2,43.6,43.2,42.8,41.6,41.0,42.1,42.4,42.3,42.7,43.9,44.2,43.6,41.9,40.4,39.0,40.8,40.2,40.1,39.6,38.8,38.2,36.9,35.8,36.4,37.3,38.5,38.9,39.0,41.8,45.4,48.7,50.8,51.7,52.1,51.3,49.8,48.6,47.8,47.0,46.3,45.9,45.6,45.7,46.1,46.3,46.4,46.3,46.3,45.8,45.4,45.5,47.1,49.3,51.2,52.4,53.1,53.5,53.4,53.0,52.4,51.6,50.5,49.6,49.0,48.6,48.1,47.6,47.0,46.4,46.0,45.5,45.1,44.4,43.7,43.9,45.6,48.1,50.3,51.7,52.8,53.5,52.7,51.5,50.2,48.8,47.4,46.2,45.5,45.0,44.6,44.3,44.2,43.9,43.4,43.0,42.6,42.3,42.0,42.2,43.0,44.3,45.5,46.8,48.1,48.9,49.0,48.7,48.1,47.4,46.5,45.7,45.1,44.5,44.3,44.5,45.1]}} > Finished chain. ' The current temperature in Munich, Germany is 46.5°F.' Note that we supply information about the API: open_meteo_docs . OPEN_METEO_DOCS [ 0 : 500 ] 'BASE URL: https://api.open-meteo.com/\\n\\nAPI Documentation\\nThe API endpoint /v1/forecast accepts a geographical coordinate, a list of weather variables and responds with a JSON hourly weather forecast for 7 days. Time always starts at 0:00 today and contains 168 hours. All URL parameters are listed below:\\n\\nParameter\\tFormat\\tRequired\\tDefault\\tDescription\\nlatitude, longitude\\tFloating point\\tYes\\t\\tGeographical WGS84 coordinate of the location\\nhourly\\tString array\\tNo\\t\\tA list of weather variables which shou' Under the hood, we do two things: api_request_chain : Generate an API URL based on the input question\nand the api_docs api_answer_chain : generate a final answer based on the API\nresponse We can look at the LangSmith\ntrace to inspect this: The api_request_chain produces the API url from our question and\nthe API documentation: Here we make the API request with the API url. The api_answer_chain takes the response from the API and provides\nus with a natural language response: Going deeper ​ Test with other APIs import os os . environ [ \"TMDB_BEARER_TOKEN\" ] = \"\" from langchain . chains . api import tmdb_docs headers = { \"Authorization\" : f\"Bearer { os . environ [ 'TMDB_BEARER_TOKEN' ] } \" } chain = APIChain . from_llm_and_api_docs ( llm , tmdb_docs . TMDB_DOCS , headers = headers , verbose = True , limit_to_domains = [ \"https://api.themoviedb.org/\" ] , ) chain . run ( \"Search for 'Avatar'\" ) import os from langchain . chains import APIChain from langchain . chains . api import podcast_docs from langchain . llms import OpenAI listen_api_key = \"xxx\" # Get api key here: https://www.listennotes.com/api/pricing/ llm = OpenAI ( temperature = 0 ) headers = { \"X-ListenAPI-Key\" : listen_api_key } chain = APIChain . from_llm_and_api_docs ( llm , podcast_docs . PODCAST_DOCS , headers = headers , verbose = True , limit_to_domains = [ \"https://listen-api.listennotes.com/\" ] , ) chain . run ( \"Search for 'silicon valley bank' podcast episodes, audio length is more than 30 minutes, return only 1 results\" ) Web requests URL requests are such a common use-case that we have the LLMRequestsChain , which makes an HTTP GET request. from langchain . chains import LLMChain , LLMRequestsChain from langchain . llms import OpenAI from langchain . prompts import PromptTemplate template = \"\"\"Between >>> and <<< are the raw search result text from google. Extract the answer to the question '{query}' or say \"not found\" if the information is not contained. Use the format Extracted:<answer or \"not found\"> >>> {requests_result} <<< Extracted:\"\"\" PROMPT = PromptTemplate ( input_variables = [ \"query\" , \"requests_result\" ] , template = template , ) chain = LLMRequestsChain ( llm_chain = LLMChain ( llm = OpenAI ( temperature = 0 ) , prompt = PROMPT ) ) question = \"What are the Three (3) biggest countries, and their respective sizes?\" inputs = { \"query\" : question , \"url\" : \"https://www.google.com/search?q=\" + question . replace ( \" \" , \"+\" ) , } chain ( inputs ) {'query': 'What are the Three (3) biggest countries, and their respective sizes?', 'url': 'https://www.google.com/search?q=What+are+the+Three+(3)+biggest+countries,+and+their+respective+sizes?', 'output': ' Russia (17,098,242 km²), Canada (9,984,670 km²), China (9,706,961 km²)'} Previous RAG using local models Next Chatbots"
    },
    {
        "title": "Chatbots | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/chatbots",
        "text_content": "Chatbots On this page Chatbots Open In Colab Use case ​ Chatbots are one of the central LLM use-cases. The core features of\nchatbots are that they can have long-running conversations and have\naccess to information that users want to know about. Aside from basic prompting and LLMs, memory and retrieval are the core\ncomponents of a chatbot. Memory allows a chatbot to remember past\ninteractions, and retrieval provides a chatbot with up-to-date,\ndomain-specific information. Overview ​ The chat model interface is based around messages rather than raw text.\nSeveral components are important to consider for chat: chat model : See here for a list of chat\nmodel integrations and here for\ndocumentation on the chat model interface in LangChain. You can use LLMs (see here ) for chatbots as\nwell, but chat models have a more conversational tone and natively\nsupport a message interface. prompt template : Prompt templates make it easy to assemble prompts\nthat combine default messages, user input, chat history, and\n(optionally) additional retrieved context. memory : See here for in-depth\ndocumentation on memory types retriever (optional): See\nhere for in-depth\ndocumentation on retrieval systems. These are useful if you want to\nbuild a chatbot with domain-specific knowledge. Quickstart ​ Here’s a quick preview of how we can create chatbot interfaces. First\nlet’s install some dependencies and set the required credentials: !pip install langchain openai # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv() With a plain chat model, we can get chat completions by passing one or\nmore messages to the model. The chat model will respond with a message. from langchain . chat_models import ChatOpenAI from langchain . schema import HumanMessage , SystemMessage chat = ChatOpenAI ( ) chat ( [ HumanMessage ( content = \"Translate this sentence from English to French: I love programming.\" ) ] ) AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False) And if we pass in a list of messages: messages = [ SystemMessage ( content = \"You are a helpful assistant that translates English to French.\" ) , HumanMessage ( content = \"I love programming.\" ) , ] chat ( messages ) AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False) We can then wrap our chat model in a ConversationChain , which has\nbuilt-in memory for remembering past user inputs and model outputs. from langchain . chains import ConversationChain conversation = ConversationChain ( llm = chat ) conversation . run ( \"Translate this sentence from English to French: I love programming.\" ) 'Je adore la programmation.' conversation . run ( \"Translate it to German.\" ) 'Ich liebe Programmieren.' Memory ​ As we mentioned above, the core component of chatbots is the memory\nsystem. One of the simplest and most commonly used forms of memory is ConversationBufferMemory : This memory allows for storing of messages in a buffer When called in a chain, it returns all of the messages it has stored LangChain comes with many other types of memory, too. See\nhere for in-depth documentation on memory types. For now let’s take a quick look at ConversationBufferMemory. We can\nmanually add a few chat messages to the memory like so: from langchain . memory import ConversationBufferMemory memory = ConversationBufferMemory ( ) memory . chat_memory . add_user_message ( \"hi!\" ) memory . chat_memory . add_ai_message ( \"whats up?\" ) And now we can load from our memory. The key method exposed by all Memory classes is load_memory_variables . This takes in any initial\nchain input and returns a list of memory variables which are added to\nthe chain input. Since this simple memory type doesn’t actually take into account the\nchain input when loading memory, we can pass in an empty input for now: memory . load_memory_variables ( { } ) {'history': 'Human: hi!\\nAI: whats up?'} We can also keep a sliding window of the most recent k interactions\nusing ConversationBufferWindowMemory . from langchain . memory import ConversationBufferWindowMemory memory = ConversationBufferWindowMemory ( k = 1 ) memory . save_context ( { \"input\" : \"hi\" } , { \"output\" : \"whats up\" } ) memory . save_context ( { \"input\" : \"not much you\" } , { \"output\" : \"not much\" } ) memory . load_memory_variables ( { } ) {'history': 'Human: not much you\\nAI: not much'} ConversationSummaryMemory is an extension of this theme. It creates a summary of the conversation over time. This memory is most useful for longer conversations where the full\nmessage history would consume many tokens. from langchain . llms import OpenAI from langchain . memory import ConversationSummaryMemory llm = OpenAI ( temperature = 0 ) memory = ConversationSummaryMemory ( llm = llm ) memory . save_context ( { \"input\" : \"hi\" } , { \"output\" : \"whats up\" } ) memory . save_context ( { \"input\" : \"im working on better docs for chatbots\" } , { \"output\" : \"oh, that sounds like a lot of work\" } , ) memory . save_context ( { \"input\" : \"yes, but it's worth the effort\" } , { \"output\" : \"agreed, good docs are important!\" } , ) memory . load_memory_variables ( { } ) {'history': '\\nThe human greets the AI, to which the AI responds. The human then mentions they are working on better docs for chatbots, to which the AI responds that it sounds like a lot of work. The human agrees that it is worth the effort, and the AI agrees that good docs are important.'} ConversationSummaryBufferMemory extends this a bit further: It uses token length rather than number of interactions to determine\nwhen to flush interactions. from langchain . memory import ConversationSummaryBufferMemory memory = ConversationSummaryBufferMemory ( llm = llm , max_token_limit = 10 ) memory . save_context ( { \"input\" : \"hi\" } , { \"output\" : \"whats up\" } ) memory . save_context ( { \"input\" : \"not much you\" } , { \"output\" : \"not much\" } ) Conversation ​ We can unpack what goes under the hood with ConversationChain . We can specify our memory, ConversationSummaryMemory and we can\nspecify the prompt. from langchain . chains import LLMChain from langchain . prompts import ( ChatPromptTemplate , HumanMessagePromptTemplate , MessagesPlaceholder , SystemMessagePromptTemplate , ) # LLM llm = ChatOpenAI ( ) # Prompt prompt = ChatPromptTemplate ( messages = [ SystemMessagePromptTemplate . from_template ( \"You are a nice chatbot having a conversation with a human.\" ) , # The `variable_name` here is what must align with memory MessagesPlaceholder ( variable_name = \"chat_history\" ) , HumanMessagePromptTemplate . from_template ( \"{question}\" ) , ] ) # Notice that we `return_messages=True` to fit into the MessagesPlaceholder # Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name memory = ConversationBufferMemory ( memory_key = \"chat_history\" , return_messages = True ) conversation = LLMChain ( llm = llm , prompt = prompt , verbose = True , memory = memory ) # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation ( { \"question\" : \"hi\" } ) > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi > Finished chain. {'question': 'hi', 'chat_history': [HumanMessage(content='hi', additional_kwargs={}, example=False), AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False)], 'text': 'Hello! How can I assist you today?'} conversation ( { \"question\" : \"Translate this sentence from English to French: I love programming.\" } ) > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. > Finished chain. {'question': 'Translate this sentence from English to French: I love programming.', 'chat_history': [HumanMessage(content='hi', additional_kwargs={}, example=False), AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False), HumanMessage(content='Translate this sentence from English to French: I love programming.', additional_kwargs={}, example=False), AIMessage(content='Sure! The translation of \"I love programming\" from English to French is \"J\\'adore programmer.\"', additional_kwargs={}, example=False)], 'text': 'Sure! The translation of \"I love programming\" from English to French is \"J\\'adore programmer.\"'} conversation ( { \"question\" : \"Now translate the sentence to German.\" } ) > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. AI: Sure! The translation of \"I love programming\" from English to French is \"J'adore programmer.\" Human: Now translate the sentence to German. > Finished chain. {'question': 'Now translate the sentence to German.', 'chat_history': [HumanMessage(content='hi', additional_kwargs={}, example=False), AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False), HumanMessage(content='Translate this sentence from English to French: I love programming.', additional_kwargs={}, example=False), AIMessage(content='Sure! The translation of \"I love programming\" from English to French is \"J\\'adore programmer.\"', additional_kwargs={}, example=False), HumanMessage(content='Now translate the sentence to German.', additional_kwargs={}, example=False), AIMessage(content='Certainly! The translation of \"I love programming\" from English to German is \"Ich liebe das Programmieren.\"', additional_kwargs={}, example=False)], 'text': 'Certainly! The translation of \"I love programming\" from English to German is \"Ich liebe das Programmieren.\"'} We can see the chat history preserved in the prompt using the LangSmith\ntrace . Chat Retrieval ​ Now, suppose we want to chat with\ndocuments or some other source of knowledge. This is popular use case, combining chat with document\nretrieval . It allows us to chat with specific information that the model was not\ntrained on. !pip install tiktoken chromadb Load a blog post. from langchain . document_loaders import WebBaseLoader loader = WebBaseLoader ( \"https://lilianweng.github.io/posts/2023-06-23-agent/\" ) data = loader . load ( ) Split and store this in a vector. from langchain . text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter ( chunk_size = 500 , chunk_overlap = 0 ) all_splits = text_splitter . split_documents ( data ) from langchain . embeddings import OpenAIEmbeddings from langchain . vectorstores import Chroma vectorstore = Chroma . from_documents ( documents = all_splits , embedding = OpenAIEmbeddings ( ) ) Create our memory, as before, but’s let’s use ConversationSummaryMemory . memory = ConversationSummaryMemory ( llm = llm , memory_key = \"chat_history\" , return_messages = True ) from langchain . chains import ConversationalRetrievalChain from langchain . chat_models import ChatOpenAI llm = ChatOpenAI ( ) retriever = vectorstore . as_retriever ( ) qa = ConversationalRetrievalChain . from_llm ( llm , retriever = retriever , memory = memory ) qa ( \"How do agents use Task decomposition?\" ) {'question': 'How do agents use Task decomposition?', 'chat_history': [SystemMessage(content='', additional_kwargs={})], 'answer': 'Agents can use task decomposition in several ways:\\n\\n1. Simple prompting: Agents can use Language Model based prompting to break down tasks into subgoals. For example, by providing prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\", the agent can generate a sequence of smaller steps that lead to the completion of the overall task.\\n\\n2. Task-specific instructions: Agents can be given task-specific instructions to guide their planning process. For example, if the task is to write a novel, the agent can be instructed to \"Write a story outline.\" This provides a high-level structure for the task and helps in breaking it down into smaller components.\\n\\n3. Human inputs: Agents can also take inputs from humans to decompose tasks. This can be done through direct communication or by leveraging human expertise. Humans can provide guidance and insights to help the agent break down complex tasks into manageable subgoals.\\n\\nOverall, task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.'} qa ( \"What are the various ways to implement memory to support it?\" ) {'question': 'What are the various ways to implement memory to support it?', 'chat_history': [SystemMessage(content='The human asks how agents use task decomposition. The AI explains that agents can use task decomposition in several ways, including simple prompting, task-specific instructions, and human inputs. Task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.', additional_kwargs={})], 'answer': 'There are several ways to implement memory to support task decomposition:\\n\\n1. Long-Term Memory Management: This involves storing and organizing information in a long-term memory system. The agent can retrieve past experiences, knowledge, and learned strategies to guide the task decomposition process.\\n\\n2. Internet Access: The agent can use internet access to search for relevant information and gather resources to aid in task decomposition. This allows the agent to access a vast amount of information and utilize it in the decomposition process.\\n\\n3. GPT-3.5 Powered Agents: The agent can delegate simple tasks to GPT-3.5 powered agents. These agents can perform specific tasks or provide assistance in task decomposition, allowing the main agent to focus on higher-level planning and decision-making.\\n\\n4. File Output: The agent can store the results of task decomposition in files or documents. This allows for easy retrieval and reference during the execution of the task.\\n\\nThese memory resources help the agent in organizing and managing information, making informed decisions, and effectively decomposing complex tasks into smaller, manageable subgoals.'} Again, we can use the LangSmith\ntrace to explore the prompt structure. Going deeper ​ Agents, such as the conversational retrieval\nagent ,\ncan be used for retrieval when necessary while also holding a\nconversation. Previous Interacting with APIs Next Extraction"
    },
    {
        "title": "Extraction | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/extraction",
        "text_content": "Extraction On this page Extraction Open In Collab Use case ​ Getting structured output from raw LLM generations is hard. For example, suppose you need the model output formatted with a specific\nschema for: Extracting a structured row to insert into a database Extracting API parameters Extracting different parts of a user query (e.g., for semantic vs\nkeyword search) Overview ​ There are two primary approaches for this: Functions : Some LLMs can call functions to extract arbitrary entities from LLM responses. Parsing : Output parsers are classes that structure LLM responses. Only some LLMs support functions (e.g., OpenAI), and they are more\ngeneral than parsers. Parsers extract precisely what is enumerated in a provided schema (e.g.,\nspecific attributes of a person). Functions can infer things beyond of a provided schema (e.g., attributes\nabout a person that you did not ask for). Quickstart ​ OpenAI functions are one way to get started with extraction. Define a schema that specifies the properties we want to extract from\nthe LLM output. Then, we can use create_extraction_chain to extract our desired schema\nusing an OpenAI function call. pip install langchain openai # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv() from langchain . chains import create_extraction_chain from langchain . chat_models import ChatOpenAI # Schema schema = { \"properties\" : { \"name\" : { \"type\" : \"string\" } , \"height\" : { \"type\" : \"integer\" } , \"hair_color\" : { \"type\" : \"string\" } , } , \"required\" : [ \"name\" , \"height\" ] , } # Input inp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\"\" # Run chain llm = ChatOpenAI ( temperature = 0 , model = \"gpt-3.5-turbo\" ) chain = create_extraction_chain ( schema , llm ) chain . run ( inp ) [{'name': 'Alex', 'height': 5, 'hair_color': 'blonde'}, {'name': 'Claudia', 'height': 6, 'hair_color': 'brunette'}] Option 1: OpenAI functions ​ Looking under the hood ​ Let’s dig into what is happening when we call create_extraction_chain . The LangSmith\ntrace shows that we call the function information_extraction on the input\nstring, inp . This information_extraction function is defined here and returns a dict. We can see the dict in the model output: { \"info\": [ { \"name\": \"Alex\", \"height\": 5, \"hair_color\": \"blonde\" }, { \"name\": \"Claudia\", \"height\": 6, \"hair_color\": \"brunette\" } ] } The create_extraction_chain then parses the raw LLM output for us\nusing JsonKeyOutputFunctionsParser . This results in the list of JSON objects returned by the chain above: [{'name': 'Alex', 'height': 5, 'hair_color': 'blonde'}, {'name': 'Claudia', 'height': 6, 'hair_color': 'brunette'}] Multiple entity types ​ We can extend this further. Let’s say we want to differentiate between dogs and people. We can add person_ and dog_ prefixes for each property schema = { \"properties\" : { \"person_name\" : { \"type\" : \"string\" } , \"person_height\" : { \"type\" : \"integer\" } , \"person_hair_color\" : { \"type\" : \"string\" } , \"dog_name\" : { \"type\" : \"string\" } , \"dog_breed\" : { \"type\" : \"string\" } , } , \"required\" : [ \"person_name\" , \"person_height\" ] , } chain = create_extraction_chain ( schema , llm ) inp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde. Alex's dog Frosty is a labrador and likes to play hide and seek.\"\"\" chain . run ( inp ) [{'person_name': 'Alex', 'person_height': 5, 'person_hair_color': 'blonde', 'dog_name': 'Frosty', 'dog_breed': 'labrador'}, {'person_name': 'Claudia', 'person_height': 6, 'person_hair_color': 'brunette'}] Unrelated entities ​ If we use required: [] , we allow the model to return only person\nattributes or only dog attributes for a single entity (person or\ndog). schema = { \"properties\" : { \"person_name\" : { \"type\" : \"string\" } , \"person_height\" : { \"type\" : \"integer\" } , \"person_hair_color\" : { \"type\" : \"string\" } , \"dog_name\" : { \"type\" : \"string\" } , \"dog_breed\" : { \"type\" : \"string\" } , } , \"required\" : [ ] , } chain = create_extraction_chain ( schema , llm ) inp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde. Willow is a German Shepherd that likes to play with other dogs and can always be found playing with Milo, a border collie that lives close by.\"\"\" chain . run ( inp ) [{'person_name': 'Alex', 'person_height': 5, 'person_hair_color': 'blonde'}, {'person_name': 'Claudia', 'person_height': 6, 'person_hair_color': 'brunette'}, {'dog_name': 'Willow', 'dog_breed': 'German Shepherd'}, {'dog_name': 'Milo', 'dog_breed': 'border collie'}] Extra information ​ The power of functions (relative to using parsers alone) lies in the\nability to perform semantic extraction. In particular, we can ask for things that are not explicitly enumerated in the schema . Suppose we want unspecified additional information about dogs. We can use add a placeholder for unstructured extraction, dog_extra_info . schema = { \"properties\" : { \"person_name\" : { \"type\" : \"string\" } , \"person_height\" : { \"type\" : \"integer\" } , \"person_hair_color\" : { \"type\" : \"string\" } , \"dog_name\" : { \"type\" : \"string\" } , \"dog_breed\" : { \"type\" : \"string\" } , \"dog_extra_info\" : { \"type\" : \"string\" } , } , } chain = create_extraction_chain ( schema , llm ) chain . run ( inp ) [{'person_name': 'Alex', 'person_height': 5, 'person_hair_color': 'blonde'}, {'person_name': 'Claudia', 'person_height': 6, 'person_hair_color': 'brunette'}, {'dog_name': 'Willow', 'dog_breed': 'German Shepherd', 'dog_extra_info': 'likes to play with other dogs'}, {'dog_name': 'Milo', 'dog_breed': 'border collie', 'dog_extra_info': 'lives close by'}] This gives us additional information about the dogs. Pydantic ​ Pydantic is a data validation and settings management library for\nPython. It allows you to create data classes with attributes that are\nautomatically validated when you instantiate an object. Lets define a class with attributes annotated with types. from typing import Optional from langchain . chains import create_extraction_chain_pydantic from langchain_core . pydantic_v1 import BaseModel # Pydantic data class class Properties ( BaseModel ) : person_name : str person_height : int person_hair_color : str dog_breed : Optional [ str ] dog_name : Optional [ str ] # Extraction chain = create_extraction_chain_pydantic ( pydantic_schema = Properties , llm = llm ) # Run inp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\"\" chain . run ( inp ) [Properties(person_name='Alex', person_height=5, person_hair_color='blonde', dog_breed=None, dog_name=None), Properties(person_name='Claudia', person_height=6, person_hair_color='brunette', dog_breed=None, dog_name=None)] As we can see from the trace ,\nwe use the function information_extraction , as above, with the\nPydantic schema. Option 2: Parsing ​ Output parsers are classes\nthat help structure language model responses. As shown above, they are used to parse the output of the OpenAI function\ncalls in create_extraction_chain . But, they can be used independent of functions. Pydantic ​ Just as a above, let’s parse a generation based on a Pydantic data\nclass. from typing import Optional , Sequence from langchain . llms import OpenAI from langchain . output_parsers import PydanticOutputParser from langchain . prompts import ( PromptTemplate , ) from pydantic import BaseModel , Field , validator class Person ( BaseModel ) : person_name : str person_height : int person_hair_color : str dog_breed : Optional [ str ] dog_name : Optional [ str ] class People ( BaseModel ) : \"\"\"Identifying information about all people in a text.\"\"\" people : Sequence [ Person ] # Run query = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\"\" # Set up a parser + inject instructions into the prompt template. parser = PydanticOutputParser ( pydantic_object = People ) # Prompt prompt = PromptTemplate ( template = \"Answer the user query.\\n{format_instructions}\\n{query}\\n\" , input_variables = [ \"query\" ] , partial_variables = { \"format_instructions\" : parser . get_format_instructions ( ) } , ) # Run _input = prompt . format_prompt ( query = query ) model = OpenAI ( temperature = 0 ) output = model ( _input . to_string ( ) ) parser . parse ( output ) People(people=[Person(person_name='Alex', person_height=5, person_hair_color='blonde', dog_breed=None, dog_name=None), Person(person_name='Claudia', person_height=6, person_hair_color='brunette', dog_breed=None, dog_name=None)]) We can see from the LangSmith\ntrace that we get the same output as above. We can see that we provide a two-shot prompt in order to instruct the\nLLM to output in our desired format. And, we need to do a bit more work: Define a class that holds multiple instances of Person Explicitly parse the output of the LLM to the Pydantic class We can see this for other cases, too. from langchain . llms import OpenAI from langchain . output_parsers import PydanticOutputParser from langchain . prompts import ( PromptTemplate , ) from pydantic import BaseModel , Field , validator # Define your desired data structure. class Joke ( BaseModel ) : setup : str = Field ( description = \"question to set up a joke\" ) punchline : str = Field ( description = \"answer to resolve the joke\" ) # You can add custom validation logic easily with Pydantic. @validator ( \"setup\" ) def question_ends_with_question_mark ( cls , field ) : if field [ - 1 ] != \"?\" : raise ValueError ( \"Badly formed question!\" ) return field # And a query intended to prompt a language model to populate the data structure. joke_query = \"Tell me a joke.\" # Set up a parser + inject instructions into the prompt template. parser = PydanticOutputParser ( pydantic_object = Joke ) # Prompt prompt = PromptTemplate ( template = \"Answer the user query.\\n{format_instructions}\\n{query}\\n\" , input_variables = [ \"query\" ] , partial_variables = { \"format_instructions\" : parser . get_format_instructions ( ) } , ) # Run _input = prompt . format_prompt ( query = joke_query ) model = OpenAI ( temperature = 0 ) output = model ( _input . to_string ( ) ) parser . parse ( output ) Joke(setup='Why did the chicken cross the road?', punchline='To get to the other side!') As we can see, we get an output of the Joke class, which respects our\noriginally desired schema: ‘setup’ and ‘punchline’. We can look at the LangSmith\ntrace to see exactly what is going on under the hood. Going deeper ​ The output parser documentation includes various parser examples for specific types\n(e.g., lists, datetime, enum, etc). The experimental Anthropic function\ncalling support provides similar functionality to Anthropic chat models. LlamaCPP natively supports constrained decoding using custom grammars, making\nit easy to output structured content using local LLMs JSONFormer offers\nanother way for structured decoding of a subset of the JSON Schema. Kor is another library for\nextraction where schema and examples can be provided to the LLM. Previous Chatbots Next Summarization"
    },
    {
        "title": "Summarization | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/summarization",
        "text_content": "Summarization On this page Summarization Open In Colab Use case ​ Suppose you have a set of documents (PDFs, Notion pages, customer\nquestions, etc.) and you want to summarize the content. LLMs are a great tool for this given their proficiency in understanding\nand synthesizing text. In this walkthrough we’ll go over how to perform document summarization\nusing LLMs. Overview ​ A central question for building a summarizer is how to pass your\ndocuments into the LLM’s context window. Two common approaches for this\nare: Stuff : Simply “stuff” all your documents into a single prompt.\nThis is the simplest approach (see here for more on the StuffDocumentsChains , which is used for this method). Map-reduce : Summarize each document on it’s own in a “map” step\nand then “reduce” the summaries into a final summary (see here for more on the MapReduceDocumentsChain , which is used for this method). Quickstart ​ To give you a sneak preview, either pipeline can be wrapped in a single\nobject: load_summarize_chain . Suppose we want to summarize a blog post. We can create this in a few\nlines of code. First set environment variables and install packages: !pip install openai tiktoken chromadb langchain # Set env var OPENAI_API_KEY or load from a .env file # import dotenv # dotenv.load_dotenv() Requirement already satisfied: openai in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.27.8) Requirement already satisfied: tiktoken in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.4.0) Requirement already satisfied: chromadb in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.4.4) Requirement already satisfied: langchain in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.0.299) Requirement already satisfied: requests>=2.20 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from openai) (2.31.0) Requirement already satisfied: tqdm in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from openai) (4.64.1) Requirement already satisfied: aiohttp in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from openai) (3.8.5) Requirement already satisfied: regex>=2022.1.18 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from tiktoken) (2023.6.3) Requirement already satisfied: pydantic<2.0,>=1.9 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.10.12) Requirement already satisfied: chroma-hnswlib==0.7.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.7.2) Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.99.1) Requirement already satisfied: uvicorn[standard]>=0.18.3 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.23.2) Requirement already satisfied: numpy>=1.21.6 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.24.4) Requirement already satisfied: posthog>=2.4.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (3.0.1) Requirement already satisfied: typing-extensions>=4.5.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (4.7.1) Requirement already satisfied: pulsar-client>=3.1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (3.2.0) Requirement already satisfied: onnxruntime>=1.14.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.15.1) Requirement already satisfied: tokenizers>=0.13.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.13.3) Requirement already satisfied: pypika>=0.48.9 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.48.9) Collecting tqdm (from openai) Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.6/57.6 kB 2.7 MB/s eta 0:00:00 Requirement already satisfied: overrides>=7.3.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (7.4.0) Requirement already satisfied: importlib-resources in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (6.0.0) Requirement already satisfied: PyYAML>=5.3 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (6.0.1) Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (2.0.20) Requirement already satisfied: anyio<4.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (3.7.1) Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (4.0.3) Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (0.5.9) Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (1.33) Requirement already satisfied: langsmith<0.1.0,>=0.0.38 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (0.0.42) Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (2.8.5) Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (8.2.3) Requirement already satisfied: attrs>=17.3.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (23.1.0) Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (3.2.0) Requirement already satisfied: multidict<7.0,>=4.5 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (6.0.4) Requirement already satisfied: yarl<2.0,>=1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (1.9.2) Requirement already satisfied: frozenlist>=1.1.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (1.4.0) Requirement already satisfied: aiosignal>=1.1.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1) Requirement already satisfied: idna>=2.8 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from anyio<4.0->langchain) (3.4) Requirement already satisfied: sniffio>=1.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from anyio<4.0->langchain) (1.3.0) Requirement already satisfied: exceptiongroup in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from anyio<4.0->langchain) (1.1.3) Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1) Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (1.5.1) Requirement already satisfied: typing-inspect>=0.4.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0) Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0) Requirement already satisfied: jsonpointer>=1.9 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4) Requirement already satisfied: coloredlogs in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1) Requirement already satisfied: flatbuffers in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26) Requirement already satisfied: packaging in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (23.1) Requirement already satisfied: protobuf in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (4.23.4) Requirement already satisfied: sympy in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12) Requirement already satisfied: six>=1.5 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.16.0) Requirement already satisfied: monotonic>=1.5 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.6) Requirement already satisfied: backoff>=1.10.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.2.1) Requirement already satisfied: python-dateutil>2.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.8.2) Requirement already satisfied: certifi in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22) Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.16) Requirement already satisfied: click>=7.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.7) Requirement already satisfied: h11>=0.8 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0) Requirement already satisfied: httptools>=0.5.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0) Requirement already satisfied: python-dotenv>=0.13 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0) Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0) Requirement already satisfied: watchfiles>=0.13 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0) Requirement already satisfied: websockets>=10.4 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3) Requirement already satisfied: zipp>=3.1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from importlib-resources->chromadb) (3.16.2) Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0) Requirement already satisfied: humanfriendly>=9.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0) Requirement already satisfied: mpmath>=0.19 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0) Using cached tqdm-4.66.1-py3-none-any.whl (78 kB) Installing collected packages: tqdm Attempting uninstall: tqdm Found existing installation: tqdm 4.64.1 Uninstalling tqdm-4.64.1: Successfully uninstalled tqdm-4.64.1 ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. clarifai 9.8.1 requires tqdm==4.64.1, but you have tqdm 4.66.1 which is incompatible. Successfully installed tqdm-4.66.1 We can use chain_type=\"stuff\" , especially if using larger context\nwindow models such as: 16k token OpenAI gpt-3.5-turbo-1106 100k token Anthropic Claude-2 We can also supply chain_type=\"map_reduce\" or chain_type=\"refine\" (read more here ). from langchain . chains . summarize import load_summarize_chain from langchain . chat_models import ChatOpenAI from langchain . document_loaders import WebBaseLoader loader = WebBaseLoader ( \"https://lilianweng.github.io/posts/2023-06-23-agent/\" ) docs = loader . load ( ) llm = ChatOpenAI ( temperature = 0 , model_name = \"gpt-3.5-turbo-1106\" ) chain = load_summarize_chain ( llm , chain_type = \"stuff\" ) chain . run ( docs ) 'The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and proof-of-concept examples of LLM-powered agents in various domains. It also highlights the challenges and limitations of using LLMs in agent systems.' Option 1. Stuff ​ When we use load_summarize_chain with chain_type=\"stuff\" , we will\nuse the StuffDocumentsChain . The chain will take a list of documents, inserts them all into a prompt,\nand passes that prompt to an LLM: from langchain . chains . combine_documents . stuff import StuffDocumentsChain from langchain . chains . llm import LLMChain from langchain . prompts import PromptTemplate # Define prompt prompt_template = \"\"\"Write a concise summary of the following: \"{text}\" CONCISE SUMMARY:\"\"\" prompt = PromptTemplate . from_template ( prompt_template ) # Define LLM chain llm = ChatOpenAI ( temperature = 0 , model_name = \"gpt-3.5-turbo-16k\" ) llm_chain = LLMChain ( llm = llm , prompt = prompt ) # Define StuffDocumentsChain stuff_chain = StuffDocumentsChain ( llm_chain = llm_chain , document_variable_name = \"text\" ) docs = loader . load ( ) print ( stuff_chain . run ( docs ) ) The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and proof-of-concept examples of LLM-powered agents in various domains, such as scientific discovery and generative agents simulation. It also highlights the challenges and limitations of using LLMs in agent systems. Great! We can see that we reproduce the earlier result using the load_summarize_chain . Go deeper ​ You can easily customize the prompt. You can easily try different LLMs, (e.g., Claude ) via the llm parameter. Option 2. Map-Reduce ​ Let’s unpack the map reduce approach. For this, we’ll first map each\ndocument to an individual summary using an LLMChain . Then we’ll use a ReduceDocumentsChain to combine those summaries into a single global\nsummary. First, we specify the LLMChain to use for mapping each document to an\nindividual summary: from langchain . chains import MapReduceDocumentsChain , ReduceDocumentsChain from langchain . text_splitter import CharacterTextSplitter llm = ChatOpenAI ( temperature = 0 ) # Map map_template = \"\"\"The following is a set of documents {docs} Based on this list of docs, please identify the main themes Helpful Answer:\"\"\" map_prompt = PromptTemplate . from_template ( map_template ) map_chain = LLMChain ( llm = llm , prompt = map_prompt ) We can also use the Prompt Hub to store and fetch prompts. This will work with your LangSmith API\nkey . For example, see the map prompt here . from langchain import hub map_prompt = hub . pull ( \"rlm/map-prompt\" ) map_chain = LLMChain ( llm = llm , prompt = map_prompt ) The ReduceDocumentsChain handles taking the document mapping results\nand reducing them into a single output. It wraps a generic CombineDocumentsChain (like StuffDocumentsChain ) but adds the\nability to collapse documents before passing it to the CombineDocumentsChain if their cumulative size exceeds token_max . In\nthis example, we can actually re-use our chain for combining our docs to\nalso collapse our docs. So if the cumulative number of tokens in our mapped documents exceeds\n4000 tokens, then we’ll recursively pass in the documents in batches of\n\\< 4000 tokens to our StuffDocumentsChain to create batched summaries.\nAnd once those batched summaries are cumulatively less than 4000 tokens,\nwe’ll pass them all one last time to the StuffDocumentsChain to create\nthe final summary. # Reduce reduce_template = \"\"\"The following is set of summaries: {docs} Take these and distill it into a final, consolidated summary of the main themes. Helpful Answer:\"\"\" reduce_prompt = PromptTemplate . from_template ( reduce_template ) # Note we can also get this from the prompt hub, as noted above reduce_prompt = hub . pull ( \"rlm/map-prompt\" ) reduce_prompt ChatPromptTemplate(input_variables=['docs'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['docs'], template='The following is a set of documents:\\n{docs}\\nBased on this list of docs, please identify the main themes \\nHelpful Answer:'))]) # Run chain reduce_chain = LLMChain ( llm = llm , prompt = reduce_prompt ) # Takes a list of documents, combines them into a single string, and passes this to an LLMChain combine_documents_chain = StuffDocumentsChain ( llm_chain = reduce_chain , document_variable_name = \"docs\" ) # Combines and iteravely reduces the mapped documents reduce_documents_chain = ReduceDocumentsChain ( # This is final chain that is called. combine_documents_chain = combine_documents_chain , # If documents exceed context for `StuffDocumentsChain` collapse_documents_chain = combine_documents_chain , # The maximum number of tokens to group documents into. token_max = 4000 , ) Combining our map and reduce chains into one: # Combining documents by mapping a chain over them, then combining results map_reduce_chain = MapReduceDocumentsChain ( # Map chain llm_chain = map_chain , # Reduce chain reduce_documents_chain = reduce_documents_chain , # The variable name in the llm_chain to put the documents in document_variable_name = \"docs\" , # Return the results of the map steps in the output return_intermediate_steps = False , ) text_splitter = CharacterTextSplitter . from_tiktoken_encoder ( chunk_size = 1000 , chunk_overlap = 0 ) split_docs = text_splitter . split_documents ( docs ) Created a chunk of size 1003, which is longer than the specified 1000 print ( map_reduce_chain . run ( split_docs ) ) Based on the list of documents provided, the main themes can be identified as follows: 1. LLM-powered autonomous agents: The documents discuss the concept of building agents with LLM as their core controller and highlight the potential of LLM beyond generating written content. They explore the capabilities of LLM as a general problem solver. 2. Agent system overview: The documents provide an overview of the components that make up a LLM-powered autonomous agent system, including planning, memory, and tool use. Each component is explained in detail, highlighting its role in enhancing the agent's capabilities. 3. Planning: The documents discuss how the agent breaks down large tasks into smaller subgoals and utilizes self-reflection to improve the quality of its actions and results. 4. Memory: The documents explain the importance of both short-term and long-term memory in an agent system. Short-term memory is utilized for in-context learning, while long-term memory allows the agent to retain and recall information over extended periods. 5. Tool use: The documents highlight the agent's ability to call external APIs for additional information and resources that may be missing from its pre-trained model weights. This includes accessing current information, executing code, and retrieving proprietary information. 6. Case studies and proof-of-concept examples: The documents provide examples of how LLM-powered autonomous agents can be applied in various domains, such as scientific discovery and generative agent simulations. These case studies serve as examples of the capabilities and potential applications of such agents. 7. Challenges: The documents acknowledge the challenges associated with building and utilizing LLM-powered autonomous agents, although specific challenges are not mentioned in the given set of documents. 8. Citation and references: The documents include a citation and reference section, indicating that the information presented is based on existing research and sources. Overall, the main themes in the provided documents revolve around LLM-powered autonomous agents, their components and capabilities, planning, memory, tool use, case studies, and challenges. Go deeper ​ Customization As shown above, you can customize the LLMs and prompts for map and\nreduce stages. Real-world use-case See this blog\npost case-study on analyzing user interactions (questions about LangChain\ndocumentation)! The blog post and associated repo also introduce\nclustering as a means of summarization. This opens up a third path beyond the stuff or map-reduce approaches that is worth considering. Option 3. Refine ​ Refine is similar to map-reduce: The refine documents chain constructs a response by looping over the\ninput documents and iteratively updating its answer. For each\ndocument, it passes all non-document inputs, the current document, and\nthe latest intermediate answer to an LLM chain to get a new answer. This can be easily run with the chain_type=\"refine\" specified. chain = load_summarize_chain ( llm , chain_type = \"refine\" ) chain . run ( split_docs ) 'The article explores the concept of building autonomous agents powered by large language models (LLMs) and their potential as problem solvers. It discusses different approaches to task decomposition, the integration of self-reflection into LLM-based agents, and the use of external classical planners for long-horizon planning. The new context introduces the Chain of Hindsight (CoH) approach and Algorithm Distillation (AD) for training models to produce better outputs. It also discusses different types of memory and the use of external memory for fast retrieval. The article explores the concept of tool use and introduces the MRKL system and experiments on fine-tuning LLMs to use external tools. It introduces HuggingGPT, a framework that uses ChatGPT as a task planner, and discusses the challenges of using LLM-powered agents in real-world scenarios. The article concludes with case studies on scientific discovery agents and the use of LLM-powered agents in anticancer drug discovery. It also introduces the concept of generative agents that combine LLM with memory, planning, and reflection mechanisms. The conversation samples provided discuss the implementation of a game architecture and the challenges in building LLM-centered agents. The article provides references to related research papers and resources for further exploration.' It’s also possible to supply a prompt and return intermediate steps. prompt_template = \"\"\"Write a concise summary of the following: {text} CONCISE SUMMARY:\"\"\" prompt = PromptTemplate . from_template ( prompt_template ) refine_template = ( \"Your job is to produce a final summary\\n\" \"We have provided an existing summary up to a certain point: {existing_answer}\\n\" \"We have the opportunity to refine the existing summary\" \"(only if needed) with some more context below.\\n\" \"------------\\n\" \"{text}\\n\" \"------------\\n\" \"Given the new context, refine the original summary in Italian\" \"If the context isn't useful, return the original summary.\" ) refine_prompt = PromptTemplate . from_template ( refine_template ) chain = load_summarize_chain ( llm = llm , chain_type = \"refine\" , question_prompt = prompt , refine_prompt = refine_prompt , return_intermediate_steps = True , input_key = \"input_documents\" , output_key = \"output_text\" , ) result = chain ( { \"input_documents\" : split_docs } , return_only_outputs = True ) print ( result [ \"output_text\" ] ) Il presente articolo discute il concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. Esplora i diversi componenti di un sistema di agenti alimentato da LLM, tra cui la pianificazione, la memoria e l'uso degli strumenti. Dimostrazioni di concetto come AutoGPT mostrano il potenziale di LLM come risolutore generale di problemi. Approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorarsi iterativamente. Tuttavia, ci sono sfide da affrontare, come la limitata capacità di contesto che limita l'inclusione di informazioni storiche dettagliate e la difficoltà di pianificazione a lungo termine e decomposizione delle attività. Inoltre, l'affidabilità dell'interfaccia di linguaggio naturale tra LLM e componenti esterni come la memoria e gli strumenti è incerta, poiché i LLM possono commettere errori di formattazione e mostrare comportamenti ribelli. Nonostante ciò, il sistema AutoGPT viene menzionato come esempio di dimostrazione di concetto che utilizza LLM come controller principale per agenti autonomi. Questo articolo fa riferimento a diverse fonti che esplorano approcci e applicazioni specifiche di LLM nell'ambito degli agenti autonomi. print ( \"\\n\\n\" . join ( result [ \"intermediate_steps\" ] [ : 3 ] ) ) This article discusses the concept of building autonomous agents using LLM (large language model) as the core controller. The article explores the different components of an LLM-powered agent system, including planning, memory, and tool use. It also provides examples of proof-of-concept demos and highlights the potential of LLM as a general problem solver. Questo articolo discute del concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. L'articolo esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l'uso degli strumenti. Vengono forniti anche esempi di dimostrazioni di proof-of-concept e si evidenzia il potenziale di LLM come risolutore generale di problemi. Inoltre, vengono presentati approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion che consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente. Questo articolo discute del concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. L'articolo esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l'uso degli strumenti. Vengono forniti anche esempi di dimostrazioni di proof-of-concept e si evidenzia il potenziale di LLM come risolutore generale di problemi. Inoltre, vengono presentati approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion che consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente. Il nuovo contesto riguarda l'approccio Chain of Hindsight (CoH) che permette al modello di migliorare autonomamente i propri output attraverso un processo di apprendimento supervisionato. Viene anche presentato l'approccio Algorithm Distillation (AD) che applica lo stesso concetto alle traiettorie di apprendimento per compiti di reinforcement learning. Splitting and summarizing in a single chain ​ For convenience, we can wrap both the text splitting of our long\ndocument and summarizing in a single AnalyzeDocumentsChain . from langchain . chains import AnalyzeDocumentChain summarize_document_chain = AnalyzeDocumentChain ( combine_docs_chain = chain , text_splitter = text_splitter ) summarize_document_chain . run ( docs [ 0 ] . page_content ) ValueError: `run` not supported when there is not exactly one output key. Got ['output_text', 'intermediate_steps']. Previous Extraction Next Tagging"
    },
    {
        "title": "Tagging | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/tagging",
        "text_content": "Tagging On this page Tagging Open In Colab Use case ​ Tagging means labeling a document with classes such as: sentiment language style (formal, informal etc.) covered topics political tendency Overview ​ Tagging has a few components: function : Like extraction , tagging\nuses functions to specify how the model should tag a document schema : defines how we want to tag the document Quickstart ​ Let’s see a very straightforward example of how we can use OpenAI\nfunctions for tagging in LangChain. !pip install langchain openai # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv() from langchain . chains import create_tagging_chain , create_tagging_chain_pydantic from langchain . chat_models import ChatOpenAI We specify a few properties with their expected type in our schema. # Schema schema = { \"properties\" : { \"sentiment\" : { \"type\" : \"string\" } , \"aggressiveness\" : { \"type\" : \"integer\" } , \"language\" : { \"type\" : \"string\" } , } } # LLM llm = ChatOpenAI ( temperature = 0 , model = \"gpt-3.5-turbo-0613\" ) chain = create_tagging_chain ( schema , llm ) inp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\" chain . run ( inp ) {'sentiment': 'positive', 'language': 'Spanish'} inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\" chain . run ( inp ) {'sentiment': 'enojado', 'aggressiveness': 1, 'language': 'es'} As we can see in the examples, it correctly interprets what we want. The results vary so that we get, for example, sentiments in different\nlanguages (‘positive’, ‘enojado’ etc.). We will see how to control these results in the next section. Finer control ​ Careful schema definition gives us more control over the model’s output. Specifically, we can define: possible values for each property description to make sure that the model understands the property required properties to be returned Here is an example of how we can use _enum_ , _description_ , and _required_ to control for each of the previously mentioned aspects: schema = { \"properties\" : { \"aggressiveness\" : { \"type\" : \"integer\" , \"enum\" : [ 1 , 2 , 3 , 4 , 5 ] , \"description\" : \"describes how aggressive the statement is, the higher the number the more aggressive\" , } , \"language\" : { \"type\" : \"string\" , \"enum\" : [ \"spanish\" , \"english\" , \"french\" , \"german\" , \"italian\" ] , } , } , \"required\" : [ \"language\" , \"sentiment\" , \"aggressiveness\" ] , } chain = create_tagging_chain ( schema , llm ) Now the answers are much better! inp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\" chain . run ( inp ) {'aggressiveness': 0, 'language': 'spanish'} inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\" chain . run ( inp ) {'aggressiveness': 5, 'language': 'spanish'} inp = \"Weather is ok here, I can go outside without much more than a coat\" chain . run ( inp ) {'aggressiveness': 0, 'language': 'english'} The LangSmith\ntrace lets us peek under the hood: As with extraction , we call the information_extraction function here on the input string. This OpenAI function extraction information based upon the provided\nschema. Pydantic ​ We can also use a Pydantic schema to specify the required properties and\ntypes. We can also send other arguments, such as enum or description , to\neach field. This lets us specify our schema in the same manner that we would a new\nclass or function in Python with purely Pythonic types. from langchain_core . pydantic_v1 import BaseModel , Field class Tags ( BaseModel ) : sentiment : str = Field ( . . . , enum = [ \"happy\" , \"neutral\" , \"sad\" ] ) aggressiveness : int = Field ( . . . , description = \"describes how aggressive the statement is, the higher the number the more aggressive\" , enum = [ 1 , 2 , 3 , 4 , 5 ] , ) language : str = Field ( . . . , enum = [ \"spanish\" , \"english\" , \"french\" , \"german\" , \"italian\" ] ) chain = create_tagging_chain_pydantic ( Tags , llm ) inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\" res = chain . run ( inp ) res Tags(sentiment='sad', aggressiveness=5, language='spanish') Going deeper ​ You can use the metadata\ntagger document transformer to extract metadata from a LangChain Document . This covers the same basic functionality as the tagging chain, only\napplied to a LangChain Document . Previous Summarization Next Web scraping"
    },
    {
        "title": "Web scraping | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/web_scraping",
        "text_content": "Web scraping On this page Web scraping Open In Collab Use case ​ Web research is\none of the killer LLM applications: Users have highlighted\nit as one of his top desired AI tools. OSS repos like gpt-researcher are\ngrowing in popularity. Overview ​ Gathering content from the web has a few components: Search : Query to url (e.g., using GoogleSearchAPIWrapper ). Loading : Url to HTML (e.g., using AsyncHtmlLoader , AsyncChromiumLoader , etc). Transforming : HTML to formatted text (e.g., using HTML2Text or Beautiful Soup ). Quickstart ​ pip install - q openai langchain playwright beautifulsoup4 playwright install # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv() Scraping HTML content using a headless instance of Chromium. The async nature of the scraping process is handled using Python’s\nasyncio library. The actual interaction with the web pages is handled by Playwright. from langchain . document_loaders import AsyncChromiumLoader from langchain . document_transformers import BeautifulSoupTransformer # Load HTML loader = AsyncChromiumLoader ( [ \"https://www.wsj.com\" ] ) html = loader . load ( ) Scrape text content tags such as <p>, <li>, <div>, and <a> tags from\nthe HTML content: <p> : The paragraph tag. It defines a paragraph in HTML and is used\nto group together related sentences and/or phrases. <li> : The list item tag. It is used within ordered ( <ol> ) and\nunordered ( <ul> ) lists to define individual items within the list. <div> : The division tag. It is a block-level element used to group\nother inline or block-level elements. <a> : The anchor tag. It is used to define hyperlinks. <span> : an inline container used to mark up a part of a text, or a\npart of a document. For many news websites (e.g., WSJ, CNN), headlines and summaries are all\nin <span> tags. # Transform bs_transformer = BeautifulSoupTransformer ( ) docs_transformed = bs_transformer . transform_documents ( html , tags_to_extract = [ \"span\" ] ) # Result docs_transformed [ 0 ] . page_content [ 0 : 500 ] 'English EditionEnglish中文 (Chinese)日本語 (Japanese) More Other Products from WSJBuy Side from WSJWSJ ShopWSJ Wine Other Products from WSJ Search Quotes and Companies Search Quotes and Companies 0.15% 0.03% 0.12% -0.42% 4.102% -0.69% -0.25% -0.15% -1.82% 0.24% 0.19% -1.10% About Evan His Family Reflects His Reporting How You Can Help Write a Message Life in Detention Latest News Get Email Updates Four Americans Released From Iranian Prison The Americans will remain under house arrest until they are ' These Documents now are staged for downstream usage in various LLM\napps, as discussed below. Loader ​ AsyncHtmlLoader ​ The AsyncHtmlLoader uses the aiohttp library to make asynchronous HTTP requests, suitable\nfor simpler and lightweight scraping. AsyncChromiumLoader ​ The AsyncChromiumLoader uses Playwright to launch a Chromium instance, which can handle\nJavaScript rendering and more complex web interactions. Chromium is one of the browsers supported by Playwright, a library used\nto control browser automation. Headless mode means that the browser is running without a graphical user\ninterface, which is commonly used for web scraping. from langchain . document_loaders import AsyncHtmlLoader urls = [ \"https://www.espn.com\" , \"https://lilianweng.github.io/posts/2023-06-23-agent/\" ] loader = AsyncHtmlLoader ( urls ) docs = loader . load ( ) Transformer ​ HTML2Text ​ HTML2Text provides\na straightforward conversion of HTML content into plain text (with\nmarkdown-like formatting) without any specific tag manipulation. It’s best suited for scenarios where the goal is to extract\nhuman-readable text without needing to manipulate specific HTML\nelements. Beautiful Soup ​ Beautiful Soup offers more fine-grained control over HTML content,\nenabling specific tag extraction, removal, and content cleaning. It’s suited for cases where you want to extract specific information and\nclean up the HTML content according to your needs. from langchain . document_loaders import AsyncHtmlLoader urls = [ \"https://www.espn.com\" , \"https://lilianweng.github.io/posts/2023-06-23-agent/\" ] loader = AsyncHtmlLoader ( urls ) docs = loader . load ( ) Fetching pages: 100%|#############################################################################################################| 2/2 [00:00<00:00,  7.01it/s] from langchain . document_transformers import Html2TextTransformer html2text = Html2TextTransformer ( ) docs_transformed = html2text . transform_documents ( docs ) docs_transformed [ 0 ] . page_content [ 0 : 500 ] \"Skip to main content  Skip to navigation\\n\\n<\\n\\n>\\n\\nMenu\\n\\n## ESPN\\n\\n  * Search\\n\\n  *   * scores\\n\\n  * NFL\\n  * MLB\\n  * NBA\\n  * NHL\\n  * Soccer\\n  * NCAAF\\n  * …\\n\\n    * Women's World Cup\\n    * LLWS\\n    * NCAAM\\n    * NCAAW\\n    * Sports Betting\\n    * Boxing\\n    * CFL\\n    * NCAA\\n    * Cricket\\n    * F1\\n    * Golf\\n    * Horse\\n    * MMA\\n    * NASCAR\\n    * NBA G League\\n    * Olympic Sports\\n    * PLL\\n    * Racing\\n    * RN BB\\n    * RN FB\\n    * Rugby\\n    * Tennis\\n    * WNBA\\n    * WWE\\n    * X Games\\n    * XFL\\n\\n  * More\" Scraping with extraction ​ LLM with function calling ​ Web scraping is challenging for many reasons. One of them is the changing nature of modern websites’ layouts and\ncontent, which requires modifying scraping scripts to accommodate the\nchanges. Using Function (e.g., OpenAI) with an extraction chain, we avoid having\nto change your code constantly when websites change. We’re using gpt-3.5-turbo-0613 to guarantee access to OpenAI Functions\nfeature (although this might be available to everyone by time of\nwriting). We’re also keeping temperature at 0 to keep randomness of the LLM\ndown. from langchain . chat_models import ChatOpenAI llm = ChatOpenAI ( temperature = 0 , model = \"gpt-3.5-turbo-0613\" ) Define a schema ​ Next, you define a schema to specify what kind of data you want to\nextract. Here, the key names matter as they tell the LLM what kind of information\nthey want. So, be as detailed as possible. In this example, we want to scrape only news article’s name and summary\nfrom The Wall Street Journal website. from langchain . chains import create_extraction_chain schema = { \"properties\" : { \"news_article_title\" : { \"type\" : \"string\" } , \"news_article_summary\" : { \"type\" : \"string\" } , } , \"required\" : [ \"news_article_title\" , \"news_article_summary\" ] , } def extract ( content : str , schema : dict ) : return create_extraction_chain ( schema = schema , llm = llm ) . run ( content ) Run the web scraper w/ BeautifulSoup ​ As shown above, we’ll be using BeautifulSoupTransformer . import pprint from langchain . text_splitter import RecursiveCharacterTextSplitter def scrape_with_playwright ( urls , schema ) : loader = AsyncChromiumLoader ( urls ) docs = loader . load ( ) bs_transformer = BeautifulSoupTransformer ( ) docs_transformed = bs_transformer . transform_documents ( docs , tags_to_extract = [ \"span\" ] ) print ( \"Extracting content with LLM\" ) # Grab the first 1000 tokens of the site splitter = RecursiveCharacterTextSplitter . from_tiktoken_encoder ( chunk_size = 1000 , chunk_overlap = 0 ) splits = splitter . split_documents ( docs_transformed ) # Process the first split extracted_content = extract ( schema = schema , content = splits [ 0 ] . page_content ) pprint . pprint ( extracted_content ) return extracted_content urls = [ \"https://www.wsj.com\" ] extracted_content = scrape_with_playwright ( urls , schema = schema ) Extracting content with LLM [{'news_article_summary': 'The Americans will remain under house arrest until ' 'they are allowed to return to the U.S. in coming ' 'weeks, following a monthslong diplomatic push by ' 'the Biden administration.', 'news_article_title': 'Four Americans Released From Iranian Prison'}, {'news_article_summary': 'Price pressures continued cooling last month, with ' 'the CPI rising a mild 0.2% from June, likely ' 'deterring the Federal Reserve from raising interest ' 'rates at its September meeting.', 'news_article_title': 'Cooler July Inflation Opens Door to Fed Pause on ' 'Rates'}, {'news_article_summary': 'The company has decided to eliminate 27 of its 30 ' 'clothing labels, such as Lark & Ro and Goodthreads, ' 'as it works to fend off antitrust scrutiny and cut ' 'costs.', 'news_article_title': 'Amazon Cuts Dozens of House Brands'}, {'news_article_summary': 'President Biden’s order comes on top of a slowing ' 'Chinese economy, Covid lockdowns and rising ' 'tensions between the two powers.', 'news_article_title': 'U.S. Investment Ban on China Poised to Deepen Divide'}, {'news_article_summary': 'The proposed trial date in the ' 'election-interference case comes on the same day as ' 'the former president’s not guilty plea on ' 'additional Mar-a-Lago charges.', 'news_article_title': 'Trump Should Be Tried in January, Prosecutors Tell ' 'Judge'}, {'news_article_summary': 'The CEO who started in June says the platform has ' '“an entirely different road map” for the future.', 'news_article_title': 'Yaccarino Says X Is Watching Threads but Has Its Own ' 'Vision'}, {'news_article_summary': 'Students foot the bill for flagship state ' 'universities that pour money into new buildings and ' 'programs with little pushback.', 'news_article_title': 'Colleges Spend Like There’s No Tomorrow. ‘These ' 'Places Are Just Devouring Money.’'}, {'news_article_summary': 'Wildfires fanned by hurricane winds have torn ' 'through parts of the Hawaiian island, devastating ' 'the popular tourist town of Lahaina.', 'news_article_title': 'Maui Wildfires Leave at Least 36 Dead'}, {'news_article_summary': 'After its large armored push stalled, Kyiv has ' 'fallen back on the kind of tactics that brought it ' 'success earlier in the war.', 'news_article_title': 'Ukraine Uses Small-Unit Tactics to Retake Captured ' 'Territory'}, {'news_article_summary': 'President Guillermo Lasso says the Aug. 20 election ' 'will proceed, as the Andean country grapples with ' 'rising drug gang violence.', 'news_article_title': 'Ecuador Declares State of Emergency After ' 'Presidential Hopeful Killed'}, {'news_article_summary': 'This year’s hurricane season, which typically runs ' 'from June to the end of November, has been ' 'difficult to predict, climate scientists said.', 'news_article_title': 'Atlantic Hurricane Season Prediction Increased to ' '‘Above Normal,’ NOAA Says'}, {'news_article_summary': 'The NFL is raising the price of its NFL+ streaming ' 'packages as it adds the NFL Network and RedZone.', 'news_article_title': 'NFL to Raise Price of NFL+ Streaming Packages as It ' 'Adds NFL Network, RedZone'}, {'news_article_summary': 'Russia is planning a moon mission as part of the ' 'new space race.', 'news_article_title': 'Russia’s Moon Mission and the New Space Race'}, {'news_article_summary': 'Tapestry’s $8.5 billion acquisition of Capri would ' 'create a conglomerate with more than $12 billion in ' 'annual sales, but it would still lack the ' 'high-wattage labels and diversity that have fueled ' 'LVMH’s success.', 'news_article_title': \"Why the Coach and Kors Marriage Doesn't Scare LVMH\"}, {'news_article_summary': 'The Supreme Court has blocked Purdue Pharma’s $6 ' 'billion Sackler opioid settlement.', 'news_article_title': 'Supreme Court Blocks Purdue Pharma’s $6 Billion ' 'Sackler Opioid Settlement'}, {'news_article_summary': 'The Social Security COLA is expected to rise in ' '2024, but not by a lot.', 'news_article_title': 'Social Security COLA Expected to Rise in 2024, but ' 'Not by a Lot'}] We can compare the headlines scraped to the page: Looking at the LangSmith\ntrace ,\nwe can see what is going on under the hood: It’s following what is explained in the extraction . We call the information_extraction function on the input text. It will attempt to populate the provided schema from the url\ncontent. Research automation ​ Related to scraping, we may want to answer specific questions using\nsearched content. We can automate the process of web\nresearch using a\nretriever, such as the WebResearchRetriever ( docs ). Copy requirements from\nhere : pip install -r requirements.txt Set GOOGLE_CSE_ID and GOOGLE_API_KEY . from langchain . chat_models . openai import ChatOpenAI from langchain . embeddings import OpenAIEmbeddings from langchain . retrievers . web_research import WebResearchRetriever from langchain . utilities import GoogleSearchAPIWrapper from langchain . vectorstores import Chroma # Vectorstore vectorstore = Chroma ( embedding_function = OpenAIEmbeddings ( ) , persist_directory = \"./chroma_db_oai\" ) # LLM llm = ChatOpenAI ( temperature = 0 ) # Search search = GoogleSearchAPIWrapper ( ) Initialize retriever with the above tools to: Use an LLM to generate multiple relevant search queries (one LLM\ncall) Execute a search for each query Choose the top K links per query (multiple search calls in parallel) Load the information from all chosen links (scrape pages in\nparallel) Index those documents into a vectorstore Find the most relevant documents for each original generated search\nquery # Initialize web_research_retriever = WebResearchRetriever . from_llm ( vectorstore = vectorstore , llm = llm , search = search ) # Run import logging logging . basicConfig ( ) logging . getLogger ( \"langchain.retrievers.web_research\" ) . setLevel ( logging . INFO ) from langchain . chains import RetrievalQAWithSourcesChain user_input = \"How do LLM Powered Autonomous Agents work?\" qa_chain = RetrievalQAWithSourcesChain . from_chain_type ( llm , retriever = web_research_retriever ) result = qa_chain ( { \"question\" : user_input } ) result INFO:langchain.retrievers.web_research:Generating questions for Google Search ... INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'How do LLM Powered Autonomous Agents work?', 'text': LineList(lines=['1. What is the functioning principle of LLM Powered Autonomous Agents?\\n', '2. How do LLM Powered Autonomous Agents operate?\\n'])} INFO:langchain.retrievers.web_research:Questions for Google Search: ['1. What is the functioning principle of LLM Powered Autonomous Agents?\\n', '2. How do LLM Powered Autonomous Agents operate?\\n'] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{'title': 'LLM Powered Autonomous Agents | Hacker News', 'link': 'https://news.ycombinator.com/item?id=36488871', 'snippet': 'Jun 26, 2023 ... Exactly. A temperature of 0 means you always pick the highest probability token (i.e. the \"max\" function), while a temperature of 1 means you\\xa0...'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\" , \"What are the subgoals for achieving XYZ?\" , (2) by\\xa0...'}] INFO:langchain.retrievers.web_research:New URLs to load: [] INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls... {'question': 'How do LLM Powered Autonomous Agents work?', 'answer': \"LLM-powered autonomous agents work by using LLM as the agent's brain, complemented by several key components such as planning, memory, and tool use. In terms of planning, the agent breaks down large tasks into smaller subgoals and can reflect and refine its actions based on past experiences. Memory is divided into short-term memory, which is used for in-context learning, and long-term memory, which allows the agent to retain and recall information over extended periods. Tool use involves the agent calling external APIs for additional information. These agents have been used in various applications, including scientific discovery and generative agents simulation.\", 'sources': ''} Going deeper ​ Here’s a app that\nwraps this retriever with a lighweight UI. Question answering over a website ​ To answer questions over a specific website, you can use Apify’s Website Content\nCrawler Actor, which\ncan deeply crawl websites such as documentation, knowledge bases, help\ncenters, or blogs, and extract text content from the web pages. In the example below, we will deeply crawl the Python documentation of\nLangChain’s Chat LLM models and answer a question over it. First, install the requirements pip install apify-client openai langchain chromadb tiktoken Next, set OPENAI_API_KEY and APIFY_API_TOKEN in your environment\nvariables. The full code follows: from langchain . docstore . document import Document from langchain . indexes import VectorstoreIndexCreator from langchain . utilities import ApifyWrapper apify = ApifyWrapper ( ) # Call the Actor to obtain text from the crawled webpages loader = apify . call_actor ( actor_id = \"apify/website-content-crawler\" , run_input = { \"startUrls\" : [ { \"url\" : \"https://python.langchain.com/docs/integrations/chat/\" } ] } , dataset_mapping_function = lambda item : Document ( page_content = item [ \"text\" ] or \"\" , metadata = { \"source\" : item [ \"url\" ] } ) , ) # Create a vector store based on the crawled data index = VectorstoreIndexCreator ( ) . from_loaders ( [ loader ] ) # Query the vector store query = \"Are any OpenAI chat models integrated in LangChain?\" result = index . query ( query ) print ( result ) Yes, LangChain offers integration with OpenAI chat models. You can use the ChatOpenAI class to interact with OpenAI models. Previous Tagging Next Synthetic data generation"
    },
    {
        "title": "Synthetic data generation | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/data_generation",
        "text_content": "Synthetic data generation On this page Synthetic data generation Open In Colab Use case ​ Synthetic data is artificially generated data, rather than data\ncollected from real-world events. It’s used to simulate real data\nwithout compromising privacy or encountering real-world limitations. Benefits of Synthetic Data: Privacy and Security : No real personal data at risk of breaches. Data Augmentation : Expands datasets for machine learning. Flexibility : Create specific or rare scenarios. Cost-effective : Often cheaper than real-world data collection. Regulatory Compliance : Helps navigate strict data protection\nlaws. Model Robustness : Can lead to better generalizing AI models. Rapid Prototyping : Enables quick testing without real data. Controlled Experimentation : Simulate specific conditions. Access to Data : Alternative when real data isn’t available. Note: Despite the benefits, synthetic data should be used carefully, as\nit may not always capture real-world complexities. Quickstart ​ In this notebook, we’ll dive deep into generating synthetic medical\nbilling records using the langchain library. This tool is particularly\nuseful when you want to develop or test algorithms but don’t want to use\nreal patient data due to privacy concerns or data availability issues. Setup ​ First, you’ll need to have the langchain library installed, along with\nits dependencies. Since we’re using the OpenAI generator chain, we’ll\ninstall that as well. Since this is an experimental lib, we’ll need to\ninclude langchain_experimental in our installs. We’ll then import the\nnecessary modules. !pip install - U langchain langchain_experimental openai # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv() from langchain . chat_models import ChatOpenAI from langchain . prompts import FewShotPromptTemplate , PromptTemplate from langchain_core . pydantic_v1 import BaseModel from langchain_experimental . tabular_synthetic_data . openai import ( OPENAI_TEMPLATE , create_openai_data_generator , ) from langchain_experimental . tabular_synthetic_data . prompts import ( SYNTHETIC_FEW_SHOT_PREFIX , SYNTHETIC_FEW_SHOT_SUFFIX , ) 1. Define Your Data Model ​ Every dataset has a structure or a “schema”. The MedicalBilling class\nbelow serves as our schema for the synthetic data. By defining this,\nwe’re informing our synthetic data generator about the shape and nature\nof data we expect. class MedicalBilling ( BaseModel ) : patient_id : int patient_name : str diagnosis_code : str procedure_code : str total_charge : float insurance_claim_amount : float For instance, every record will have a patient_id that’s an integer, a patient_name that’s a string, and so on. 2. Sample Data ​ To guide the synthetic data generator, it’s useful to provide it with a\nfew real-world-like examples. These examples serve as a “seed” - they’re\nrepresentative of the kind of data you want, and the generator will use\nthem to create more data that looks similar. Here are some fictional medical billing records: examples = [ { \"example\" : \"\"\"Patient ID: 123456, Patient Name: John Doe, Diagnosis Code: J20.9, Procedure Code: 99203, Total Charge: $500, Insurance Claim Amount: $350\"\"\" } , { \"example\" : \"\"\"Patient ID: 789012, Patient Name: Johnson Smith, Diagnosis Code: M54.5, Procedure Code: 99213, Total Charge: $150, Insurance Claim Amount: $120\"\"\" } , { \"example\" : \"\"\"Patient ID: 345678, Patient Name: Emily Stone, Diagnosis Code: E11.9, Procedure Code: 99214, Total Charge: $300, Insurance Claim Amount: $250\"\"\" } , ] 3. Craft a Prompt Template ​ The generator doesn’t magically know how to create our data; we need to\nguide it. We do this by creating a prompt template. This template helps\ninstruct the underlying language model on how to produce synthetic data\nin the desired format. OPENAI_TEMPLATE = PromptTemplate ( input_variables = [ \"example\" ] , template = \"{example}\" ) prompt_template = FewShotPromptTemplate ( prefix = SYNTHETIC_FEW_SHOT_PREFIX , examples = examples , suffix = SYNTHETIC_FEW_SHOT_SUFFIX , input_variables = [ \"subject\" , \"extra\" ] , example_prompt = OPENAI_TEMPLATE , ) The FewShotPromptTemplate includes: prefix and suffix : These likely contain guiding context or\ninstructions. examples : The sample data we defined earlier. input_variables : These variables (“subject”, “extra”) are\nplaceholders you can dynamically fill later. For instance, “subject”\nmight be filled with “medical_billing” to guide the model further. example_prompt : This prompt template is the format we want each\nexample row to take in our prompt. 4. Creating the Data Generator ​ With the schema and the prompt ready, the next step is to create the\ndata generator. This object knows how to communicate with the underlying\nlanguage model to get synthetic data. synthetic_data_generator = create_openai_data_generator ( output_schema = MedicalBilling , llm = ChatOpenAI ( temperature = 1 ) , # You'll need to replace with your actual Language Model instance prompt = prompt_template , ) 5. Generate Synthetic Data ​ Finally, let’s get our synthetic data! synthetic_results = synthetic_data_generator . generate ( subject = \"medical_billing\" , extra = \"the name must be chosen at random. Make it something you wouldn't normally choose.\" , runs = 10 , ) This command asks the generator to produce 10 synthetic medical billing\nrecords. The results are stored in synthetic_results . The output will\nbe a list of the MedicalBilling pydantic models. Other implementations ​ from langchain . chat_models import ChatOpenAI from langchain_experimental . synthetic_data import ( DatasetGenerator , create_data_generation_chain , ) # LLM model = ChatOpenAI ( model_name = \"gpt-3.5-turbo\" , temperature = 0.7 ) chain = create_data_generation_chain ( model ) chain ( { \"fields\" : [ \"blue\" , \"yellow\" ] , \"preferences\" : { } } ) {'fields': ['blue', 'yellow'], 'preferences': {}, 'text': 'The vibrant blue sky contrasted beautifully with the bright yellow sun, creating a stunning display of colors that instantly lifted the spirits of all who gazed upon it.'} chain ( { \"fields\" : { \"colors\" : [ \"blue\" , \"yellow\" ] } , \"preferences\" : { \"style\" : \"Make it in a style of a weather forecast.\" } , } ) {'fields': {'colors': ['blue', 'yellow']}, 'preferences': {'style': 'Make it in a style of a weather forecast.'}, 'text': \"Good morning! Today's weather forecast brings a beautiful combination of colors to the sky, with hues of blue and yellow gently blending together like a mesmerizing painting.\"} chain ( { \"fields\" : { \"actor\" : \"Tom Hanks\" , \"movies\" : [ \"Forrest Gump\" , \"Green Mile\" ] } , \"preferences\" : None , } ) {'fields': {'actor': 'Tom Hanks', 'movies': ['Forrest Gump', 'Green Mile']}, 'preferences': None, 'text': 'Tom Hanks, the renowned actor known for his incredible versatility and charm, has graced the silver screen in unforgettable movies such as \"Forrest Gump\" and \"Green Mile\".'} chain ( { \"fields\" : [ { \"actor\" : \"Tom Hanks\" , \"movies\" : [ \"Forrest Gump\" , \"Green Mile\" ] } , { \"actor\" : \"Mads Mikkelsen\" , \"movies\" : [ \"Hannibal\" , \"Another round\" ] } , ] , \"preferences\" : { \"minimum_length\" : 200 , \"style\" : \"gossip\" } , } ) {'fields': [{'actor': 'Tom Hanks', 'movies': ['Forrest Gump', 'Green Mile']}, {'actor': 'Mads Mikkelsen', 'movies': ['Hannibal', 'Another round']}], 'preferences': {'minimum_length': 200, 'style': 'gossip'}, 'text': 'Did you know that Tom Hanks, the beloved Hollywood actor known for his roles in \"Forrest Gump\" and \"Green Mile\", has shared the screen with the talented Mads Mikkelsen, who gained international acclaim for his performances in \"Hannibal\" and \"Another round\"? These two incredible actors have brought their exceptional skills and captivating charisma to the big screen, delivering unforgettable performances that have enthralled audiences around the world. Whether it\\'s Hanks\\' endearing portrayal of Forrest Gump or Mikkelsen\\'s chilling depiction of Hannibal Lecter, these movies have solidified their places in cinematic history, leaving a lasting impact on viewers and cementing their status as true icons of the silver screen.'} As we can see created examples are diversified and possess information\nwe wanted them to have. Also, their style reflects the given preferences\nquite well. Generating exemplary dataset for extraction benchmarking purposes ​ inp = [ { \"Actor\" : \"Tom Hanks\" , \"Film\" : [ \"Forrest Gump\" , \"Saving Private Ryan\" , \"The Green Mile\" , \"Toy Story\" , \"Catch Me If You Can\" , ] , } , { \"Actor\" : \"Tom Hardy\" , \"Film\" : [ \"Inception\" , \"The Dark Knight Rises\" , \"Mad Max: Fury Road\" , \"The Revenant\" , \"Dunkirk\" , ] , } , ] generator = DatasetGenerator ( model , { \"style\" : \"informal\" , \"minimal length\" : 500 } ) dataset = generator ( inp ) dataset [{'fields': {'Actor': 'Tom Hanks', 'Film': ['Forrest Gump', 'Saving Private Ryan', 'The Green Mile', 'Toy Story', 'Catch Me If You Can']}, 'preferences': {'style': 'informal', 'minimal length': 500}, 'text': 'Tom Hanks, the versatile and charismatic actor, has graced the silver screen in numerous iconic films including the heartwarming and inspirational \"Forrest Gump,\" the intense and gripping war drama \"Saving Private Ryan,\" the emotionally charged and thought-provoking \"The Green Mile,\" the beloved animated classic \"Toy Story,\" and the thrilling and captivating true story adaptation \"Catch Me If You Can.\" With his impressive range and genuine talent, Hanks continues to captivate audiences worldwide, leaving an indelible mark on the world of cinema.'}, {'fields': {'Actor': 'Tom Hardy', 'Film': ['Inception', 'The Dark Knight Rises', 'Mad Max: Fury Road', 'The Revenant', 'Dunkirk']}, 'preferences': {'style': 'informal', 'minimal length': 500}, 'text': 'Tom Hardy, the versatile actor known for his intense performances, has graced the silver screen in numerous iconic films, including \"Inception,\" \"The Dark Knight Rises,\" \"Mad Max: Fury Road,\" \"The Revenant,\" and \"Dunkirk.\" Whether he\\'s delving into the depths of the subconscious mind, donning the mask of the infamous Bane, or navigating the treacherous wasteland as the enigmatic Max Rockatansky, Hardy\\'s commitment to his craft is always evident. From his breathtaking portrayal of the ruthless Eames in \"Inception\" to his captivating transformation into the ferocious Max in \"Mad Max: Fury Road,\" Hardy\\'s dynamic range and magnetic presence captivate audiences and leave an indelible mark on the world of cinema. In his most physically demanding role to date, he endured the harsh conditions of the freezing wilderness as he portrayed the rugged frontiersman John Fitzgerald in \"The Revenant,\" earning him critical acclaim and an Academy Award nomination. In Christopher Nolan\\'s war epic \"Dunkirk,\" Hardy\\'s stoic and heroic portrayal of Royal Air Force pilot Farrier showcases his ability to convey deep emotion through nuanced performances. With his chameleon-like ability to inhabit a wide range of characters and his unwavering commitment to his craft, Tom Hardy has undoubtedly solidified his place as one of the most talented and sought-after actors of his generation.'}] Extraction from generated examples ​ Okay, let’s see if we can now extract output from this generated data\nand how it compares with our case! from typing import List from langchain . chains import create_extraction_chain_pydantic from langchain . llms import OpenAI from langchain . output_parsers import PydanticOutputParser from langchain . prompts import PromptTemplate from pydantic import BaseModel , Field class Actor ( BaseModel ) : Actor : str = Field ( description = \"name of an actor\" ) Film : List [ str ] = Field ( description = \"list of names of films they starred in\" ) Parsers ​ llm = OpenAI ( ) parser = PydanticOutputParser ( pydantic_object = Actor ) prompt = PromptTemplate ( template = \"Extract fields from a given text.\\n{format_instructions}\\n{text}\\n\" , input_variables = [ \"text\" ] , partial_variables = { \"format_instructions\" : parser . get_format_instructions ( ) } , ) _input = prompt . format_prompt ( text = dataset [ 0 ] [ \"text\" ] ) output = llm ( _input . to_string ( ) ) parsed = parser . parse ( output ) parsed Actor(Actor='Tom Hanks', Film=['Forrest Gump', 'Saving Private Ryan', 'The Green Mile', 'Toy Story', 'Catch Me If You Can']) ( parsed . Actor == inp [ 0 ] [ \"Actor\" ] ) & ( parsed . Film == inp [ 0 ] [ \"Film\" ] ) True Extractors ​ extractor = create_extraction_chain_pydantic ( pydantic_schema = Actor , llm = model ) extracted = extractor . run ( dataset [ 1 ] [ \"text\" ] ) extracted [Actor(Actor='Tom Hardy', Film=['Inception', 'The Dark Knight Rises', 'Mad Max: Fury Road', 'The Revenant', 'Dunkirk'])] ( extracted [ 0 ] . Actor == inp [ 1 ] [ \"Actor\" ] ) & ( extracted [ 0 ] . Film == inp [ 1 ] [ \"Film\" ] ) True Previous Web scraping Next Graph querying"
    },
    {
        "title": "Graph querying | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/graph/",
        "text_content": "Graph querying Graph querying Graph databases give us a powerful way to represent and query real-world relationships. There are a number of chains that make it easy to use LLMs to interact with various graph DBs. 📄️ Diffbot Graph Transformer Open In Colab 📄️ ArangoDB QA chain Open In Colab 📄️ Neo4j DB QA chain This notebook shows how to use LLMs to provide a natural language 📄️ FalkorDBQAChain This notebook shows how to use LLMs to provide a natural language 📄️ HugeGraph QA Chain This notebook shows how to use LLMs to provide a natural language 📄️ KuzuQAChain This notebook shows how to use LLMs to provide a natural language 📄️ Memgraph QA chain This notebook shows how to use LLMs to provide a natural language 📄️ NebulaGraphQAChain This notebook shows how to use LLMs to provide a natural language 📄️ NetworkX NetworkX is a Python package for the 📄️ GraphSparqlQAChain Graph databases are an excellent choice for applications based on 📄️ Neptune Open Cypher QA Chain This QA chain queries Neptune graph database using openCypher and Previous Synthetic data generation Next Diffbot Graph Transformer"
    },
    {
        "title": "CSV | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/csv",
        "text_content": "Components Document loaders CSV On this page CSV A comma-separated values\n(CSV) file is a\ndelimited text file that uses a comma to separate values. Each line of\nthe file is a data record. Each record consists of one or more fields,\nseparated by commas. Load csv data\nwith a single row per document. from langchain . document_loaders . csv_loader import CSVLoader loader = CSVLoader ( file_path = \"./example_data/mlb_teams_2012.csv\" ) data = loader . load ( ) print ( data ) [Document(page_content='Team: Nationals\\n\"Payroll (millions)\": 81.34\\n\"Wins\": 98', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 0}, lookup_index=0), Document(page_content='Team: Reds\\n\"Payroll (millions)\": 82.20\\n\"Wins\": 97', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 1}, lookup_index=0), Document(page_content='Team: Yankees\\n\"Payroll (millions)\": 197.96\\n\"Wins\": 95', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 2}, lookup_index=0), Document(page_content='Team: Giants\\n\"Payroll (millions)\": 117.62\\n\"Wins\": 94', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 3}, lookup_index=0), Document(page_content='Team: Braves\\n\"Payroll (millions)\": 83.31\\n\"Wins\": 94', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 4}, lookup_index=0), Document(page_content='Team: Athletics\\n\"Payroll (millions)\": 55.37\\n\"Wins\": 94', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 5}, lookup_index=0), Document(page_content='Team: Rangers\\n\"Payroll (millions)\": 120.51\\n\"Wins\": 93', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 6}, lookup_index=0), Document(page_content='Team: Orioles\\n\"Payroll (millions)\": 81.43\\n\"Wins\": 93', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 7}, lookup_index=0), Document(page_content='Team: Rays\\n\"Payroll (millions)\": 64.17\\n\"Wins\": 90', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 8}, lookup_index=0), Document(page_content='Team: Angels\\n\"Payroll (millions)\": 154.49\\n\"Wins\": 89', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 9}, lookup_index=0), Document(page_content='Team: Tigers\\n\"Payroll (millions)\": 132.30\\n\"Wins\": 88', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 10}, lookup_index=0), Document(page_content='Team: Cardinals\\n\"Payroll (millions)\": 110.30\\n\"Wins\": 88', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 11}, lookup_index=0), Document(page_content='Team: Dodgers\\n\"Payroll (millions)\": 95.14\\n\"Wins\": 86', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 12}, lookup_index=0), Document(page_content='Team: White Sox\\n\"Payroll (millions)\": 96.92\\n\"Wins\": 85', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 13}, lookup_index=0), Document(page_content='Team: Brewers\\n\"Payroll (millions)\": 97.65\\n\"Wins\": 83', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 14}, lookup_index=0), Document(page_content='Team: Phillies\\n\"Payroll (millions)\": 174.54\\n\"Wins\": 81', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 15}, lookup_index=0), Document(page_content='Team: Diamondbacks\\n\"Payroll (millions)\": 74.28\\n\"Wins\": 81', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 16}, lookup_index=0), Document(page_content='Team: Pirates\\n\"Payroll (millions)\": 63.43\\n\"Wins\": 79', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 17}, lookup_index=0), Document(page_content='Team: Padres\\n\"Payroll (millions)\": 55.24\\n\"Wins\": 76', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 18}, lookup_index=0), Document(page_content='Team: Mariners\\n\"Payroll (millions)\": 81.97\\n\"Wins\": 75', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 19}, lookup_index=0), Document(page_content='Team: Mets\\n\"Payroll (millions)\": 93.35\\n\"Wins\": 74', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 20}, lookup_index=0), Document(page_content='Team: Blue Jays\\n\"Payroll (millions)\": 75.48\\n\"Wins\": 73', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 21}, lookup_index=0), Document(page_content='Team: Royals\\n\"Payroll (millions)\": 60.91\\n\"Wins\": 72', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 22}, lookup_index=0), Document(page_content='Team: Marlins\\n\"Payroll (millions)\": 118.07\\n\"Wins\": 69', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 23}, lookup_index=0), Document(page_content='Team: Red Sox\\n\"Payroll (millions)\": 173.18\\n\"Wins\": 69', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 24}, lookup_index=0), Document(page_content='Team: Indians\\n\"Payroll (millions)\": 78.43\\n\"Wins\": 68', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 25}, lookup_index=0), Document(page_content='Team: Twins\\n\"Payroll (millions)\": 94.08\\n\"Wins\": 66', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 26}, lookup_index=0), Document(page_content='Team: Rockies\\n\"Payroll (millions)\": 78.06\\n\"Wins\": 64', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 27}, lookup_index=0), Document(page_content='Team: Cubs\\n\"Payroll (millions)\": 88.19\\n\"Wins\": 61', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 28}, lookup_index=0), Document(page_content='Team: Astros\\n\"Payroll (millions)\": 60.65\\n\"Wins\": 55', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 29}, lookup_index=0)] Customizing the csv parsing and loading ​ See the csv module documentation for more information of what csv args are supported. loader = CSVLoader ( file_path = \"./example_data/mlb_teams_2012.csv\" , csv_args = { \"delimiter\" : \",\" , \"quotechar\" : '\"' , \"fieldnames\" : [ \"MLB Team\" , \"Payroll in millions\" , \"Wins\" ] , } , ) data = loader . load ( ) print ( data ) [Document(page_content='MLB Team: Team\\nPayroll in millions: \"Payroll (millions)\"\\nWins: \"Wins\"', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 0}, lookup_index=0), Document(page_content='MLB Team: Nationals\\nPayroll in millions: 81.34\\nWins: 98', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 1}, lookup_index=0), Document(page_content='MLB Team: Reds\\nPayroll in millions: 82.20\\nWins: 97', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 2}, lookup_index=0), Document(page_content='MLB Team: Yankees\\nPayroll in millions: 197.96\\nWins: 95', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 3}, lookup_index=0), Document(page_content='MLB Team: Giants\\nPayroll in millions: 117.62\\nWins: 94', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 4}, lookup_index=0), Document(page_content='MLB Team: Braves\\nPayroll in millions: 83.31\\nWins: 94', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 5}, lookup_index=0), Document(page_content='MLB Team: Athletics\\nPayroll in millions: 55.37\\nWins: 94', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 6}, lookup_index=0), Document(page_content='MLB Team: Rangers\\nPayroll in millions: 120.51\\nWins: 93', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 7}, lookup_index=0), Document(page_content='MLB Team: Orioles\\nPayroll in millions: 81.43\\nWins: 93', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 8}, lookup_index=0), Document(page_content='MLB Team: Rays\\nPayroll in millions: 64.17\\nWins: 90', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 9}, lookup_index=0), Document(page_content='MLB Team: Angels\\nPayroll in millions: 154.49\\nWins: 89', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 10}, lookup_index=0), Document(page_content='MLB Team: Tigers\\nPayroll in millions: 132.30\\nWins: 88', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 11}, lookup_index=0), Document(page_content='MLB Team: Cardinals\\nPayroll in millions: 110.30\\nWins: 88', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 12}, lookup_index=0), Document(page_content='MLB Team: Dodgers\\nPayroll in millions: 95.14\\nWins: 86', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 13}, lookup_index=0), Document(page_content='MLB Team: White Sox\\nPayroll in millions: 96.92\\nWins: 85', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 14}, lookup_index=0), Document(page_content='MLB Team: Brewers\\nPayroll in millions: 97.65\\nWins: 83', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 15}, lookup_index=0), Document(page_content='MLB Team: Phillies\\nPayroll in millions: 174.54\\nWins: 81', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 16}, lookup_index=0), Document(page_content='MLB Team: Diamondbacks\\nPayroll in millions: 74.28\\nWins: 81', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 17}, lookup_index=0), Document(page_content='MLB Team: Pirates\\nPayroll in millions: 63.43\\nWins: 79', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 18}, lookup_index=0), Document(page_content='MLB Team: Padres\\nPayroll in millions: 55.24\\nWins: 76', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 19}, lookup_index=0), Document(page_content='MLB Team: Mariners\\nPayroll in millions: 81.97\\nWins: 75', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 20}, lookup_index=0), Document(page_content='MLB Team: Mets\\nPayroll in millions: 93.35\\nWins: 74', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 21}, lookup_index=0), Document(page_content='MLB Team: Blue Jays\\nPayroll in millions: 75.48\\nWins: 73', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 22}, lookup_index=0), Document(page_content='MLB Team: Royals\\nPayroll in millions: 60.91\\nWins: 72', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 23}, lookup_index=0), Document(page_content='MLB Team: Marlins\\nPayroll in millions: 118.07\\nWins: 69', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 24}, lookup_index=0), Document(page_content='MLB Team: Red Sox\\nPayroll in millions: 173.18\\nWins: 69', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 25}, lookup_index=0), Document(page_content='MLB Team: Indians\\nPayroll in millions: 78.43\\nWins: 68', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 26}, lookup_index=0), Document(page_content='MLB Team: Twins\\nPayroll in millions: 94.08\\nWins: 66', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 27}, lookup_index=0), Document(page_content='MLB Team: Rockies\\nPayroll in millions: 78.06\\nWins: 64', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 28}, lookup_index=0), Document(page_content='MLB Team: Cubs\\nPayroll in millions: 88.19\\nWins: 61', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 29}, lookup_index=0), Document(page_content='MLB Team: Astros\\nPayroll in millions: 60.65\\nWins: 55', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 30}, lookup_index=0)] Specify a column to identify the document source ​ Use the source_column argument to specify a source for the document\ncreated from each row. Otherwise file_path will be used as the source\nfor all documents created from the CSV file. This is useful when using documents loaded from CSV files for chains\nthat answer questions using sources. loader = CSVLoader ( file_path = \"./example_data/mlb_teams_2012.csv\" , source_column = \"Team\" ) data = loader . load ( ) print ( data ) [Document(page_content='Team: Nationals\\n\"Payroll (millions)\": 81.34\\n\"Wins\": 98', lookup_str='', metadata={'source': 'Nationals', 'row': 0}, lookup_index=0), Document(page_content='Team: Reds\\n\"Payroll (millions)\": 82.20\\n\"Wins\": 97', lookup_str='', metadata={'source': 'Reds', 'row': 1}, lookup_index=0), Document(page_content='Team: Yankees\\n\"Payroll (millions)\": 197.96\\n\"Wins\": 95', lookup_str='', metadata={'source': 'Yankees', 'row': 2}, lookup_index=0), Document(page_content='Team: Giants\\n\"Payroll (millions)\": 117.62\\n\"Wins\": 94', lookup_str='', metadata={'source': 'Giants', 'row': 3}, lookup_index=0), Document(page_content='Team: Braves\\n\"Payroll (millions)\": 83.31\\n\"Wins\": 94', lookup_str='', metadata={'source': 'Braves', 'row': 4}, lookup_index=0), Document(page_content='Team: Athletics\\n\"Payroll (millions)\": 55.37\\n\"Wins\": 94', lookup_str='', metadata={'source': 'Athletics', 'row': 5}, lookup_index=0), Document(page_content='Team: Rangers\\n\"Payroll (millions)\": 120.51\\n\"Wins\": 93', lookup_str='', metadata={'source': 'Rangers', 'row': 6}, lookup_index=0), Document(page_content='Team: Orioles\\n\"Payroll (millions)\": 81.43\\n\"Wins\": 93', lookup_str='', metadata={'source': 'Orioles', 'row': 7}, lookup_index=0), Document(page_content='Team: Rays\\n\"Payroll (millions)\": 64.17\\n\"Wins\": 90', lookup_str='', metadata={'source': 'Rays', 'row': 8}, lookup_index=0), Document(page_content='Team: Angels\\n\"Payroll (millions)\": 154.49\\n\"Wins\": 89', lookup_str='', metadata={'source': 'Angels', 'row': 9}, lookup_index=0), Document(page_content='Team: Tigers\\n\"Payroll (millions)\": 132.30\\n\"Wins\": 88', lookup_str='', metadata={'source': 'Tigers', 'row': 10}, lookup_index=0), Document(page_content='Team: Cardinals\\n\"Payroll (millions)\": 110.30\\n\"Wins\": 88', lookup_str='', metadata={'source': 'Cardinals', 'row': 11}, lookup_index=0), Document(page_content='Team: Dodgers\\n\"Payroll (millions)\": 95.14\\n\"Wins\": 86', lookup_str='', metadata={'source': 'Dodgers', 'row': 12}, lookup_index=0), Document(page_content='Team: White Sox\\n\"Payroll (millions)\": 96.92\\n\"Wins\": 85', lookup_str='', metadata={'source': 'White Sox', 'row': 13}, lookup_index=0), Document(page_content='Team: Brewers\\n\"Payroll (millions)\": 97.65\\n\"Wins\": 83', lookup_str='', metadata={'source': 'Brewers', 'row': 14}, lookup_index=0), Document(page_content='Team: Phillies\\n\"Payroll (millions)\": 174.54\\n\"Wins\": 81', lookup_str='', metadata={'source': 'Phillies', 'row': 15}, lookup_index=0), Document(page_content='Team: Diamondbacks\\n\"Payroll (millions)\": 74.28\\n\"Wins\": 81', lookup_str='', metadata={'source': 'Diamondbacks', 'row': 16}, lookup_index=0), Document(page_content='Team: Pirates\\n\"Payroll (millions)\": 63.43\\n\"Wins\": 79', lookup_str='', metadata={'source': 'Pirates', 'row': 17}, lookup_index=0), Document(page_content='Team: Padres\\n\"Payroll (millions)\": 55.24\\n\"Wins\": 76', lookup_str='', metadata={'source': 'Padres', 'row': 18}, lookup_index=0), Document(page_content='Team: Mariners\\n\"Payroll (millions)\": 81.97\\n\"Wins\": 75', lookup_str='', metadata={'source': 'Mariners', 'row': 19}, lookup_index=0), Document(page_content='Team: Mets\\n\"Payroll (millions)\": 93.35\\n\"Wins\": 74', lookup_str='', metadata={'source': 'Mets', 'row': 20}, lookup_index=0), Document(page_content='Team: Blue Jays\\n\"Payroll (millions)\": 75.48\\n\"Wins\": 73', lookup_str='', metadata={'source': 'Blue Jays', 'row': 21}, lookup_index=0), Document(page_content='Team: Royals\\n\"Payroll (millions)\": 60.91\\n\"Wins\": 72', lookup_str='', metadata={'source': 'Royals', 'row': 22}, lookup_index=0), Document(page_content='Team: Marlins\\n\"Payroll (millions)\": 118.07\\n\"Wins\": 69', lookup_str='', metadata={'source': 'Marlins', 'row': 23}, lookup_index=0), Document(page_content='Team: Red Sox\\n\"Payroll (millions)\": 173.18\\n\"Wins\": 69', lookup_str='', metadata={'source': 'Red Sox', 'row': 24}, lookup_index=0), Document(page_content='Team: Indians\\n\"Payroll (millions)\": 78.43\\n\"Wins\": 68', lookup_str='', metadata={'source': 'Indians', 'row': 25}, lookup_index=0), Document(page_content='Team: Twins\\n\"Payroll (millions)\": 94.08\\n\"Wins\": 66', lookup_str='', metadata={'source': 'Twins', 'row': 26}, lookup_index=0), Document(page_content='Team: Rockies\\n\"Payroll (millions)\": 78.06\\n\"Wins\": 64', lookup_str='', metadata={'source': 'Rockies', 'row': 27}, lookup_index=0), Document(page_content='Team: Cubs\\n\"Payroll (millions)\": 88.19\\n\"Wins\": 61', lookup_str='', metadata={'source': 'Cubs', 'row': 28}, lookup_index=0), Document(page_content='Team: Astros\\n\"Payroll (millions)\": 60.65\\n\"Wins\": 55', lookup_str='', metadata={'source': 'Astros', 'row': 29}, lookup_index=0)] UnstructuredCSVLoader ​ You can also load the table using the UnstructuredCSVLoader . One\nadvantage of using UnstructuredCSVLoader is that if you use it in \"elements\" mode, an HTML representation of the table will be available\nin the metadata. from langchain . document_loaders . csv_loader import UnstructuredCSVLoader loader = UnstructuredCSVLoader ( file_path = \"example_data/mlb_teams_2012.csv\" , mode = \"elements\" ) docs = loader . load ( ) print ( docs [ 0 ] . metadata [ \"text_as_html\" ] ) <table border=\"1\" class=\"dataframe\"> <tbody> <tr> <td>Nationals</td> <td>81.34</td> <td>98</td> </tr> <tr> <td>Reds</td> <td>82.20</td> <td>97</td> </tr> <tr> <td>Yankees</td> <td>197.96</td> <td>95</td> </tr> <tr> <td>Giants</td> <td>117.62</td> <td>94</td> </tr> <tr> <td>Braves</td> <td>83.31</td> <td>94</td> </tr> <tr> <td>Athletics</td> <td>55.37</td> <td>94</td> </tr> <tr> <td>Rangers</td> <td>120.51</td> <td>93</td> </tr> <tr> <td>Orioles</td> <td>81.43</td> <td>93</td> </tr> <tr> <td>Rays</td> <td>64.17</td> <td>90</td> </tr> <tr> <td>Angels</td> <td>154.49</td> <td>89</td> </tr> <tr> <td>Tigers</td> <td>132.30</td> <td>88</td> </tr> <tr> <td>Cardinals</td> <td>110.30</td> <td>88</td> </tr> <tr> <td>Dodgers</td> <td>95.14</td> <td>86</td> </tr> <tr> <td>White Sox</td> <td>96.92</td> <td>85</td> </tr> <tr> <td>Brewers</td> <td>97.65</td> <td>83</td> </tr> <tr> <td>Phillies</td> <td>174.54</td> <td>81</td> </tr> <tr> <td>Diamondbacks</td> <td>74.28</td> <td>81</td> </tr> <tr> <td>Pirates</td> <td>63.43</td> <td>79</td> </tr> <tr> <td>Padres</td> <td>55.24</td> <td>76</td> </tr> <tr> <td>Mariners</td> <td>81.97</td> <td>75</td> </tr> <tr> <td>Mets</td> <td>93.35</td> <td>74</td> </tr> <tr> <td>Blue Jays</td> <td>75.48</td> <td>73</td> </tr> <tr> <td>Royals</td> <td>60.91</td> <td>72</td> </tr> <tr> <td>Marlins</td> <td>118.07</td> <td>69</td> </tr> <tr> <td>Red Sox</td> <td>173.18</td> <td>69</td> </tr> <tr> <td>Indians</td> <td>78.43</td> <td>68</td> </tr> <tr> <td>Twins</td> <td>94.08</td> <td>66</td> </tr> <tr> <td>Rockies</td> <td>78.06</td> <td>64</td> </tr> <tr> <td>Cubs</td> <td>88.19</td> <td>61</td> </tr> <tr> <td>Astros</td> <td>60.65</td> <td>55</td> </tr> </tbody> </table> Previous Couchbase Next Cube Semantic Layer"
    },
    {
        "title": "Pandas DataFrame | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/pandas_dataframe",
        "text_content": "Components Document loaders Pandas DataFrame Pandas DataFrame This notebook goes over how to load data from a pandas DataFrame. #!pip install pandas import pandas as pd df = pd . read_csv ( \"example_data/mlb_teams_2012.csv\" ) df . head ( ) Team \"Payroll (millions)\" \"Wins\" 0 Nationals 81.34 98 1 Reds 82.20 97 2 Yankees 197.96 95 3 Giants 117.62 94 4 Braves 83.31 94 from langchain . document_loaders import DataFrameLoader loader = DataFrameLoader ( df , page_content_column = \"Team\" ) loader . load ( ) [Document(page_content='Nationals', metadata={' \"Payroll (millions)\"': 81.34, ' \"Wins\"': 98}), Document(page_content='Reds', metadata={' \"Payroll (millions)\"': 82.2, ' \"Wins\"': 97}), Document(page_content='Yankees', metadata={' \"Payroll (millions)\"': 197.96, ' \"Wins\"': 95}), Document(page_content='Giants', metadata={' \"Payroll (millions)\"': 117.62, ' \"Wins\"': 94}), Document(page_content='Braves', metadata={' \"Payroll (millions)\"': 83.31, ' \"Wins\"': 94}), Document(page_content='Athletics', metadata={' \"Payroll (millions)\"': 55.37, ' \"Wins\"': 94}), Document(page_content='Rangers', metadata={' \"Payroll (millions)\"': 120.51, ' \"Wins\"': 93}), Document(page_content='Orioles', metadata={' \"Payroll (millions)\"': 81.43, ' \"Wins\"': 93}), Document(page_content='Rays', metadata={' \"Payroll (millions)\"': 64.17, ' \"Wins\"': 90}), Document(page_content='Angels', metadata={' \"Payroll (millions)\"': 154.49, ' \"Wins\"': 89}), Document(page_content='Tigers', metadata={' \"Payroll (millions)\"': 132.3, ' \"Wins\"': 88}), Document(page_content='Cardinals', metadata={' \"Payroll (millions)\"': 110.3, ' \"Wins\"': 88}), Document(page_content='Dodgers', metadata={' \"Payroll (millions)\"': 95.14, ' \"Wins\"': 86}), Document(page_content='White Sox', metadata={' \"Payroll (millions)\"': 96.92, ' \"Wins\"': 85}), Document(page_content='Brewers', metadata={' \"Payroll (millions)\"': 97.65, ' \"Wins\"': 83}), Document(page_content='Phillies', metadata={' \"Payroll (millions)\"': 174.54, ' \"Wins\"': 81}), Document(page_content='Diamondbacks', metadata={' \"Payroll (millions)\"': 74.28, ' \"Wins\"': 81}), Document(page_content='Pirates', metadata={' \"Payroll (millions)\"': 63.43, ' \"Wins\"': 79}), Document(page_content='Padres', metadata={' \"Payroll (millions)\"': 55.24, ' \"Wins\"': 76}), Document(page_content='Mariners', metadata={' \"Payroll (millions)\"': 81.97, ' \"Wins\"': 75}), Document(page_content='Mets', metadata={' \"Payroll (millions)\"': 93.35, ' \"Wins\"': 74}), Document(page_content='Blue Jays', metadata={' \"Payroll (millions)\"': 75.48, ' \"Wins\"': 73}), Document(page_content='Royals', metadata={' \"Payroll (millions)\"': 60.91, ' \"Wins\"': 72}), Document(page_content='Marlins', metadata={' \"Payroll (millions)\"': 118.07, ' \"Wins\"': 69}), Document(page_content='Red Sox', metadata={' \"Payroll (millions)\"': 173.18, ' \"Wins\"': 69}), Document(page_content='Indians', metadata={' \"Payroll (millions)\"': 78.43, ' \"Wins\"': 68}), Document(page_content='Twins', metadata={' \"Payroll (millions)\"': 94.08, ' \"Wins\"': 66}), Document(page_content='Rockies', metadata={' \"Payroll (millions)\"': 78.06, ' \"Wins\"': 64}), Document(page_content='Cubs', metadata={' \"Payroll (millions)\"': 88.19, ' \"Wins\"': 61}), Document(page_content='Astros', metadata={' \"Payroll (millions)\"': 60.65, ' \"Wins\"': 55})] # Use lazy load for larger table, which won't read the full table into memory for i in loader . lazy_load ( ) : print ( i ) page_content='Nationals' metadata={' \"Payroll (millions)\"': 81.34, ' \"Wins\"': 98} page_content='Reds' metadata={' \"Payroll (millions)\"': 82.2, ' \"Wins\"': 97} page_content='Yankees' metadata={' \"Payroll (millions)\"': 197.96, ' \"Wins\"': 95} page_content='Giants' metadata={' \"Payroll (millions)\"': 117.62, ' \"Wins\"': 94} page_content='Braves' metadata={' \"Payroll (millions)\"': 83.31, ' \"Wins\"': 94} page_content='Athletics' metadata={' \"Payroll (millions)\"': 55.37, ' \"Wins\"': 94} page_content='Rangers' metadata={' \"Payroll (millions)\"': 120.51, ' \"Wins\"': 93} page_content='Orioles' metadata={' \"Payroll (millions)\"': 81.43, ' \"Wins\"': 93} page_content='Rays' metadata={' \"Payroll (millions)\"': 64.17, ' \"Wins\"': 90} page_content='Angels' metadata={' \"Payroll (millions)\"': 154.49, ' \"Wins\"': 89} page_content='Tigers' metadata={' \"Payroll (millions)\"': 132.3, ' \"Wins\"': 88} page_content='Cardinals' metadata={' \"Payroll (millions)\"': 110.3, ' \"Wins\"': 88} page_content='Dodgers' metadata={' \"Payroll (millions)\"': 95.14, ' \"Wins\"': 86} page_content='White Sox' metadata={' \"Payroll (millions)\"': 96.92, ' \"Wins\"': 85} page_content='Brewers' metadata={' \"Payroll (millions)\"': 97.65, ' \"Wins\"': 83} page_content='Phillies' metadata={' \"Payroll (millions)\"': 174.54, ' \"Wins\"': 81} page_content='Diamondbacks' metadata={' \"Payroll (millions)\"': 74.28, ' \"Wins\"': 81} page_content='Pirates' metadata={' \"Payroll (millions)\"': 63.43, ' \"Wins\"': 79} page_content='Padres' metadata={' \"Payroll (millions)\"': 55.24, ' \"Wins\"': 76} page_content='Mariners' metadata={' \"Payroll (millions)\"': 81.97, ' \"Wins\"': 75} page_content='Mets' metadata={' \"Payroll (millions)\"': 93.35, ' \"Wins\"': 74} page_content='Blue Jays' metadata={' \"Payroll (millions)\"': 75.48, ' \"Wins\"': 73} page_content='Royals' metadata={' \"Payroll (millions)\"': 60.91, ' \"Wins\"': 72} page_content='Marlins' metadata={' \"Payroll (millions)\"': 118.07, ' \"Wins\"': 69} page_content='Red Sox' metadata={' \"Payroll (millions)\"': 173.18, ' \"Wins\"': 69} page_content='Indians' metadata={' \"Payroll (millions)\"': 78.43, ' \"Wins\"': 68} page_content='Twins' metadata={' \"Payroll (millions)\"': 94.08, ' \"Wins\"': 66} page_content='Rockies' metadata={' \"Payroll (millions)\"': 78.06, ' \"Wins\"': 64} page_content='Cubs' metadata={' \"Payroll (millions)\"': 88.19, ' \"Wins\"': 61} page_content='Astros' metadata={' \"Payroll (millions)\"': 60.65, ' \"Wins\"': 55} Previous Org-mode Next Polars DataFrame"
    },
    {
        "title": "Document loaders | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/",
        "text_content": ""
    },
    {
        "title": "SQL Database | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/toolkits/sql_database",
        "text_content": "Components Agents and toolkits SQL Database On this page SQL Database This notebook showcases an agent designed to interact with a SQL databases. The agent builds off of SQLDatabaseChain and is designed to answer more general questions about a database, as\nwell as recover from errors. Note that, as this agent is in active development, all answers might not\nbe correct. Additionally, it is not guaranteed that the agent won’t\nperform DML statements on your database given certain questions. Be\ncareful running it on sensitive data! This uses the example Chinook database. To set it up follow the\ninstructions on https://database.guide/2-sample-databases-sqlite/ ,\nplacing the .db file in a notebooks folder at the root of this\nrepository. Initialization ​ from langchain . agents import create_sql_agent from langchain . agents . agent_toolkits import SQLDatabaseToolkit from langchain . agents . agent_types import AgentType from langchain . llms . openai import OpenAI from langchain . sql_database import SQLDatabase db = SQLDatabase . from_uri ( \"sqlite:///../../../../../notebooks/Chinook.db\" ) toolkit = SQLDatabaseToolkit ( db = db , llm = OpenAI ( temperature = 0 ) ) Using ZERO_SHOT_REACT_DESCRIPTION ​ This shows how to initialize the agent using the\nZERO_SHOT_REACT_DESCRIPTION agent type. agent_executor = create_sql_agent ( llm = OpenAI ( temperature = 0 ) , toolkit = toolkit , verbose = True , agent_type = AgentType . ZERO_SHOT_REACT_DESCRIPTION , ) Using OpenAI Functions ​ This shows how to initialize the agent using the OPENAI_FUNCTIONS agent\ntype. Note that this is an alternative to the above. # agent_executor = create_sql_agent( #     llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\"), #     toolkit=toolkit, #     verbose=True, #     agent_type=AgentType.OPENAI_FUNCTIONS # ) Disclaimer ⚠️ ​ The query chain may generate insert/update/delete queries. When this is\nnot expected, use a custom prompt or create a SQL users without write\npermissions. The final user might overload your SQL database by asking a simple\nquestion such as “run the biggest query possible”. The generated query\nmight look like: SELECT * FROM \"public\" . \"users\" JOIN \"public\" . \"user_permissions\" ON \"public\" . \"users\" . id = \"public\" . \"user_permissions\" . user_id JOIN \"public\" . \"projects\" ON \"public\" . \"users\" . id = \"public\" . \"projects\" . user_id JOIN \"public\" . \"events\" ON \"public\" . \"projects\" . id = \"public\" . \"events\" . project_id ; For a transactional SQL database, if one of the table above contains\nmillions of rows, the query might cause trouble to other applications\nusing the same database. Most datawarehouse oriented databases support user-level quota, for\nlimiting resource usage. Example: describing a table ​ agent_executor . run ( \"Describe the playlisttrack table\" ) > Entering new  chain... Invoking: `list_tables_sql_db` with `{}` Album, Artist, Track, PlaylistTrack, InvoiceLine, sales_table, Playlist, Genre, Employee, Customer, Invoice, MediaType Invoking: `schema_sql_db` with `PlaylistTrack` CREATE TABLE \"PlaylistTrack\" ( \"PlaylistId\" INTEGER NOT NULL, \"TrackId\" INTEGER NOT NULL, PRIMARY KEY (\"PlaylistId\", \"TrackId\"), FOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), FOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\") ) /* 3 rows from PlaylistTrack table: PlaylistId  TrackId 1   3402 1   3389 1   3390 */The `PlaylistTrack` table has two columns: `PlaylistId` and `TrackId`. It is a junction table that represents the relationship between playlists and tracks. Here is the schema of the `PlaylistTrack` table: ``` CREATE TABLE \"PlaylistTrack\" ( \"PlaylistId\" INTEGER NOT NULL, \"TrackId\" INTEGER NOT NULL, PRIMARY KEY (\"PlaylistId\", \"TrackId\"), FOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), FOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\") ) ``` Here are three sample rows from the `PlaylistTrack` table: ``` PlaylistId   TrackId 1            3402 1            3389 1            3390 ``` Please let me know if there is anything else I can help you with. > Finished chain. 'The `PlaylistTrack` table has two columns: `PlaylistId` and `TrackId`. It is a junction table that represents the relationship between playlists and tracks. \\n\\nHere is the schema of the `PlaylistTrack` table:\\n\\n```\\nCREATE TABLE \"PlaylistTrack\" (\\n\\t\"PlaylistId\" INTEGER NOT NULL, \\n\\t\"TrackId\" INTEGER NOT NULL, \\n\\tPRIMARY KEY (\"PlaylistId\", \"TrackId\"), \\n\\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \\n\\tFOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\")\\n)\\n```\\n\\nHere are three sample rows from the `PlaylistTrack` table:\\n\\n```\\nPlaylistId   TrackId\\n1            3402\\n1            3389\\n1            3390\\n```\\n\\nPlease let me know if there is anything else I can help you with.' Example: describing a table, recovering from an error ​ In this example, the agent tries to search for a table that doesn’t\nexist, but finds the next best result agent_executor . run ( \"Describe the playlistsong table\" ) > Entering new AgentExecutor chain... Action: list_tables_sql_db Action Input: \"\" Observation: Genre, PlaylistTrack, MediaType, Invoice, InvoiceLine, Track, Playlist, Customer, Album, Employee, Artist Thought: I should look at the schema of the PlaylistSong table Action: schema_sql_db Action Input: \"PlaylistSong\" Observation: Error: table_names {'PlaylistSong'} not found in database Thought: I should check the spelling of the table Action: list_tables_sql_db Action Input: \"\" Observation: Genre, PlaylistTrack, MediaType, Invoice, InvoiceLine, Track, Playlist, Customer, Album, Employee, Artist Thought: The table is called PlaylistTrack Action: schema_sql_db Action Input: \"PlaylistTrack\" Observation: CREATE TABLE \"PlaylistTrack\" ( \"PlaylistId\" INTEGER NOT NULL, \"TrackId\" INTEGER NOT NULL, PRIMARY KEY (\"PlaylistId\", \"TrackId\"), FOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), FOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\") ) SELECT * FROM 'PlaylistTrack' LIMIT 3; PlaylistId TrackId 1 3402 1 3389 1 3390 Thought: I now know the final answer Final Answer: The PlaylistTrack table contains two columns, PlaylistId and TrackId, which are both integers and are used to link Playlist and Track tables. > Finished chain. 'The PlaylistTrack table contains two columns, PlaylistId and TrackId, which are both integers and are used to link Playlist and Track tables.' Example: running queries ​ agent_executor . run ( \"List the total sales per country. Which country's customers spent the most?\" ) > Entering new AgentExecutor chain... Action: list_tables_sql_db Action Input: \"\" Observation: Invoice, MediaType, Artist, InvoiceLine, Genre, Playlist, Employee, Album, PlaylistTrack, Track, Customer Thought: I should look at the schema of the relevant tables to see what columns I can use. Action: schema_sql_db Action Input: \"Invoice, Customer\" Observation: CREATE TABLE \"Customer\" ( \"CustomerId\" INTEGER NOT NULL, \"FirstName\" NVARCHAR(40) NOT NULL, \"LastName\" NVARCHAR(20) NOT NULL, \"Company\" NVARCHAR(80), \"Address\" NVARCHAR(70), \"City\" NVARCHAR(40), \"State\" NVARCHAR(40), \"Country\" NVARCHAR(40), \"PostalCode\" NVARCHAR(10), \"Phone\" NVARCHAR(24), \"Fax\" NVARCHAR(24), \"Email\" NVARCHAR(60) NOT NULL, \"SupportRepId\" INTEGER, PRIMARY KEY (\"CustomerId\"), FOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\") ) SELECT * FROM 'Customer' LIMIT 3; CustomerId FirstName LastName Company Address City State Country PostalCode Phone Fax Email SupportRepId 1 Luís Gonçalves Embraer - Empresa Brasileira de Aeronáutica S.A. Av. Brigadeiro Faria Lima, 2170 São José dos Campos SP Brazil 12227-000 +55 (12) 3923-5555 +55 (12) 3923-5566 luisg@embraer.com.br 3 2 Leonie Köhler None Theodor-Heuss-Straße 34 Stuttgart None Germany 70174 +49 0711 2842222 None leonekohler@surfeu.de 5 3 François Tremblay None 1498 rue Bélanger Montréal QC Canada H2G 1A7 +1 (514) 721-4711 None ftremblay@gmail.com 3 CREATE TABLE \"Invoice\" ( \"InvoiceId\" INTEGER NOT NULL, \"CustomerId\" INTEGER NOT NULL, \"InvoiceDate\" DATETIME NOT NULL, \"BillingAddress\" NVARCHAR(70), \"BillingCity\" NVARCHAR(40), \"BillingState\" NVARCHAR(40), \"BillingCountry\" NVARCHAR(40), \"BillingPostalCode\" NVARCHAR(10), \"Total\" NUMERIC(10, 2) NOT NULL, PRIMARY KEY (\"InvoiceId\"), FOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\") ) SELECT * FROM 'Invoice' LIMIT 3; InvoiceId CustomerId InvoiceDate BillingAddress BillingCity BillingState BillingCountry BillingPostalCode Total 1 2 2009-01-01 00:00:00 Theodor-Heuss-Straße 34 Stuttgart None Germany 70174 1.98 2 4 2009-01-02 00:00:00 Ullevålsveien 14 Oslo None Norway 0171 3.96 3 8 2009-01-03 00:00:00 Grétrystraat 63 Brussels None Belgium 1000 5.94 Thought: I should query the Invoice and Customer tables to get the total sales per country. Action: query_sql_db Action Input: SELECT c.Country, SUM(i.Total) AS TotalSales FROM Invoice i INNER JOIN Customer c ON i.CustomerId = c.CustomerId GROUP BY c.Country ORDER BY TotalSales DESC LIMIT 10 Observation: [('USA', 523.0600000000003), ('Canada', 303.9599999999999), ('France', 195.09999999999994), ('Brazil', 190.09999999999997), ('Germany', 156.48), ('United Kingdom', 112.85999999999999), ('Czech Republic', 90.24000000000001), ('Portugal', 77.23999999999998), ('India', 75.25999999999999), ('Chile', 46.62)] Thought: I now know the final answer Final Answer: The customers from the USA spent the most, with a total of $523.06. > Finished chain. 'The customers from the USA spent the most, with a total of $523.06.' agent_executor . run ( \"Show the total number of tracks in each playlist. The Playlist name should be included in the result.\" ) > Entering new AgentExecutor chain... Action: list_tables_sql_db Action Input: \"\" Observation: Invoice, MediaType, Artist, InvoiceLine, Genre, Playlist, Employee, Album, PlaylistTrack, Track, Customer Thought: I should look at the schema of the Playlist and PlaylistTrack tables to see what columns I can use. Action: schema_sql_db Action Input: \"Playlist, PlaylistTrack\" Observation: CREATE TABLE \"Playlist\" ( \"PlaylistId\" INTEGER NOT NULL, \"Name\" NVARCHAR(120), PRIMARY KEY (\"PlaylistId\") ) SELECT * FROM 'Playlist' LIMIT 3; PlaylistId Name 1 Music 2 Movies 3 TV Shows CREATE TABLE \"PlaylistTrack\" ( \"PlaylistId\" INTEGER NOT NULL, \"TrackId\" INTEGER NOT NULL, PRIMARY KEY (\"PlaylistId\", \"TrackId\"), FOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), FOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\") ) SELECT * FROM 'PlaylistTrack' LIMIT 3; PlaylistId TrackId 1 3402 1 3389 1 3390 Thought: I can use a SELECT statement to get the total number of tracks in each playlist. Action: query_checker_sql_db Action Input: SELECT Playlist.Name, COUNT(PlaylistTrack.TrackId) AS TotalTracks FROM Playlist INNER JOIN PlaylistTrack ON Playlist.PlaylistId = PlaylistTrack.PlaylistId GROUP BY Playlist.Name Observation: SELECT Playlist.Name, COUNT(PlaylistTrack.TrackId) AS TotalTracks FROM Playlist INNER JOIN PlaylistTrack ON Playlist.PlaylistId = PlaylistTrack.PlaylistId GROUP BY Playlist.Name Thought: The query looks correct, I can now execute it. Action: query_sql_db Action Input: SELECT Playlist.Name, COUNT(PlaylistTrack.TrackId) AS TotalTracks FROM Playlist INNER JOIN PlaylistTrack ON Playlist.PlaylistId = PlaylistTrack.PlaylistId GROUP BY Playlist.Name LIMIT 10 Observation: [('90’s Music', 1477), ('Brazilian Music', 39), ('Classical', 75), ('Classical 101 - Deep Cuts', 25), ('Classical 101 - Next Steps', 25), ('Classical 101 - The Basics', 25), ('Grunge', 15), ('Heavy Metal Classic', 26), ('Music', 6580), ('Music Videos', 1)] Thought: I now know the final answer. Final Answer: The total number of tracks in each playlist are: '90’s Music' (1477), 'Brazilian Music' (39), 'Classical' (75), 'Classical 101 - Deep Cuts' (25), 'Classical 101 - Next Steps' (25), 'Classical 101 - The Basics' (25), 'Grunge' (15), 'Heavy Metal Classic' (26), 'Music' (6580), 'Music Videos' (1). > Finished chain. \"The total number of tracks in each playlist are: '90’s Music' (1477), 'Brazilian Music' (39), 'Classical' (75), 'Classical 101 - Deep Cuts' (25), 'Classical 101 - Next Steps' (25), 'Classical 101 - The Basics' (25), 'Grunge' (15), 'Heavy Metal Classic' (26), 'Music' (6580), 'Music Videos' (1).\" Recovering from an error ​ In this example, the agent is able to recover from an error after\ninitially trying to access an attribute ( Track.ArtistId ) which doesn’t\nexist. agent_executor . run ( \"Who are the top 3 best selling artists?\" ) > Entering new AgentExecutor chain... Action: list_tables_sql_db Action Input: \"\" Observation: MediaType, Track, Invoice, Album, Playlist, Customer, Employee, InvoiceLine, PlaylistTrack, Genre, Artist Thought: I should look at the schema of the Artist, InvoiceLine, and Track tables to see what columns I can use. Action: schema_sql_db Action Input: \"Artist, InvoiceLine, Track\" Observation: CREATE TABLE \"Artist\" ( \"ArtistId\" INTEGER NOT NULL, \"Name\" NVARCHAR(120), PRIMARY KEY (\"ArtistId\") ) SELECT * FROM 'Artist' LIMIT 3; ArtistId Name 1 AC/DC 2 Accept 3 Aerosmith CREATE TABLE \"Track\" ( \"TrackId\" INTEGER NOT NULL, \"Name\" NVARCHAR(200) NOT NULL, \"AlbumId\" INTEGER, \"MediaTypeId\" INTEGER NOT NULL, \"GenreId\" INTEGER, \"Composer\" NVARCHAR(220), \"Milliseconds\" INTEGER NOT NULL, \"Bytes\" INTEGER, \"UnitPrice\" NUMERIC(10, 2) NOT NULL, PRIMARY KEY (\"TrackId\"), FOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), FOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), FOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\") ) SELECT * FROM 'Track' LIMIT 3; TrackId Name AlbumId MediaTypeId GenreId Composer Milliseconds Bytes UnitPrice 1 For Those About To Rock (We Salute You) 1 1 1 Angus Young, Malcolm Young, Brian Johnson 343719 11170334 0.99 2 Balls to the Wall 2 2 1 None 342562 5510424 0.99 3 Fast As a Shark 3 2 1 F. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman 230619 3990994 0.99 CREATE TABLE \"InvoiceLine\" ( \"InvoiceLineId\" INTEGER NOT NULL, \"InvoiceId\" INTEGER NOT NULL, \"TrackId\" INTEGER NOT NULL, \"UnitPrice\" NUMERIC(10, 2) NOT NULL, \"Quantity\" INTEGER NOT NULL, PRIMARY KEY (\"InvoiceLineId\"), FOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), FOREIGN KEY(\"InvoiceId\") REFERENCES \"Invoice\" (\"InvoiceId\") ) SELECT * FROM 'InvoiceLine' LIMIT 3; InvoiceLineId InvoiceId TrackId UnitPrice Quantity 1 1 2 0.99 1 2 1 4 0.99 1 3 2 6 0.99 1 Thought: I should query the database to get the top 3 best selling artists. Action: query_sql_db Action Input: SELECT Artist.Name, SUM(InvoiceLine.Quantity) AS TotalQuantity FROM Artist INNER JOIN Track ON Artist.ArtistId = Track.ArtistId INNER JOIN InvoiceLine ON Track.TrackId = InvoiceLine.TrackId GROUP BY Artist.Name ORDER BY TotalQuantity DESC LIMIT 3 Observation: Error: (sqlite3.OperationalError) no such column: Track.ArtistId [SQL: SELECT Artist.Name, SUM(InvoiceLine.Quantity) AS TotalQuantity FROM Artist INNER JOIN Track ON Artist.ArtistId = Track.ArtistId INNER JOIN InvoiceLine ON Track.TrackId = InvoiceLine.TrackId GROUP BY Artist.Name ORDER BY TotalQuantity DESC LIMIT 3] (Background on this error at: https://sqlalche.me/e/14/e3q8) Thought: I should double check my query before executing it. Action: query_checker_sql_db Action Input: SELECT Artist.Name, SUM(InvoiceLine.Quantity) AS TotalQuantity FROM Artist INNER JOIN Track ON Artist.ArtistId = Track.ArtistId INNER JOIN InvoiceLine ON Track.TrackId = InvoiceLine.TrackId GROUP BY Artist.Name ORDER BY TotalQuantity DESC LIMIT 3 Observation: SELECT Artist.Name, SUM(InvoiceLine.Quantity) AS TotalQuantity FROM Artist INNER JOIN Track ON Artist.ArtistId = Track.ArtistId INNER JOIN InvoiceLine ON Track.TrackId = InvoiceLine.TrackId GROUP BY Artist.Name ORDER BY TotalQuantity DESC LIMIT 3; Thought: I now know the final answer. Action: query_sql_db Action Input: SELECT Artist.Name, SUM(InvoiceLine.Quantity) AS TotalQuantity FROM Artist INNER JOIN Album ON Artist.ArtistId = Album.ArtistId INNER JOIN Track ON Album.AlbumId = Track.AlbumId INNER JOIN InvoiceLine ON Track.TrackId = InvoiceLine.TrackId GROUP BY Artist.Name ORDER BY TotalQuantity DESC LIMIT 3 Observation: [('Iron Maiden', 140), ('U2', 107), ('Metallica', 91)] Thought: I now know the final answer. Final Answer: The top 3 best selling artists are Iron Maiden, U2, and Metallica. > Finished chain. 'The top 3 best selling artists are Iron Maiden, U2, and Metallica.' Previous Spark SQL Next Steam Game Recommendation & Game Details"
    },
    {
        "title": "Pandas Dataframe | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/toolkits/pandas",
        "text_content": "Components Agents and toolkits Pandas Dataframe On this page Pandas Dataframe This notebook shows how to use agents to interact with a Pandas DataFrame . It is mostly optimized for question answering. NOTE: this agent calls the Python agent under the hood, which\nexecutes LLM generated Python code - this can be bad if the LLM\ngenerated Python code is harmful. Use cautiously. from langchain . agents . agent_types import AgentType from langchain . chat_models import ChatOpenAI from langchain_experimental . agents . agent_toolkits import create_pandas_dataframe_agent import pandas as pd from langchain . llms import OpenAI df = pd . read_csv ( \"titanic.csv\" ) Using ZERO_SHOT_REACT_DESCRIPTION ​ This shows how to initialize the agent using the ZERO_SHOT_REACT_DESCRIPTION agent type. Note that this is an\nalternative to the above. agent = create_pandas_dataframe_agent ( OpenAI ( temperature = 0 ) , df , verbose = True ) Using OpenAI Functions ​ This shows how to initialize the agent using the OPENAI_FUNCTIONS agent\ntype. Note that this is an alternative to the above. agent = create_pandas_dataframe_agent ( ChatOpenAI ( temperature = 0 , model = \"gpt-3.5-turbo-0613\" ) , df , verbose = True , agent_type = AgentType . OPENAI_FUNCTIONS , ) agent . run ( \"how many rows are there?\" ) > Entering new  chain... Invoking: `python_repl_ast` with `df.shape[0]` 891There are 891 rows in the dataframe. > Finished chain. 'There are 891 rows in the dataframe.' agent . run ( \"how many people have more than 3 siblings\" ) > Entering new AgentExecutor chain... Thought: I need to count the number of people with more than 3 siblings Action: python_repl_ast Action Input: df[df['SibSp'] > 3].shape[0] Observation: 30 Thought: I now know the final answer Final Answer: 30 people have more than 3 siblings. > Finished chain. '30 people have more than 3 siblings.' agent . run ( \"whats the square root of the average age?\" ) > Entering new AgentExecutor chain... Thought: I need to calculate the average age first Action: python_repl_ast Action Input: df['Age'].mean() Observation: 29.69911764705882 Thought: I now need to calculate the square root of the average age Action: python_repl_ast Action Input: math.sqrt(df['Age'].mean()) Observation: NameError(\"name 'math' is not defined\") Thought: I need to import the math library Action: python_repl_ast Action Input: import math Observation: Thought: I now need to calculate the square root of the average age Action: python_repl_ast Action Input: math.sqrt(df['Age'].mean()) Observation: 5.449689683556195 Thought: I now know the final answer Final Answer: The square root of the average age is 5.449689683556195. > Finished chain. 'The square root of the average age is 5.449689683556195.' Multi DataFrame Example ​ This next part shows how the agent can interact with multiple dataframes\npassed in as a list. df1 = df . copy ( ) df1 [ \"Age\" ] = df1 [ \"Age\" ] . fillna ( df1 [ \"Age\" ] . mean ( ) ) agent = create_pandas_dataframe_agent ( OpenAI ( temperature = 0 ) , [ df , df1 ] , verbose = True ) agent . run ( \"how many rows in the age column are different?\" ) > Entering new AgentExecutor chain... Thought: I need to compare the age columns in both dataframes Action: python_repl_ast Action Input: len(df1[df1['Age'] != df2['Age']]) Observation: 177 Thought: I now know the final answer Final Answer: 177 rows in the age column are different. > Finished chain. '177 rows in the age column are different.' Previous Natural Language APIs Next PlayWright Browser"
    },
    {
        "title": "CSV | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/toolkits/csv",
        "text_content": "Components Agents and toolkits CSV On this page CSV This notebook shows how to use agents to interact with data in CSV format. It is mostly optimized for question answering. NOTE: this agent calls the Pandas DataFrame agent under the hood,\nwhich in turn calls the Python agent, which executes LLM generated\nPython code - this can be bad if the LLM generated Python code is\nharmful. Use cautiously. from langchain . agents . agent_types import AgentType from langchain . chat_models import ChatOpenAI from langchain . llms import OpenAI from langchain_experimental . agents . agent_toolkits import create_csv_agent Using ZERO_SHOT_REACT_DESCRIPTION ​ This shows how to initialize the agent using the ZERO_SHOT_REACT_DESCRIPTION agent type. agent = create_csv_agent ( OpenAI ( temperature = 0 ) , \"titanic.csv\" , verbose = True , agent_type = AgentType . ZERO_SHOT_REACT_DESCRIPTION , ) Using OpenAI Functions ​ This shows how to initialize the agent using the OPENAI_FUNCTIONS agent\ntype. Note that this is an alternative to the above. agent = create_csv_agent ( ChatOpenAI ( temperature = 0 , model = \"gpt-3.5-turbo-0613\" ) , \"titanic.csv\" , verbose = True , agent_type = AgentType . OPENAI_FUNCTIONS , ) agent . run ( \"how many rows are there?\" ) Error in on_chain_start callback: 'name' Invoking: `python_repl_ast` with `df.shape[0]` 891There are 891 rows in the dataframe. > Finished chain. 'There are 891 rows in the dataframe.' agent . run ( \"how many people have more than 3 siblings\" ) Error in on_chain_start callback: 'name' Invoking: `python_repl_ast` with `df[df['SibSp'] > 3]['PassengerId'].count()` 30There are 30 people in the dataframe who have more than 3 siblings. > Finished chain. 'There are 30 people in the dataframe who have more than 3 siblings.' agent . run ( \"whats the square root of the average age?\" ) Error in on_chain_start callback: 'name' Invoking: `python_repl_ast` with `import pandas as pd import math # Create a dataframe data = {'Age': [22, 38, 26, 35, 35]} df = pd.DataFrame(data) # Calculate the average age average_age = df['Age'].mean() # Calculate the square root of the average age square_root = math.sqrt(average_age) square_root` 5.585696017507576The square root of the average age is approximately 5.59. > Finished chain. 'The square root of the average age is approximately 5.59.' Multi CSV Example ​ This next part shows how the agent can interact with multiple csv files\npassed in as a list. agent = create_csv_agent ( ChatOpenAI ( temperature = 0 , model = \"gpt-3.5-turbo-0613\" ) , [ \"titanic.csv\" , \"titanic_age_fillna.csv\" ] , verbose = True , agent_type = AgentType . OPENAI_FUNCTIONS , ) agent . run ( \"how many rows in the age column are different between the two dfs?\" ) Error in on_chain_start callback: 'name' Invoking: `python_repl_ast` with `df1['Age'].nunique() - df2['Age'].nunique()` -1There is 1 row in the age column that is different between the two dataframes. > Finished chain. 'There is 1 row in the age column that is different between the two dataframes.' Previous ClickUp Next Document Comparison"
    },
    {
        "title": "Anthropic | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/platforms/anthropic",
        "text_content": "Providers Anthropic On this page Anthropic All functionality related to Anthropic models. Anthropic is an AI safety and research company, and is the creator of Claude.\nThis page covers all integrations between Anthropic models and LangChain. Prompting Overview ​ Claude is chat-based model, meaning it is trained on conversation data.\nHowever, it is a text based API, meaning it takes in single string.\nIt expects this string to be in a particular format.\nThis means that it is up the user to ensure that is the case.\nLangChain provides several utilities and helper functions to make sure prompts that you write -\nwhether formatted as a string or as a list of messages - end up formatted correctly. Specifically, Claude is trained to fill in text for the Assistant role as part of an ongoing dialogue\nbetween a human user ( Human: ) and an AI assistant ( Assistant: ). Prompts sent via the API must contain \\n\\nHuman: and \\n\\nAssistant: as the signals of who's speaking.\nThe final turn must always be \\n\\nAssistant: - the input string cannot have \\n\\nHuman: as the final role. Because Claude is chat-based but accepts a string as input, it can be treated as either a LangChain ChatModel or LLM .\nThis means there are two wrappers in LangChain - ChatAnthropic and Anthropic .\nIt is generally recommended to use the ChatAnthropic wrapper, and format your prompts as ChatMessage s (we will show examples of this below).\nThis is because it keeps your prompt in a general format that you can easily then also use with other models (should you want to).\nHowever, if you want more fine-grained control over the prompt, you can use the Anthropic wrapper - we will show and example of this as well.\nThe Anthropic wrapper however is deprecated, as all functionality can be achieved in a more generic way using ChatAnthropic . Prompting Best Practices ​ Anthropic models have several prompting best practices compared to OpenAI models. No System Messages Anthropic models are not trained on the concept of a \"system message\".\nWe have worked with the Anthropic team to handle them somewhat appropriately (a Human message with an admin tag)\nbut this is largely a hack and it is recommended that you do not use system messages. AI Messages Can Continue A completion from Claude is a continuation of the last text in the string which allows you further control over Claude's output.\nFor example, putting words in Claude's mouth in a prompt like this: \\n\\nHuman: Tell me a joke about bears\\n\\nAssistant: What do you call a bear with no teeth? This will return a completion like this A gummy bear! instead of a whole new assistant message with a different random bear joke. ChatAnthropic ​ ChatAnthropic is a subclass of LangChain's ChatModel , meaning it works best with ChatPromptTemplate .\nYou can import this wrapper with the following code: from langchain.chat_models import ChatAnthropic model = ChatAnthropic() When working with ChatModels, it is preferred that you design your prompts as ChatPromptTemplate s.\nHere is an example below of doing that: from langchain.prompts import ChatPromptTemplate prompt = ChatPromptTemplate.from_messages([ (\"system\", \"You are a helpful chatbot\"), (\"human\", \"Tell me a joke about {topic}\"), ]) You can then use this in a chain as follows: chain = prompt | model chain.invoke({\"topic\": \"bears\"}) How is the prompt actually being formatted under the hood? We can see that by running the following code prompt_value = prompt.format_prompt(topic=\"bears\") model.convert_prompt(prompt_value) This produces the following formatted string: '\\n\\nYou are a helpful chatbot\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant:' We can see that under the hood LangChain is not appending any prefix/suffix to SystemMessage 's. This is because Anthropic has no concept of SystemMessage .\nAnthropic requires all prompts to end with assistant messages. This means if the last message is not an assistant message, the suffix Assistant: will automatically be inserted. If you decide instead to use a normal PromptTemplate (one that just works on a single string) let's take a look at\nwhat happens: from langchain.prompts import PromptTemplate prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\") prompt_value = prompt.format_prompt(topic=\"bears\") model.convert_prompt(prompt_value) This produces the following formatted string: '\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant:' We can see that it automatically adds the Human and Assistant tags.\nWhat is happening under the hood?\nFirst: the string gets converted to a single human message. This happens generically (because we are using a subclass of ChatModel ).\nThen, similarly to the above example, an empty Assistant message is getting appended.\nThis is Anthropic specific. [Deprecated] Anthropic ​ This Anthropic wrapper is subclassed from LLM .\nWe can import it with: from langchain.llms import Anthropic model = Anthropic() This model class is designed to work with normal PromptTemplates. An example of that is below: prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\") chain = prompt | model chain.invoke({\"topic\": \"bears\"}) Let's see what is going on with the prompt templating under the hood! prompt_value = prompt.format_prompt(topic=\"bears\") model.convert_prompt(prompt_value) This outputs the following '\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant: Sure, here you go:\\n' Notice that it adds the Human tag at the start of the string, and then finishes it with \\n\\nAssistant: Sure, here you go: .\nThe extra Sure, here you go was added on purpose by the Anthropic team. What happens if we have those symbols in the prompt directly? prompt = PromptTemplate.from_template(\"Human: Tell me a joke about {topic}\") prompt_value = prompt.format_prompt(topic=\"bears\") model.convert_prompt(prompt_value) This outputs: '\\n\\nHuman: Tell me a joke about bears' We can see that we detect that the user is trying to use the special tokens, and so we don't do any formatting. ChatAnthropicMessages (Beta) ​ ChatAnthropicMessages uses the beta release of Anthropic's new Messages API. You can use it from the langchain-anthropic package, which you can install with pip install langchain-anthropic . For more information, see the ChatAnthropicMessages docs Previous Providers Next AWS"
    },
    {
        "title": "AWS | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/platforms/aws",
        "text_content": "Providers AWS On this page AWS The LangChain integrations related to Amazon AWS platform. LLMs ​ Bedrock ​ Amazon Bedrock is a fully managed service that offers a choice of\nhigh-performing foundation models (FMs) from leading AI companies like AI21 Labs , Anthropic , Cohere , Meta , Stability AI , and Amazon via a single API, along with a broad set of capabilities you need to\nbuild generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock ,\nyou can easily experiment with and evaluate top FMs for your use case, privately customize them with\nyour data using techniques such as fine-tuning and Retrieval Augmented Generation ( RAG ), and build\nagents that execute tasks using your enterprise systems and data sources. Since Amazon Bedrock is\nserverless, you don't have to manage any infrastructure, and you can securely integrate and deploy\ngenerative AI capabilities into your applications using the AWS services you are already familiar with. See a usage example . from langchain . llms . bedrock import Bedrock Amazon API Gateway ​ Amazon API Gateway is a fully managed service that makes it easy for\ndevelopers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\"\nfor applications to access data, business logic, or functionality from your backend services. Using API Gateway , you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication\napplications. API Gateway supports containerized and serverless workloads, as well as web applications. API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of\nconcurrent API calls, including traffic management, CORS support, authorization and access control,\nthrottling, monitoring, and API version management. API Gateway has no minimum fees or startup costs.\nYou pay for the API calls you receive and the amount of data transferred out and, with the API Gateway tiered pricing model, you can reduce your cost as your API usage scales. See a usage example . from langchain . llms import AmazonAPIGateway SageMaker Endpoint ​ Amazon SageMaker is a system that can build, train, and deploy\nmachine learning (ML) models with fully managed infrastructure, tools, and workflows. We use SageMaker to host our model and expose it as the SageMaker Endpoint . See a usage example . from langchain . llms import SagemakerEndpoint from langchain . llms . sagemaker_endpoint import LLMContentHandler Chat models ​ Bedrock Chat ​ See a usage example . from langchain . chat_models import BedrockChat Text Embedding Models ​ Bedrock ​ See a usage example . from langchain . embeddings import BedrockEmbeddings SageMaker Endpoint ​ See a usage example . from langchain . embeddings import SagemakerEndpointEmbeddings from langchain . llms . sagemaker_endpoint import ContentHandlerBase Chains ​ Amazon Comprehend Moderation Chain ​ Amazon Comprehend is a natural-language processing (NLP) service that\nuses machine learning to uncover valuable insights and connections in text. We need to install the boto3 and nltk libraries. pip install boto3 nltk See a usage example . from langchain_experimental . comprehend_moderation import AmazonComprehendModerationChain Document loaders ​ AWS S3 Directory and File ​ Amazon Simple Storage Service (Amazon S3) is an object storage service. AWS S3 Directory AWS S3 Buckets See a usage example for S3DirectoryLoader . See a usage example for S3FileLoader . from langchain . document_loaders import S3DirectoryLoader , S3FileLoader Amazon Textract ​ Amazon Textract is a machine\nlearning (ML) service that automatically extracts text, handwriting, and data from scanned documents. See a usage example . from langchain . document_loaders import AmazonTextractPDFLoader Memory ​ AWS DynamoDB ​ AWS DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. We have to configure the AWS CLI . We need to install the boto3 library. pip install boto3 See a usage example . from langchain . memory import DynamoDBChatMessageHistory Retrievers ​ Amazon Kendra ​ Amazon Kendra is an intelligent search service\nprovided by Amazon Web Services ( AWS ). It utilizes advanced natural language processing (NLP) and machine\nlearning algorithms to enable powerful search capabilities across various data sources within an organization. Kendra is designed to help users find the information they need quickly and accurately,\nimproving productivity and decision-making. With Kendra , we can search across a wide range of content types, including documents, FAQs, knowledge bases,\nmanuals, and websites. It supports multiple languages and can understand complex queries, synonyms, and\ncontextual meanings to provide highly relevant search results. We need to install the boto3 library. pip install boto3 See a usage example . from langchain . retrievers import AmazonKendraRetriever Amazon Bedrock (Knowledge Bases) ​ Knowledge bases for Amazon Bedrock is an Amazon Web Services ( AWS ) offering which lets you quickly build RAG applications by using your\nprivate data to customize foundation model response. We need to install the boto3 library. pip install boto3 See a usage example . from langchain . retrievers import AmazonKnowledgeBasesRetriever Vector stores ​ Amazon OpenSearch Service ​ Amazon OpenSearch Service performs\ninteractive log analytics, real-time application monitoring, website search, and more. OpenSearch is\nan open source,\ndistributed search and analytics suite derived from Elasticsearch . Amazon OpenSearch Service offers the\nlatest versions of OpenSearch , support for many versions of Elasticsearch , as well as\nvisualization capabilities powered by OpenSearch Dashboards and Kibana . We need to install several python libraries. pip install boto3 requests requests-aws4auth See a usage example . from langchain . vectorstores import OpenSearchVectorSearch Tools ​ AWS Lambda ​ Amazon AWS Lambda is a serverless computing service provided by Amazon Web Services ( AWS ). It helps developers to build and run applications and services without\nprovisioning or managing servers. This serverless architecture enables you to focus on writing and\ndeploying code, while AWS automatically takes care of scaling, patching, and managing the\ninfrastructure required to run your applications. We need to install boto3 python library. pip install boto3 See a usage example . Callbacks ​ SageMaker Tracking ​ Amazon SageMaker is a fully managed service that is used to quickly\nand easily build, train and deploy machine learning (ML) models. Amazon SageMaker Experiments is a capability\nof Amazon SageMaker that lets you organize, track,\ncompare and evaluate ML experiments and model versions. We need to install several python libraries. pip install google-search-results sagemaker See a usage example . from langchain . callbacks import SageMakerCallbackHandler Previous Anthropic Next Google"
    },
    {
        "title": "Google | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/platforms/google",
        "text_content": "Providers Google On this page Google All functionality related to Google Cloud Platform and other Google products. Chat models ​ Google AI ​ Access GoogleAI Gemini models such as gemini-pro and gemini-pro-vision through the ChatGoogleGenerativeAI class. pip install -U langchain-google-genai Configure your API key. export GOOGLE_API_KEY = your-api-key from langchain_google_genai import ChatGoogleGenerativeAI llm = ChatGoogleGenerativeAI ( model = \"gemini-pro\" ) llm . invoke ( \"Sing a ballad of LangChain.\" ) Gemini vision model supports image inputs when providing a single chat message. Example: from langchain_core . messages import HumanMessage from langchain_google_genai import ChatGoogleGenerativeAI llm = ChatGoogleGenerativeAI ( model = \"gemini-pro-vision\" ) # example message = HumanMessage ( content = [ { \"type\" : \"text\" , \"text\" : \"What's in this image?\" , } , # You can optionally provide text parts { \"type\" : \"image_url\" , \"image_url\" : \"https://picsum.photos/seed/picsum/200/300\" } , ] ) llm . invoke ( [ message ] ) The value of image_url can be any of the following: A public image URL A gcs file (e.g., \"gcs://path/to/file.png\") A local file path A base64 encoded image (e.g., data:image/png;base64,abcd124) A PIL image Vertex AI ​ Access PaLM chat models like chat-bison and codechat-bison via Google Cloud. We need to install google-cloud-aiplatform python package. pip install google-cloud-aiplatform See a usage example . from langchain . chat_models import ChatVertexAI Document Loaders ​ Google BigQuery ​ Google BigQuery is a serverless and cost-effective enterprise data warehouse that works across clouds and scales with your data. BigQuery is a part of the Google Cloud Platform . We need to install google-cloud-bigquery python package. pip install google-cloud-bigquery See a usage example . from langchain . document_loaders import BigQueryLoader LLMs ​ Vertex AI ​ Access to Gemini and PaLM LLMs (like text-bison and code-bison ) via Google Vertex AI . We need to install google-cloud-aiplatform python package. pip install google-cloud-aiplatform See a usage example . from langchain . llms import VertexAI Model Garden ​ Access PaLM and hundreds of OSS models via Vertex AI Model Garden . We need to install google-cloud-aiplatform python package. pip install google-cloud-aiplatform See a usage example . from langchain . llms import VertexAIModelGarden Google Cloud Storage ​ Google Cloud Storage is a managed service for storing unstructured data. We need to install google-cloud-storage python package. pip install google-cloud-storage There are two loaders for the Google Cloud Storage : the Directory and the File loaders. See a usage example . from langchain . document_loaders import GCSDirectoryLoader See a usage example . from langchain . document_loaders import GCSFileLoader Google Drive ​ Google Drive is a file storage and synchronization service developed by Google. Currently, only Google Docs are supported. We need to install several python packages. pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib See a usage example and authorization instructions . from langchain . document_loaders import GoogleDriveLoader Speech-to-Text ​ Google Cloud Speech-to-Text is an audio transcription API powered by Google's speech recognition models. This document loader transcribes audio files and outputs the text results as Documents. First, we need to install the python package. pip install google-cloud-speech See a usage example and authorization instructions . from langchain . document_loaders import GoogleSpeechToTextLoader Vector Stores ​ Google Vertex AI Vector Search ​ Google Vertex AI Vector Search ,\nformerly known as Vertex AI Matching Engine , provides the industry's leading high-scale\nlow latency vector database. These vector databases are commonly\nreferred to as vector similarity-matching or an approximate nearest neighbor (ANN) service. We need to install several python packages. pip install tensorflow google-cloud-aiplatform tensorflow-hub tensorflow-text See a usage example . from langchain . vectorstores import MatchingEngine Google ScaNN ​ Google ScaNN (Scalable Nearest Neighbors) is a python package. ScaNN is a method for efficient vector similarity search at scale. ScaNN includes search space pruning and quantization for Maximum Inner\nProduct Search and also supports other distance functions such as\nEuclidean distance. The implementation is optimized for x86 processors\nwith AVX2 support. See its Google Research github for more details. We need to install scann python package. pip install scann See a usage example . from langchain . vectorstores import ScaNN Retrievers ​ Google Drive ​ We need to install several python packages. pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib See a usage example and authorization instructions . from langchain_googledrive . retrievers import GoogleDriveRetriever Vertex AI Search ​ Google Cloud Vertex AI Search allows developers to quickly build generative AI powered search engines for customers and employees. We need to install the google-cloud-discoveryengine python package. pip install google-cloud-discoveryengine See a usage example . from langchain . retrievers import GoogleVertexAISearchRetriever Document AI Warehouse ​ Google Cloud Document AI Warehouse allows enterprises to search, store, govern, and manage documents and their AI-extracted\ndata and metadata in a single platform. from langchain . retrievers import GoogleDocumentAIWarehouseRetriever docai_wh_retriever = GoogleDocumentAIWarehouseRetriever ( project_number = . . . ) query = . . . documents = docai_wh_retriever . get_relevant_documents ( query , user_ldap = . . . ) Tools ​ Google Cloud Text-to-Speech ​ Google Cloud Text-to-Speech enables developers to\nsynthesize natural-sounding speech with 100+ voices, available in multiple languages and variants.\nIt applies DeepMind’s groundbreaking research in WaveNet and Google’s powerful neural networks\nto deliver the highest fidelity possible. We need to install a python package. pip install google-cloud-text-to-speech See a usage example and authorization instructions . from langchain . tools import GoogleCloudTextToSpeechTool Google Drive ​ We need to install several python packages. pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib See a usage example and authorization instructions . from langchain . utilities . google_drive import GoogleDriveAPIWrapper from langchain . tools . google_drive . tool import GoogleDriveSearchTool Google Places ​ We need to install a python package. pip install googlemaps See a usage example and authorization instructions . from langchain . tools import GooglePlacesTool Google Search ​ Set up a Custom Search Engine, following these instructions Get an API Key and Custom Search Engine ID from the previous step, and set them as environment variables GOOGLE_API_KEY and GOOGLE_CSE_ID respectively. from langchain . utilities import GoogleSearchAPIWrapper For a more detailed walkthrough of this wrapper, see this notebook . We can easily load this wrapper as a Tool (to use with an Agent). We can do this with: from langchain . agents import load_tools tools = load_tools ( [ \"google-search\" ] ) Google Finance ​ We need to install a python package. pip install google-search-results See a usage example and authorization instructions . from langchain . tools . google_finance import GoogleFinanceQueryRun from langchain . utilities . google_finance import GoogleFinanceAPIWrapper Google Jobs ​ We need to install a python package. pip install google-search-results See a usage example and authorization instructions . from langchain . tools . google_jobs import GoogleJobsQueryRun from langchain . utilities . google_finance import GoogleFinanceAPIWrapper Google Lens ​ See a usage example and authorization instructions . from langchain . tools . google_lens import GoogleLensQueryRun from langchain . utilities . google_lens import GoogleLensAPIWrapper Google Scholar ​ We need to install a python package. pip install google-search-results See a usage example and authorization instructions . from langchain . tools . google_scholar import GoogleScholarQueryRun from langchain . utilities . google_scholar import GoogleScholarAPIWrapper Google Trends ​ We need to install a python package. pip install google-search-results See a usage example and authorization instructions . from langchain . tools . google_trends import GoogleTrendsQueryRun from langchain . utilities . google_trends import GoogleTrendsAPIWrapper Document Transformers ​ Google Document AI ​ Document AI is a Google Cloud Platform service that transforms unstructured data from documents into structured data, making it easier\nto understand, analyze, and consume. We need to set up a GCS bucket and create your own OCR processor The GCS_OUTPUT_PATH should be a path to a folder on GCS (starting with gs:// )\nand a processor name should look like projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID .\nWe can get it either programmatically or copy from the Prediction endpoint section of the Processor details tab in the Google Cloud Console. pip install google-cloud-documentai pip install google-cloud-documentai-toolbox See a usage example . from langchain . document_loaders . blob_loaders import Blob from langchain . document_loaders . parsers import DocAIParser Google Translate ​ Google Translate is a multilingual neural machine\ntranslation service developed by Google to translate text, documents and websites\nfrom one language into another. The GoogleTranslateTransformer allows you to translate text and HTML with the Google Cloud Translation API . To use it, you should have the google-cloud-translate python package installed, and a Google Cloud project with the Translation API enabled . This transformer uses the Advanced edition (v3) . First, we need to install the python package. pip install google-cloud-translate See a usage example and authorization instructions . from langchain . document_transformers import GoogleTranslateTransformer Toolkits ​ GMail ​ Gmail is a free email service provided by Google.\nThis toolkit works with emails through the Gmail API . We need to install several python packages. pip install google-api-python-client google-auth-oauthlib google-auth-httplib2 See a usage example and authorization instructions . from langchain . agents . agent_toolkits import GmailToolkit Chat Loaders ​ GMail ​ Gmail is a free email service provided by Google.\nThis loader works with emails through the Gmail API . We need to install several python packages. pip install google-api-python-client google-auth-oauthlib google-auth-httplib2 See a usage example and authorization instructions . from langchain . chat_loaders . gmail import GMailLoader 3rd Party Integrations ​ SearchApi ​ SearchApi provides a 3rd-party API to access Google search results, YouTube search & transcripts, and other Google-related engines. See usage examples and authorization instructions . from langchain . utilities import SearchApiAPIWrapper SerpAPI ​ SerpApi provides a 3rd-party API to access Google search results. See a usage example and authorization instructions . from langchain . utilities import SerpAPIWrapper Serper.dev ​ See a usage example and authorization instructions . from langchain . utilities import GoogleSerperAPIWrapper YouTube ​ YouTube Search package searches YouTube videos avoiding using their heavily rate-limited API. It uses the form on the YouTube homepage and scrapes the resulting page. We need to install a python package. pip install youtube_search See a usage example . from langchain . tools import YouTubeSearchTool YouTube audio ​ YouTube is an online video sharing and social media platform created by Google . Use YoutubeAudioLoader to fetch / download the audio files. Then, use OpenAIWhisperParser to transcribe them to text. We need to install several python packages. pip install yt_dlp pydub librosa See a usage example and authorization instructions . from langchain . document_loaders . blob_loaders . youtube_audio import YoutubeAudioLoader from langchain . document_loaders . parsers import OpenAIWhisperParser , OpenAIWhisperParserLocal YouTube transcripts ​ YouTube is an online video sharing and social media platform created by Google . We need to install youtube-transcript-api python package. pip install youtube-transcript-api See a usage example . from langchain . document_loaders import YoutubeLoader Previous AWS Next Hugging Face"
    },
    {
        "title": "Hugging Face | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/platforms/huggingface",
        "text_content": "Providers Hugging Face On this page Hugging Face All functionality related to the Hugging Face Platform . LLMs ​ Hugging Face Hub ​ The Hugging Face Hub is a platform\nwith over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source\nand publicly available, in an online platform where people can easily\ncollaborate and build ML together. The Hub works as a central place where anyone\ncan explore, experiment, collaborate, and build technology with Machine Learning. To use, we should have the huggingface_hub python package installed . pip install huggingface_hub See a usage example . from langchain . llms import HuggingFaceHub Hugging Face Local Pipelines ​ Hugging Face models can be run locally through the HuggingFacePipeline class. We need to install transformers python package. pip install transformers See a usage example . from langchain . llms . huggingface_pipeline import HuggingFacePipeline Hugging Face TextGen Inference ​ Text Generation Inference is\na Rust, Python and gRPC server for text generation inference. Used in production at HuggingFace to power LLMs api-inference widgets. We need to install text_generation python package. pip install text_generation See a usage example . from langchain . llms import HuggingFaceTextGenInference Document Loaders ​ Hugging Face dataset ​ Hugging Face Hub is home to over 75,000 datasets in more than 100 languages\nthat can be used for a broad range of tasks across NLP, Computer Vision, and Audio.\nThey used for a diverse range of tasks such as translation, automatic speech\nrecognition, and image classification. We need to install datasets python package. pip install datasets See a usage example . from langchain . document_loaders . hugging_face_dataset import HuggingFaceDatasetLoader Embedding Models ​ Hugging Face Hub ​ The Hugging Face Hub is a platform\nwith over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source\nand publicly available, in an online platform where people can easily\ncollaborate and build ML together. The Hub works as a central place where anyone\ncan explore, experiment, collaborate, and build technology with Machine Learning. We need to install the sentence_transformers python package. pip install sentence_transformers HuggingFaceEmbeddings ​ See a usage example . from langchain . embeddings import HuggingFaceEmbeddings HuggingFaceInstructEmbeddings ​ See a usage example . from langchain . embeddings import HuggingFaceInstructEmbeddings HuggingFaceBgeEmbeddings ​ BGE models on the HuggingFace are the best open-source embedding models .\nBGE model is created by the Beijing Academy of Artificial Intelligence (BAAI) . BAAI is a private non-profit organization engaged in AI research and development. See a usage example . from langchain . embeddings import HuggingFaceBgeEmbeddings Tools ​ Hugging Face Hub Tools ​ Hugging Face Tools support text I/O and are loaded using the load_huggingface_tool function. We need to install several python packages. pip install transformers huggingface_hub See a usage example . from langchain . agents import load_huggingface_tool Previous Google Next Microsoft"
    },
    {
        "title": "Microsoft | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/platforms/microsoft",
        "text_content": "Providers Microsoft On this page Microsoft All functionality related to Microsoft Azure and other Microsoft products. Chat Models ​ Azure OpenAI ​ Microsoft Azure , often referred to as Azure is a cloud computing platform run by Microsoft , which offers access, management, and development of applications and services through global data centers. It provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems. Azure OpenAI is an Azure service with powerful language models from OpenAI including the GPT-3 , Codex and Embeddings model series for content generation, summarization, semantic search, and natural language to code translation. pip install openai tiktoken Set the environment variables to get access to the Azure OpenAI service. import os os . environ [ \"AZURE_OPENAI_ENDPOINT\" ] = \"https://<your-endpoint.openai.azure.com/\" os . environ [ \"AZURE_OPENAI_API_KEY\" ] = \"your AzureOpenAI key\" See a usage example from langchain . chat_models import AzureChatOpenAI Text Embedding Models ​ Azure OpenAI ​ See a usage example from langchain . embeddings import AzureOpenAIEmbeddings LLMs ​ Azure OpenAI ​ See a usage example . from langchain . llms import AzureOpenAI Document loaders ​ Azure Blob Storage ​ Azure Blob Storage is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data. Azure Files offers fully managed\nfile shares in the cloud that are accessible via the industry standard Server Message Block ( SMB ) protocol,\nNetwork File System ( NFS ) protocol, and Azure Files REST API . Azure Files are based on the Azure Blob Storage . Azure Blob Storage is designed for: Serving images or documents directly to a browser. Storing files for distributed access. Streaming video and audio. Writing to log files. Storing data for backup and restore, disaster recovery, and archiving. Storing data for analysis by an on-premises or Azure-hosted service. pip install azure-storage-blob See a usage example for the Azure Blob Storage . from langchain . document_loaders import AzureBlobStorageContainerLoader See a usage example for the Azure Files . from langchain . document_loaders import AzureBlobStorageFileLoader Microsoft OneDrive ​ Microsoft OneDrive (formerly SkyDrive ) is a file-hosting service operated by Microsoft. First, you need to install a python package. pip install o365 See a usage example . from langchain . document_loaders import OneDriveLoader Microsoft Word ​ Microsoft Word is a word processor developed by Microsoft. See a usage example . from langchain . document_loaders import UnstructuredWordDocumentLoader Microsoft Excel ​ Microsoft Excel is a spreadsheet editor developed by\nMicrosoft for Windows, macOS, Android, iOS and iPadOS.\nIt features calculation or computation capabilities, graphing tools, pivot tables, and a macro programming\nlanguage called Visual Basic for Applications (VBA). Excel forms part of the Microsoft 365 suite of software. The UnstructuredExcelLoader is used to load Microsoft Excel files. The loader works with both .xlsx and .xls files.\nThe page content will be the raw text of the Excel file. If you use the loader in \"elements\" mode, an HTML\nrepresentation of the Excel file will be available in the document metadata under the text_as_html key. See a usage example . from langchain . document_loaders import UnstructuredExcelLoader Microsoft SharePoint ​ Microsoft SharePoint is a website-based collaboration system\nthat uses workflow applications, “list” databases, and other web parts and security features to\nempower business teams to work together developed by Microsoft. See a usage example . from langchain . document_loaders . sharepoint import SharePointLoader Microsoft PowerPoint ​ Microsoft PowerPoint is a presentation program by Microsoft. See a usage example . from langchain . document_loaders import UnstructuredPowerPointLoader Microsoft OneNote ​ First, let's install dependencies: pip install bs4 msal See a usage example . from langchain . document_loaders . onenote import OneNoteLoader Vector stores ​ Azure Cosmos DB ​ Azure Cosmos DB for MongoDB vCore makes it easy to create a database with full native MongoDB support.\nYou can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore account's connection string.\nUse vector search in Azure Cosmos DB for MongoDB vCore to seamlessly integrate your AI-based applications with your data that's stored in Azure Cosmos DB. Installation and Setup ​ See detail configuration instructions . We need to install pymongo python package. pip install pymongo Deploy Azure Cosmos DB on Microsoft Azure ​ Azure Cosmos DB for MongoDB vCore provides developers with a fully managed MongoDB-compatible database service for building modern applications with a familiar architecture. With Cosmos DB for MongoDB vCore, developers can enjoy the benefits of native Azure integrations, low total cost of ownership (TCO), and the familiar vCore architecture when migrating existing applications or building new ones. Sign Up for free to get started today. See a usage example . from langchain . vectorstores import AzureCosmosDBVectorSearch Retrievers ​ Azure Cognitive Search ​ Azure Cognitive Search (formerly known as Azure Search ) is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications. Search is foundational to any app that surfaces text to users, where common scenarios include catalog or document search, online retail apps, or data exploration over proprietary content. When you create a search service, you'll work with the following capabilities: A search engine for full text search over a search index containing user-owned content Rich indexing, with lexical analysis and optional AI enrichment for content extraction and transformation Rich query syntax for text search, fuzzy search, autocomplete, geo-search and more Programmability through REST APIs and client libraries in Azure SDKs Azure integration at the data layer, machine learning layer, and AI (Cognitive Services) See set up instructions . See a usage example . from langchain . retrievers import AzureCognitiveSearchRetriever Utilities ​ Bing Search API ​ Microsoft Bing , commonly referred to as Bing or Bing Search ,\nis a web search engine owned and operated by Microsoft . See a usage example . from langchain . utilities import BingSearchAPIWrapper Toolkits ​ Azure Cognitive Services ​ We need to install several python packages. pip install azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision See a usage example . from langchain . agents . agent_toolkits import O365Toolkit Microsoft Office 365 email and calendar ​ We need to install O365 python package. pip install O365 See a usage example . from langchain . agents . agent_toolkits import O365Toolkit Microsoft Azure PowerBI ​ We need to install azure-identity python package. pip install azure-identity See a usage example . from langchain . agents . agent_toolkits import PowerBIToolkit from langchain . utilities . powerbi import PowerBIDataset More ​ Microsoft Presidio ​ Presidio (Origin from Latin praesidium ‘protection, garrison’)\nhelps to ensure sensitive data is properly managed and governed. It provides fast identification and\nanonymization modules for private entities in text and images such as credit card numbers, names,\nlocations, social security numbers, bitcoin wallets, US phone numbers, financial data and more. First, you need to install several python packages and download a SpaCy model. pip install langchain-experimental openai presidio-analyzer presidio-anonymizer spacy Faker python -m spacy download en_core_web_lg See usage examples . from langchain_experimental . data_anonymizer import PresidioAnonymizer , PresidioReversibleAnonymizer Previous Hugging Face Next OpenAI"
    },
    {
        "title": "OpenAI | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/platforms/openai",
        "text_content": "Providers OpenAI On this page OpenAI All functionality related to OpenAI OpenAI is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership . OpenAI conducts AI research with the declared intention of promoting and developing a friendly AI. OpenAI systems run on an Azure -based supercomputing platform from Microsoft . The OpenAI API is powered by a diverse set of models with different capabilities and price points. ChatGPT is the Artificial Intelligence (AI) chatbot developed by OpenAI . Installation and Setup ​ Install the Python SDK with pip install openai Get an OpenAI api key and set it as an environment variable ( OPENAI_API_KEY ) If you want to use OpenAI's tokenizer (only available for Python 3.9+), install it pip install tiktoken LLM ​ See a usage example . from langchain . llms import OpenAI If you are using a model hosted on Azure , you should use different wrapper for that: from langchain . llms import AzureOpenAI For a more detailed walkthrough of the Azure wrapper, see here Chat model ​ See a usage example . from langchain . chat_models import ChatOpenAI If you are using a model hosted on Azure , you should use different wrapper for that: from langchain . llms import AzureChatOpenAI For a more detailed walkthrough of the Azure wrapper, see here Text Embedding Model ​ See a usage example from langchain . embeddings import OpenAIEmbeddings Tokenizer ​ There are several places you can use the tiktoken tokenizer. By default, it is used to count tokens\nfor OpenAI LLMs. You can also use it to count tokens when splitting documents with from langchain . text_splitter import CharacterTextSplitter CharacterTextSplitter . from_tiktoken_encoder ( . . . ) For a more detailed walkthrough of this, see this notebook Document Loader ​ See a usage example . from langchain . document_loaders . chatgpt import ChatGPTLoader Retriever ​ See a usage example . from langchain . retrievers import ChatGPTPluginRetriever Chain ​ See a usage example . from langchain . chains import OpenAIModerationChain Adapter ​ See a usage example . from langchain . adapters import openai as lc_openai Tools ​ Dall-E Image Generator ​ OpenAI Dall-E are text-to-image models developed by OpenAI using deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\". See a usage example . from langchain . utilities . dalle_image_generator import DallEAPIWrapper Previous Microsoft Next Activeloop Deep Lake"
    },
    {
        "title": "Activeloop Deep Lake | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/providers/activeloop_deeplake",
        "text_content": "Providers More Activeloop Deep Lake On this page Activeloop Deep Lake This page covers how to use the Deep Lake ecosystem within LangChain. Why Deep Lake? ​ More than just a (multi-modal) vector store. You can later use the dataset to fine-tune your own LLM models. Not only stores embeddings, but also the original data with automatic version control. Truly serverless. Doesn't require another service and can be used with major cloud providers (AWS S3, GCS, etc.) Activeloop Deep Lake supports SelfQuery Retrieval: Activeloop Deep Lake Self Query Retrieval More Resources ​ Ultimate Guide to LangChain & Deep Lake: Build ChatGPT to Answer Questions on Your Financial Data Twitter the-algorithm codebase analysis with Deep Lake Code Understanding Here is whitepaper and academic paper for Deep Lake Here is a set of additional resources available for review: Deep Lake , Get started and Tutorials Installation and Setup ​ Install the Python package with pip install deeplake Wrappers ​ VectorStore ​ There exists a wrapper around Deep Lake, a data lake for Deep Learning applications, allowing you to use it as a vector store (for now), whether for semantic search or example selection. To import this vectorstore: from langchain . vectorstores import DeepLake For a more detailed walkthrough of the Deep Lake wrapper, see this notebook Previous OpenAI Next AI21 Labs"
    },
    {
        "title": "Components | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/components",
        "text_content": ""
    },
    {
        "title": "LLMs | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/llms/",
        "text_content": "Components LLMs On this page LLMs Features (natively supported) ​ All LLMs implement the Runnable interface, which comes with default implementations of all methods, ie. ainvoke , batch , abatch , stream , astream . This gives all LLMs basic support for async, streaming and batch, which by default is implemented as below: Async support defaults to calling the respective sync method in asyncio's default thread pool executor. This lets other async functions in your application make progress while the LLM is being executed, by moving this call to a background thread. Streaming support defaults to returning an Iterator (or AsyncIterator in the case of async streaming) of a single value, the final result returned by the underlying LLM provider. This obviously doesn't give you token-by-token streaming, which requires native support from the LLM provider, but ensures your code that expects an iterator of tokens can work for any of our LLM integrations. Batch support defaults to calling the underlying LLM in parallel for each input by making use of a thread pool executor (in the sync batch case) or asyncio.gather (in the async batch case). The concurrency can be controlled with the max_concurrency key in RunnableConfig . Each LLM integration can optionally provide native implementations for async, streaming or batch, which, for providers that support it, can be more efficient. The table shows, for each integration, which features have been implemented with native support. Model Invoke Async invoke Stream Async stream Batch Async batch AI21 ✅ ❌ ❌ ❌ ❌ ❌ AlephAlpha ✅ ❌ ❌ ❌ ❌ ❌ AmazonAPIGateway ✅ ❌ ❌ ❌ ❌ ❌ Anthropic ✅ ✅ ✅ ✅ ❌ ❌ Anyscale ✅ ✅ ✅ ✅ ✅ ✅ Arcee ✅ ❌ ❌ ❌ ❌ ❌ Aviary ✅ ❌ ❌ ❌ ❌ ❌ AzureMLOnlineEndpoint ✅ ❌ ❌ ❌ ❌ ❌ AzureOpenAI ✅ ✅ ✅ ✅ ✅ ✅ Banana ✅ ❌ ❌ ❌ ❌ ❌ Baseten ✅ ❌ ❌ ❌ ❌ ❌ Beam ✅ ❌ ❌ ❌ ❌ ❌ Bedrock ✅ ❌ ✅ ❌ ❌ ❌ CTransformers ✅ ✅ ❌ ❌ ❌ ❌ CTranslate2 ✅ ❌ ❌ ❌ ✅ ❌ CerebriumAI ✅ ❌ ❌ ❌ ❌ ❌ ChatGLM ✅ ❌ ❌ ❌ ❌ ❌ Clarifai ✅ ❌ ❌ ❌ ❌ ❌ Cohere ✅ ✅ ❌ ❌ ❌ ❌ Databricks ✅ ❌ ❌ ❌ ❌ ❌ DeepInfra ✅ ✅ ✅ ✅ ❌ ❌ DeepSparse ✅ ✅ ✅ ✅ ❌ ❌ EdenAI ✅ ✅ ❌ ❌ ❌ ❌ Fireworks ✅ ✅ ✅ ✅ ✅ ✅ ForefrontAI ✅ ❌ ❌ ❌ ❌ ❌ GPT4All ✅ ❌ ❌ ❌ ❌ ❌ GigaChat ✅ ✅ ✅ ✅ ✅ ✅ GooglePalm ✅ ❌ ✅ ❌ ✅ ❌ GooseAI ✅ ❌ ❌ ❌ ❌ ❌ GradientLLM ✅ ✅ ❌ ❌ ✅ ✅ HuggingFaceEndpoint ✅ ❌ ❌ ❌ ❌ ❌ HuggingFaceHub ✅ ❌ ❌ ❌ ❌ ❌ HuggingFacePipeline ✅ ❌ ❌ ❌ ✅ ❌ HuggingFaceTextGenInference ✅ ✅ ✅ ✅ ❌ ❌ HumanInputLLM ✅ ❌ ❌ ❌ ❌ ❌ JavelinAIGateway ✅ ✅ ❌ ❌ ❌ ❌ KoboldApiLLM ✅ ❌ ❌ ❌ ❌ ❌ LlamaCpp ✅ ❌ ✅ ❌ ❌ ❌ ManifestWrapper ✅ ❌ ❌ ❌ ❌ ❌ Minimax ✅ ❌ ❌ ❌ ❌ ❌ MlflowAIGateway ✅ ❌ ❌ ❌ ❌ ❌ Modal ✅ ❌ ❌ ❌ ❌ ❌ MosaicML ✅ ❌ ❌ ❌ ❌ ❌ NIBittensorLLM ✅ ❌ ❌ ❌ ❌ ❌ NLPCloud ✅ ❌ ❌ ❌ ❌ ❌ Nebula ✅ ❌ ❌ ❌ ❌ ❌ OctoAIEndpoint ✅ ❌ ❌ ❌ ❌ ❌ Ollama ✅ ❌ ❌ ❌ ❌ ❌ OpaquePrompts ✅ ❌ ❌ ❌ ❌ ❌ OpenAI ✅ ✅ ✅ ✅ ✅ ✅ OpenLLM ✅ ✅ ❌ ❌ ❌ ❌ OpenLM ✅ ✅ ✅ ✅ ✅ ✅ PaiEasEndpoint ✅ ❌ ✅ ❌ ❌ ❌ Petals ✅ ❌ ❌ ❌ ❌ ❌ PipelineAI ✅ ❌ ❌ ❌ ❌ ❌ Predibase ✅ ❌ ❌ ❌ ❌ ❌ PredictionGuard ✅ ❌ ❌ ❌ ❌ ❌ PromptLayerOpenAI ✅ ❌ ❌ ❌ ❌ ❌ QianfanLLMEndpoint ✅ ✅ ✅ ✅ ❌ ❌ RWKV ✅ ❌ ❌ ❌ ❌ ❌ Replicate ✅ ❌ ✅ ❌ ❌ ❌ SagemakerEndpoint ✅ ❌ ❌ ❌ ❌ ❌ SelfHostedHuggingFaceLLM ✅ ❌ ❌ ❌ ❌ ❌ SelfHostedPipeline ✅ ❌ ❌ ❌ ❌ ❌ StochasticAI ✅ ❌ ❌ ❌ ❌ ❌ TextGen ✅ ❌ ❌ ❌ ❌ ❌ TitanTakeoff ✅ ❌ ✅ ❌ ❌ ❌ TitanTakeoffPro ✅ ❌ ✅ ❌ ❌ ❌ Tongyi ✅ ❌ ❌ ❌ ❌ ❌ VLLM ✅ ❌ ❌ ❌ ✅ ❌ VLLMOpenAI ✅ ✅ ✅ ✅ ✅ ✅ VertexAI ✅ ✅ ✅ ❌ ✅ ✅ VertexAIModelGarden ✅ ✅ ❌ ❌ ✅ ✅ VolcEngineMaasLLM ✅ ❌ ✅ ❌ ❌ ❌ WatsonxLLM ✅ ❌ ✅ ❌ ✅ ❌ Writer ✅ ❌ ❌ ❌ ❌ ❌ Xinference ✅ ❌ ❌ ❌ ❌ ❌ YandexGPT ✅ ✅ ❌ ❌ ❌ ❌ Previous Components Next LLMs"
    },
    {
        "title": "Chat models | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/chat/",
        "text_content": "Components Chat models On this page Chat models Features (natively supported) ​ All ChatModels implement the Runnable interface, which comes with default implementations of all methods, ie. ainvoke , batch , abatch , stream , astream . This gives all ChatModels basic support for async, streaming and batch, which by default is implemented as below: Async support defaults to calling the respective sync method in asyncio's default thread pool executor. This lets other async functions in your application make progress while the ChatModel is being executed, by moving this call to a background thread. Streaming support defaults to returning an Iterator (or AsyncIterator in the case of async streaming) of a single value, the final result returned by the underlying ChatModel provider. This obviously doesn't give you token-by-token streaming, which requires native support from the ChatModel provider, but ensures your code that expects an iterator of tokens can work for any of our ChatModel integrations. Batch support defaults to calling the underlying ChatModel in parallel for each input by making use of a thread pool executor (in the sync batch case) or asyncio.gather (in the async batch case). The concurrency can be controlled with the max_concurrency key in RunnableConfig . Each ChatModel integration can optionally provide native implementations to truly enable async or streaming.\nThe table shows, for each integration, which features have been implemented with native support. Model Invoke Async invoke Stream Async stream AzureChatOpenAI ✅ ✅ ✅ ✅ BedrockChat ✅ ❌ ✅ ❌ ChatAnthropic ✅ ✅ ✅ ✅ ChatAnyscale ✅ ✅ ✅ ✅ ChatBaichuan ✅ ❌ ✅ ❌ ChatCohere ✅ ✅ ✅ ✅ ChatDatabricks ✅ ✅ ❌ ❌ ChatEverlyAI ✅ ✅ ✅ ✅ ChatFireworks ✅ ✅ ✅ ✅ ChatGooglePalm ✅ ✅ ❌ ❌ ChatHunyuan ✅ ❌ ✅ ❌ ChatJavelinAIGateway ✅ ✅ ❌ ❌ ChatKonko ✅ ❌ ❌ ❌ ChatLiteLLM ✅ ✅ ✅ ✅ ChatMLflowAIGateway ✅ ❌ ❌ ❌ ChatMlflow ✅ ✅ ❌ ❌ ChatOllama ✅ ❌ ✅ ❌ ChatOpenAI ✅ ✅ ✅ ✅ ChatVertexAI ✅ ✅ ✅ ❌ ChatYandexGPT ✅ ✅ ❌ ❌ ErnieBotChat ✅ ❌ ❌ ❌ GigaChat ✅ ✅ ✅ ✅ JinaChat ✅ ✅ ✅ ✅ MiniMaxChat ✅ ✅ ❌ ❌ PaiEasChatEndpoint ✅ ✅ ❌ ✅ PromptLayerChatOpenAI ✅ ❌ ❌ ❌ QianfanChatEndpoint ✅ ✅ ✅ ✅ VolcEngineMaasChat ✅ ❌ ✅ ❌ Previous YandexGPT Next Chat models"
    },
    {
        "title": "Document loaders | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders",
        "text_content": ""
    },
    {
        "title": "Document transformers | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_transformers",
        "text_content": ""
    },
    {
        "title": "Text embedding models | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/text_embedding",
        "text_content": ""
    },
    {
        "title": "Vector stores | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/vectorstores",
        "text_content": ""
    },
    {
        "title": "Retrievers | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/retrievers",
        "text_content": ""
    },
    {
        "title": "Tools | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/tools",
        "text_content": ""
    },
    {
        "title": "Agents and toolkits | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/toolkits",
        "text_content": ""
    },
    {
        "title": "Memory | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/memory",
        "text_content": ""
    },
    {
        "title": "Callbacks | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/callbacks",
        "text_content": ""
    },
    {
        "title": "Chat loaders | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/chat_loaders",
        "text_content": ""
    },
    {
        "title": "Adapters | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/adapters",
        "text_content": ""
    },
    {
        "title": "Stores | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/stores/",
        "text_content": "Components Stores On this page Stores In many different applications, having some sort of key-value storage is helpful.\nIn this section, we will look at a few different ways to store key-value pairs\nusing implementations of the ByteStore interface. Features (natively supported) ​ All ByteStore s support the following functions, which are used for modifying m ultiple key-value pairs at once: mget(key: Sequence[str]) -> List[Optional[bytes]] : get the contents of multiple keys, returning None if the key does not exist mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None : set the contents of multiple keys mdelete(key: Sequence[str]) -> None : delete multiple keys yield_keys(prefix: Optional[str] = None) -> Iterator[str] : yield all keys in the store, optionally filtering by a prefix How to pick one ​ ByteStore s are designed to be interchangeable. By default, most dependent integrations\nuse the InMemoryByteStore , which is a simple in-memory key-value store. However, if you start having other requirements, like massive scalability or persistence,\nyou can swap out the ByteStore implementation with one of the other ones documented\nin this section. Previous OpenAI Adapter Next Stores"
    },
    {
        "title": "Deployment | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/guides/deployments/",
        "text_content": "Deployment On this page Deployment In today's fast-paced technological landscape, the use of Large Language Models (LLMs) is rapidly expanding. As a result, it is crucial for developers to understand how to effectively deploy these models in production environments. LLM interfaces typically fall into two categories: Case 1: Utilizing External LLM Providers (OpenAI, Anthropic, etc.) In this scenario, most of the computational burden is handled by the LLM providers, while LangChain simplifies the implementation of business logic around these services. This approach includes features such as prompt templating, chat message generation, caching, vector embedding database creation, preprocessing, etc. Case 2: Self-hosted Open-Source Models Alternatively, developers can opt to use smaller, yet comparably capable, self-hosted open-source LLM models. This approach can significantly decrease costs, latency, and privacy concerns associated with transferring data to external LLM providers. Regardless of the framework that forms the backbone of your product, deploying LLM applications comes with its own set of challenges. It's vital to understand the trade-offs and key considerations when evaluating serving frameworks. Outline ​ This guide aims to provide a comprehensive overview of the requirements for deploying LLMs in a production setting, focusing on: Designing a Robust LLM Application Service Maintaining Cost-Efficiency Ensuring Rapid Iteration Understanding these components is crucial when assessing serving systems. LangChain integrates with several open-source projects designed to tackle these issues, providing a robust framework for productionizing your LLM applications. Some notable frameworks include: Ray Serve BentoML OpenLLM Modal Jina These links will provide further information on each ecosystem, assisting you in finding the best fit for your LLM deployment needs. Designing a Robust LLM Application Service ​ When deploying an LLM service in production, it's imperative to provide a seamless user experience free from outages. Achieving 24/7 service availability involves creating and maintaining several sub-systems surrounding your application. Monitoring ​ Monitoring forms an integral part of any system running in a production environment. In the context of LLMs, it is essential to monitor both performance and quality metrics. Performance Metrics: These metrics provide insights into the efficiency and capacity of your model. Here are some key examples: Query per second (QPS): This measures the number of queries your model processes in a second, offering insights into its utilization. Latency: This metric quantifies the delay from when your client sends a request to when they receive a response. Tokens Per Second (TPS): This represents the number of tokens your model can generate in a second. Quality Metrics: These metrics are typically customized according to the business use-case. For instance, how does the output of your system compare to a baseline, such as a previous version? Although these metrics can be calculated offline, you need to log the necessary data to use them later. Fault tolerance ​ Your application may encounter errors such as exceptions in your model inference or business logic code, causing failures and disrupting traffic. Other potential issues could arise from the machine running your application, such as unexpected hardware breakdowns or loss of spot-instances during high-demand periods. One way to mitigate these risks is by increasing redundancy through replica scaling and implementing recovery mechanisms for failed replicas. However, model replicas aren't the only potential points of failure. It's essential to build resilience against various failures that could occur at any point in your stack. Zero down time upgrade ​ System upgrades are often necessary but can result in service disruptions if not handled correctly. One way to prevent downtime during upgrades is by implementing a smooth transition process from the old version to the new one. Ideally, the new version of your LLM service is deployed, and traffic gradually shifts from the old to the new version, maintaining a constant QPS throughout the process. Load balancing ​ Load balancing, in simple terms, is a technique to distribute work evenly across multiple computers, servers, or other resources to optimize the utilization of the system, maximize throughput, minimize response time, and avoid overload of any single resource. Think of it as a traffic officer directing cars (requests) to different roads (servers) so that no single road becomes too congested. There are several strategies for load balancing. For example, one common method is the Round Robin strategy, where each request is sent to the next server in line, cycling back to the first when all servers have received a request. This works well when all servers are equally capable. However, if some servers are more powerful than others, you might use a Weighted Round Robin or Least Connections strategy, where more requests are sent to the more powerful servers, or to those currently handling the fewest active requests. Let's imagine you're running a LLM chain. If your application becomes popular, you could have hundreds or even thousands of users asking questions at the same time. If one server gets too busy (high load), the load balancer would direct new requests to another server that is less busy. This way, all your users get a timely response and the system remains stable. Maintaining Cost-Efficiency and Scalability ​ Deploying LLM services can be costly, especially when you're handling a large volume of user interactions. Charges by LLM providers are usually based on tokens used, making a chat system inference on these models potentially expensive. However, several strategies can help manage these costs without compromising the quality of the service. Self-hosting models ​ Several smaller and open-source LLMs are emerging to tackle the issue of reliance on LLM providers. Self-hosting allows you to maintain similar quality to LLM provider models while managing costs. The challenge lies in building a reliable, high-performing LLM serving system on your own machines. Resource Management and Auto-Scaling ​ Computational logic within your application requires precise resource allocation. For instance, if part of your traffic is served by an OpenAI endpoint and another part by a self-hosted model, it's crucial to allocate suitable resources for each. Auto-scaling—adjusting resource allocation based on traffic—can significantly impact the cost of running your application. This strategy requires a balance between cost and responsiveness, ensuring neither resource over-provisioning nor compromised application responsiveness. Utilizing Spot Instances ​ On platforms like AWS, spot instances offer substantial cost savings, typically priced at about a third of on-demand instances. The trade-off is a higher crash rate, necessitating a robust fault-tolerance mechanism for effective use. Independent Scaling ​ When self-hosting your models, you should consider independent scaling. For example, if you have two translation models, one fine-tuned for French and another for Spanish, incoming requests might necessitate different scaling requirements for each. Batching requests ​ In the context of Large Language Models, batching requests can enhance efficiency by better utilizing your GPU resources. GPUs are inherently parallel processors, designed to handle multiple tasks simultaneously. If you send individual requests to the model, the GPU might not be fully utilized as it's only working on a single task at a time. On the other hand, by batching requests together, you're allowing the GPU to work on multiple tasks at once, maximizing its utilization and improving inference speed. This not only leads to cost savings but can also improve the overall latency of your LLM service. In summary, managing costs while scaling your LLM services requires a strategic approach. Utilizing self-hosting models, managing resources effectively, employing auto-scaling, using spot instances, independently scaling models, and batching requests are key strategies to consider. Open-source libraries such as Ray Serve and BentoML are designed to deal with these complexities. Ensuring Rapid Iteration ​ The LLM landscape is evolving at an unprecedented pace, with new libraries and model architectures being introduced constantly. Consequently, it's crucial to avoid tying yourself to a solution specific to one particular framework. This is especially relevant in serving, where changes to your infrastructure can be time-consuming, expensive, and risky. Strive for infrastructure that is not locked into any specific machine learning library or framework, but instead offers a general-purpose, scalable serving layer. Here are some aspects where flexibility plays a key role: Model composition ​ Deploying systems like LangChain demands the ability to piece together different models and connect them via logic. Take the example of building a natural language input SQL query engine. Querying an LLM and obtaining the SQL command is only part of the system. You need to extract metadata from the connected database, construct a prompt for the LLM, run the SQL query on an engine, collect and feed back the response to the LLM as the query runs, and present the results to the user. This demonstrates the need to seamlessly integrate various complex components built in Python into a dynamic chain of logical blocks that can be served together. Cloud providers ​ Many hosted solutions are restricted to a single cloud provider, which can limit your options in today's multi-cloud world. Depending on where your other infrastructure components are built, you might prefer to stick with your chosen cloud provider. Infrastructure as Code (IaC) ​ Rapid iteration also involves the ability to recreate your infrastructure quickly and reliably. This is where Infrastructure as Code (IaC) tools like Terraform, CloudFormation, or Kubernetes YAML files come into play. They allow you to define your infrastructure in code files, which can be version controlled and quickly deployed, enabling faster and more reliable iterations. CI/CD ​ In a fast-paced environment, implementing CI/CD pipelines can significantly speed up the iteration process. They help automate the testing and deployment of your LLM applications, reducing the risk of errors and enabling faster feedback and iteration. Previous Debugging Next LangChain Templates"
    },
    {
        "title": "Evaluation | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/guides/evaluation/",
        "text_content": "Evaluation On this page Evaluation Building applications with language models involves many moving parts. One of the most critical components is ensuring that the outcomes produced by your models are reliable and useful across a broad array of inputs, and that they work well with your application's other software components. Ensuring reliability usually boils down to some combination of application design, testing & evaluation, and runtime checks. The guides in this section review the APIs and functionality LangChain provides to help you better evaluate your applications. Evaluation and testing are both critical when thinking about deploying LLM applications, since production environments require repeatable and useful outcomes. LangChain offers various types of evaluators to help you measure performance and integrity on diverse data, and we hope to encourage the community to create and share other useful evaluators so everyone can improve. These docs will introduce the evaluator types, how to use them, and provide some examples of their use in real-world scenarios. Each evaluator type in LangChain comes with ready-to-use implementations and an extensible API that allows for customization according to your unique requirements. Here are some of the types of evaluators we offer: String Evaluators : These evaluators assess the predicted string for a given input, usually comparing it against a reference string. Trajectory Evaluators : These are used to evaluate the entire trajectory of agent actions. Comparison Evaluators : These evaluators are designed to compare predictions from two runs on a common input. These evaluators can be used across various scenarios and can be applied to different chain and LLM implementations in the LangChain library. We also are working to share guides and cookbooks that demonstrate how to use these evaluators in real-world scenarios, such as: Chain Comparisons : This example uses a comparison evaluator to predict the preferred output. It reviews ways to measure confidence intervals to select statistically significant differences in aggregate preference scores across different models or prompts. LangSmith Evaluation ​ LangSmith provides an integrated evaluation and tracing framework that allows you to check for regressions, compare systems, and easily identify and fix any sources of errors and performance issues. Check out the docs on LangSmith Evaluation and additional cookbooks for more detailed information on evaluating your applications. LangChain benchmarks ​ Your application quality is a function both of the LLM you choose and the prompting and data retrieval strategies you employ to provide model contexet. We have published a number of benchmark tasks within the LangChain Benchmarks package to grade different LLM systems on tasks such as: Agent tool use Retrieval-augmented question-answering Structured Extraction Check out the docs for examples and leaderboard information. Reference Docs ​ For detailed information on the available evaluators, including how to instantiate, configure, and customize them, check out the reference documentation directly. 🗃️ String Evaluators 8 items 🗃️ Comparison Evaluators 3 items 🗃️ Trajectory Evaluators 2 items 🗃️ Examples 1 items Previous LangChain Templates Next String Evaluators"
    },
    {
        "title": "Fallbacks | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/guides/fallbacks",
        "text_content": "Fallbacks On this page Fallbacks When working with language models, you may often encounter issues from\nthe underlying APIs, whether these be rate limiting or downtime.\nTherefore, as you go to move your LLM applications into production it\nbecomes more and more important to safeguard against these. That’s why\nwe’ve introduced the concept of fallbacks. A fallback is an alternative plan that may be used in an emergency. Crucially, fallbacks can be applied not only on the LLM level but on the\nwhole runnable level. This is important because often times different\nmodels require different prompts. So if your call to OpenAI fails, you\ndon’t just want to send the same prompt to Anthropic - you probably want\nto use a different prompt template and send a different version there. Fallback for LLM API Errors ​ This is maybe the most common use case for fallbacks. A request to an\nLLM API can fail for a variety of reasons - the API could be down, you\ncould have hit rate limits, any number of things. Therefore, using\nfallbacks can help protect against these types of things. IMPORTANT: By default, a lot of the LLM wrappers catch errors and retry.\nYou will most likely want to turn those off when working with fallbacks.\nOtherwise the first wrapper will keep on retrying and not failing. from langchain . chat_models import ChatAnthropic , ChatOpenAI First, let’s mock out what happens if we hit a RateLimitError from\nOpenAI from unittest . mock import patch import httpx from openai import RateLimitError request = httpx . Request ( \"GET\" , \"/\" ) response = httpx . Response ( 200 , request = request ) error = RateLimitError ( \"rate limit\" , response = response , body = \"\" ) # Note that we set max_retries = 0 to avoid retrying on RateLimits, etc openai_llm = ChatOpenAI ( max_retries = 0 ) anthropic_llm = ChatAnthropic ( ) llm = openai_llm . with_fallbacks ( [ anthropic_llm ] ) # Let's use just the OpenAI LLm first, to show that we run into an error with patch ( \"openai.resources.chat.completions.Completions.create\" , side_effect = error ) : try : print ( openai_llm . invoke ( \"Why did the chicken cross the road?\" ) ) except RateLimitError : print ( \"Hit error\" ) Hit error # Now let's try with fallbacks to Anthropic with patch ( \"openai.resources.chat.completions.Completions.create\" , side_effect = error ) : try : print ( llm . invoke ( \"Why did the chicken cross the road?\" ) ) except RateLimitError : print ( \"Hit error\" ) content=' I don\\'t actually know why the chicken crossed the road, but here are some possible humorous answers:\\n\\n- To get to the other side!\\n\\n- It was too chicken to just stand there. \\n\\n- It wanted a change of scenery.\\n\\n- It wanted to show the possum it could be done.\\n\\n- It was on its way to a poultry farmers\\' convention.\\n\\nThe joke plays on the double meaning of \"the other side\" - literally crossing the road to the other side, or the \"other side\" meaning the afterlife. So it\\'s an anti-joke, with a silly or unexpected pun as the answer.' additional_kwargs={} example=False We can use our “LLM with Fallbacks” as we would a normal LLM. from langchain . prompts import ChatPromptTemplate prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , \"You're a nice assistant who always includes a compliment in your response\" , ) , ( \"human\" , \"Why did the {animal} cross the road\" ) , ] ) chain = prompt | llm with patch ( \"openai.resources.chat.completions.Completions.create\" , side_effect = error ) : try : print ( chain . invoke ( { \"animal\" : \"kangaroo\" } ) ) except RateLimitError : print ( \"Hit error\" ) content=\" I don't actually know why the kangaroo crossed the road, but I can take a guess! Here are some possible reasons:\\n\\n- To get to the other side (the classic joke answer!)\\n\\n- It was trying to find some food or water \\n\\n- It was trying to find a mate during mating season\\n\\n- It was fleeing from a predator or perceived threat\\n\\n- It was disoriented and crossed accidentally \\n\\n- It was following a herd of other kangaroos who were crossing\\n\\n- It wanted a change of scenery or environment \\n\\n- It was trying to reach a new habitat or territory\\n\\nThe real reason is unknown without more context, but hopefully one of those potential explanations does the joke justice! Let me know if you have any other animal jokes I can try to decipher.\" additional_kwargs={} example=False Fallback for Sequences ​ We can also create fallbacks for sequences, that are sequences\nthemselves. Here we do that with two different models: ChatOpenAI and\nthen normal OpenAI (which does not use a chat model). Because OpenAI is\nNOT a chat model, you likely want a different prompt. # First let's create a chain with a ChatModel # We add in a string output parser here so the outputs between the two are the same type from langchain_core . output_parsers import StrOutputParser chat_prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , \"You're a nice assistant who always includes a compliment in your response\" , ) , ( \"human\" , \"Why did the {animal} cross the road\" ) , ] ) # Here we're going to use a bad model name to easily create a chain that will error chat_model = ChatOpenAI ( model_name = \"gpt-fake\" ) bad_chain = chat_prompt | chat_model | StrOutputParser ( ) # Now lets create a chain with the normal OpenAI model from langchain . llms import OpenAI from langchain . prompts import PromptTemplate prompt_template = \"\"\"Instructions: You should always include a compliment in your response. Question: Why did the {animal} cross the road?\"\"\" prompt = PromptTemplate . from_template ( prompt_template ) llm = OpenAI ( ) good_chain = prompt | llm # We can now create a final chain which combines the two chain = bad_chain . with_fallbacks ( [ good_chain ] ) chain . invoke ( { \"animal\" : \"turtle\" } ) '\\n\\nAnswer: The turtle crossed the road to get to the other side, and I have to say he had some impressive determination.' Fallback for Long Inputs ​ One of the big limiting factors of LLMs is their context window.\nUsually, you can count and track the length of prompts before sending\nthem to an LLM, but in situations where that is hard/complicated, you\ncan fallback to a model with a longer context length. short_llm = ChatOpenAI ( ) long_llm = ChatOpenAI ( model = \"gpt-3.5-turbo-16k\" ) llm = short_llm . with_fallbacks ( [ long_llm ] ) inputs = \"What is the next number: \" + \", \" . join ( [ \"one\" , \"two\" ] * 3000 ) try : print ( short_llm . invoke ( inputs ) ) except Exception as e : print ( e ) This model's maximum context length is 4097 tokens. However, your messages resulted in 12012 tokens. Please reduce the length of the messages. try : print ( llm . invoke ( inputs ) ) except Exception as e : print ( e ) content='The next number in the sequence is two.' additional_kwargs={} example=False Fallback to Better Model ​ Often times we ask models to output format in a specific format (like\nJSON). Models like GPT-3.5 can do this okay, but sometimes struggle.\nThis naturally points to fallbacks - we can try with GPT-3.5 (faster,\ncheaper), but then if parsing fails we can use GPT-4. from langchain . output_parsers import DatetimeOutputParser prompt = ChatPromptTemplate . from_template ( \"what time was {event} (in %Y-%m-%dT%H:%M:%S.%fZ format - only return this value)\" ) # In this case we are going to do the fallbacks on the LLM + output parser level # Because the error will get raised in the OutputParser openai_35 = ChatOpenAI ( ) | DatetimeOutputParser ( ) openai_4 = ChatOpenAI ( model = \"gpt-4\" ) | DatetimeOutputParser ( ) only_35 = prompt | openai_35 fallback_4 = prompt | openai_35 . with_fallbacks ( [ openai_4 ] ) try : print ( only_35 . invoke ( { \"event\" : \"the superbowl in 1994\" } ) ) except Exception as e : print ( f\"Error: { e } \" ) Error: Could not parse datetime string: The Super Bowl in 1994 took place on January 30th at 3:30 PM local time. Converting this to the specified format (%Y-%m-%dT%H:%M:%S.%fZ) results in: 1994-01-30T15:30:00.000Z try : print ( fallback_4 . invoke ( { \"event\" : \"the superbowl in 1994\" } ) ) except Exception as e : print ( f\"Error: { e } \" ) 1994-01-30 15:30:00 Previous Comparing Chain Outputs Next Run LLMs locally"
    },
    {
        "title": "Run LLMs locally | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/guides/local_llms",
        "text_content": "Run LLMs locally On this page Run LLMs locally Use case ​ The popularity of projects like PrivateGPT , llama.cpp , and GPT4All underscore the demand to\nrun LLMs locally (on your own device). This has at least two important benefits: Privacy : Your data is not sent to a third party, and it is not\nsubject to the terms of service of a commercial service Cost : There is no inference fee, which is important for\ntoken-intensive applications (e.g., long-running\nsimulations ,\nsummarization) Overview ​ Running an LLM locally requires a few things: Open-source LLM : An open-source LLM that can be freely modified\nand shared Inference : Ability to run this LLM on your device w/ acceptable\nlatency Open-source LLMs ​ Users can now gain access to a rapidly growing set of open-source\nLLMs . These LLMs can be assessed across at least two dimensions (see figure): Base model : What is the base-model and how was it trained? Fine-tuning approach : Was the base-model fine-tuned and, if so,\nwhat set of\ninstructions was used? The relative performance of these models can be assessed using several\nleaderboards, including: LmSys GPT4All HuggingFace Inference ​ A few frameworks for this have emerged to support inference of\nopen-source LLMs on various devices: llama.cpp : C++\nimplementation of llama inference code with weight optimization /\nquantization gpt4all : Optimized C backend\nfor inference Ollama : Bundles model weights and\nenvironment into an app that runs on device and serves the LLM In general, these frameworks will do a few things: Quantization : Reduce the memory footprint of the raw model weights Efficient implementation for inference : Support inference on\nconsumer hardware (e.g., CPU or laptop GPU) In particular, see this excellent\npost on the importance\nof quantization. With less precision, we radically decrease the memory needed to store\nthe LLM in memory. In addition, we can see the importance of GPU memory bandwidth sheet ! A Mac M2 Max is 5-6x faster than a M1 for inference due to the larger\nGPU memory bandwidth. Quickstart ​ Ollama is one way to easily run inference on\nmacOS. The instructions here provide details, which we summarize: Download and run the app From command line, fetch a model from this list of\noptions : e.g., ollama pull llama2 When the app is running, all models are automatically served on localhost:11434 from langchain . llms import Ollama llm = Ollama ( model = \"llama2\" ) llm ( \"The first man on the moon was ...\" ) ' The first man on the moon was Neil Armstrong, who landed on the moon on July 20, 1969 as part of the Apollo 11 mission. obviously.' Stream tokens as they are being generated. from langchain . callbacks . manager import CallbackManager from langchain . callbacks . streaming_stdout import StreamingStdOutCallbackHandler llm = Ollama ( model = \"llama2\" , callback_manager = CallbackManager ( [ StreamingStdOutCallbackHandler ( ) ] ) ) llm ( \"The first man on the moon was ...\" ) The first man to walk on the moon was Neil Armstrong, an American astronaut who was part of the Apollo 11 mission in 1969. февруари 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon's surface, famously declaring \"That's one small step for man, one giant leap for mankind\" as he took his first steps. He was followed by fellow astronaut Edwin \"Buzz\" Aldrin, who also walked on the moon during the mission. ' The first man to walk on the moon was Neil Armstrong, an American astronaut who was part of the Apollo 11 mission in 1969. февруари 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon\\'s surface, famously declaring \"That\\'s one small step for man, one giant leap for mankind\" as he took his first steps. He was followed by fellow astronaut Edwin \"Buzz\" Aldrin, who also walked on the moon during the mission.' Environment ​ Inference speed is a challenge when running models locally (see above). To minimize latency, it is desirable to run models locally on GPU, which\nships with many consumer laptops e.g., Apple\ndevices . And even with GPU, the available GPU memory bandwidth (as noted above)\nis important. Running Apple silicon GPU ​ Ollama will automatically utilize the GPU on Apple devices. Other frameworks require the user to set up the environment to utilize\nthe Apple GPU. For example, llama.cpp python bindings can be configured to use the\nGPU via Metal . Metal is a graphics and compute API created by Apple providing\nnear-direct access to the GPU. See the llama.cpp setup here to enable this. In particular, ensure that conda is using the correct virtual\nenvironment that you created ( miniforge3 ). E.g., for me: conda activate /Users/rlm/miniforge3/envs/llama With the above confirmed, then: CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install -U llama-cpp-python --no-cache-dir LLMs ​ There are various ways to gain access to quantized model weights. HuggingFace - Many quantized\nmodel are available for download and can be run with framework such\nas llama.cpp gpt4all - The model explorer\noffers a leaderboard of metrics and associated quantized models\navailable for download Ollama - Several models can\nbe accessed directly via pull Ollama ​ With Ollama , fetch a model via ollama pull <model family>:<tag> : E.g., for Llama-7b: ollama pull llama2 will download the most\nbasic version of the model (e.g., smallest # parameters and 4 bit\nquantization) We can also specify a particular version from the model\nlist ,\ne.g., ollama pull llama2:13b See the full set of parameters on the API reference\npage from langchain . llms import Ollama llm = Ollama ( model = \"llama2:13b\" ) llm ( \"The first man on the moon was ... think step by step\" ) ' Sure! Here\\'s the answer, broken down step by step:\\n\\nThe first man on the moon was... Neil Armstrong.\\n\\nHere\\'s how I arrived at that answer:\\n\\n1. The first manned mission to land on the moon was Apollo 11.\\n2. The mission included three astronauts: Neil Armstrong, Edwin \"Buzz\" Aldrin, and Michael Collins.\\n3. Neil Armstrong was the mission commander and the first person to set foot on the moon.\\n4. On July 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon\\'s surface, famously declaring \"That\\'s one small step for man, one giant leap for mankind.\"\\n\\nSo, the first man on the moon was Neil Armstrong!' Llama.cpp ​ Llama.cpp is compatible with a broad set of\nmodels . For example, below we run inference on llama2-13b with 4 bit\nquantization downloaded from HuggingFace . As noted above, see the API\nreference for the full set of parameters. From the llama.cpp\ndocs , a\nfew are worth commenting on: n_gpu_layers : number of layers to be loaded into GPU memory Value: 1 Meaning: Only one layer of the model will be loaded into GPU memory\n(1 is often sufficient). n_batch : number of tokens the model should process in parallel Value: n_batch Meaning: It’s recommended to choose a value between 1 and n_ctx\n(which in this case is set to 2048) n_ctx : Token context window Value: 2048 Meaning: The model will consider a window of 2048 tokens at a time f16_kv : whether the model should use half-precision for the key/value\ncache Value: True Meaning: The model will use half-precision, which can be more memory\nefficient; Metal only supports True. % env CMAKE_ARGS = \"-DLLAMA_METAL=on\" % env FORCE_CMAKE = 1 % pip install - U llama - cpp - python - - no - cache - dirclear from langchain . callbacks . manager import CallbackManager from langchain . callbacks . streaming_stdout import StreamingStdOutCallbackHandler from langchain . llms import LlamaCpp llm = LlamaCpp ( model_path = \"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\" , n_gpu_layers = 1 , n_batch = 512 , n_ctx = 2048 , f16_kv = True , callback_manager = CallbackManager ( [ StreamingStdOutCallbackHandler ( ) ] ) , verbose = True , ) The console log will show the below to indicate Metal was enabled\nproperly from steps above: ggml_metal_init: allocating ggml_metal_init: using MPS llm ( \"The first man on the moon was ... Let's think step by step\" ) Llama.generate: prefix-match hit llama_print_timings:        load time =  9623.21 ms llama_print_timings:      sample time =   143.77 ms /   203 runs   (    0.71 ms per token,  1412.01 tokens per second) llama_print_timings: prompt eval time =   485.94 ms /     7 tokens (   69.42 ms per token,    14.40 tokens per second) llama_print_timings:        eval time =  6385.16 ms /   202 runs   (   31.61 ms per token,    31.64 tokens per second) llama_print_timings:       total time =  7279.28 ms and use logical reasoning to figure out who the first man on the moon was. Here are some clues: 1. The first man on the moon was an American. 2. He was part of the Apollo 11 mission. 3. He stepped out of the lunar module and became the first person to set foot on the moon's surface. 4. His last name is Armstrong. Now, let's use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon's surface. And finally, clue #4 gives us his last name: Armstrong. Therefore, the first man on the moon was Neil Armstrong! \" and use logical reasoning to figure out who the first man on the moon was.\\n\\nHere are some clues:\\n\\n1. The first man on the moon was an American.\\n2. He was part of the Apollo 11 mission.\\n3. He stepped out of the lunar module and became the first person to set foot on the moon's surface.\\n4. His last name is Armstrong.\\n\\nNow, let's use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon's surface. And finally, clue #4 gives us his last name: Armstrong.\\nTherefore, the first man on the moon was Neil Armstrong!\" GPT4All ​ We can use model weights downloaded from GPT4All model explorer. Similar to what is shown above, we can run inference and use the API\nreference to set parameters of interest. pip install gpt4all from langchain . llms import GPT4All llm = GPT4All ( model = \"/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin\" ) llm ( \"The first man on the moon was ... Let's think step by step\" ) \".\\n1) The United States decides to send a manned mission to the moon.2) They choose their best astronauts and train them for this specific mission.3) They build a spacecraft that can take humans to the moon, called the Lunar Module (LM).4) They also create a larger spacecraft, called the Saturn V rocket, which will launch both the LM and the Command Service Module (CSM), which will carry the astronauts into orbit.5) The mission is planned down to the smallest detail: from the trajectory of the rockets to the exact movements of the astronauts during their moon landing.6) On July 16, 1969, the Saturn V rocket launches from Kennedy Space Center in Florida, carrying the Apollo 11 mission crew into space.7) After one and a half orbits around the Earth, the LM separates from the CSM and begins its descent to the moon's surface.8) On July 20, 1969, at 2:56 pm EDT (GMT-4), Neil Armstrong becomes the first man on the moon. He speaks these\" Prompts ​ Some LLMs will benefit from specific prompts. For example, LLaMA will use special\ntokens . We can use ConditionalPromptSelector to set prompt based on the model\ntype. # Set our LLM llm = LlamaCpp ( model_path = \"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\" , n_gpu_layers = 1 , n_batch = 512 , n_ctx = 2048 , f16_kv = True , callback_manager = CallbackManager ( [ StreamingStdOutCallbackHandler ( ) ] ) , verbose = True , ) Set the associated prompt based upon the model version. from langchain . chains import LLMChain from langchain . chains . prompt_selector import ConditionalPromptSelector from langchain . prompts import PromptTemplate DEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate ( input_variables = [ \"question\" ] , template = \"\"\"<<SYS>> \\n You are an assistant tasked with improving Google search \\ results. \\n <</SYS>> \\n\\n [INST] Generate THREE Google search queries that \\ are similar to this question. The output should be a numbered list of questions \\ and each should have a question mark at the end: \\n\\n {question} [/INST]\"\"\" , ) DEFAULT_SEARCH_PROMPT = PromptTemplate ( input_variables = [ \"question\" ] , template = \"\"\"You are an assistant tasked with improving Google search \\ results. Generate THREE Google search queries that are similar to \\ this question. The output should be a numbered list of questions and each \\ should have a question mark at the end: {question}\"\"\" , ) QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector ( default_prompt = DEFAULT_SEARCH_PROMPT , conditionals = [ ( lambda llm : isinstance ( llm , LlamaCpp ) , DEFAULT_LLAMA_SEARCH_PROMPT ) ] , ) prompt = QUESTION_PROMPT_SELECTOR . get_prompt ( llm ) prompt PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='<<SYS>> \\n You are an assistant tasked with improving Google search results. \\n <</SYS>> \\n\\n [INST] Generate THREE Google search queries that are similar to this question. The output should be a numbered list of questions and each should have a question mark at the end: \\n\\n {question} [/INST]', template_format='f-string', validate_template=True) # Chain llm_chain = LLMChain ( prompt = prompt , llm = llm ) question = \"What NFL team won the Super Bowl in the year that Justin Bieber was born?\" llm_chain . run ( { \"question\" : question } ) Sure! Here are three similar search queries with a question mark at the end: 1. Which NBA team did LeBron James lead to a championship in the year he was drafted? 2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born? 3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season? llama_print_timings:        load time = 14943.19 ms llama_print_timings:      sample time =    72.93 ms /   101 runs   (    0.72 ms per token,  1384.87 tokens per second) llama_print_timings: prompt eval time = 14942.95 ms /    93 tokens (  160.68 ms per token,     6.22 tokens per second) llama_print_timings:        eval time =  3430.85 ms /   100 runs   (   34.31 ms per token,    29.15 tokens per second) llama_print_timings:       total time = 18578.26 ms '  Sure! Here are three similar search queries with a question mark at the end:\\n\\n1. Which NBA team did LeBron James lead to a championship in the year he was drafted?\\n2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born?\\n3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season?' We also can use the LangChain Prompt Hub to fetch and / or store prompts\nthat are model specific. This will work with your LangSmith API\nkey . For example, here is a prompt\nfor RAG with LLaMA-specific tokens. Use cases ​ Given an llm created from one of the models above, you can use it for many use cases . For example, here is a guide to RAG with local\nLLMs. In general, use cases for local LLMs can be driven by at least two\nfactors: Privacy : private data (e.g., journals, etc) that a user does not\nwant to share Cost : text preprocessing (extraction/tagging), summarization, and\nagent simulations are token-use-intensive tasks In addition, here is an overview on fine-tuning, which can utilize open-source LLMs. Previous Fallbacks Next Model comparison"
    },
    {
        "title": "Model comparison | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/guides/model_laboratory",
        "text_content": "Model comparison Model comparison Constructing your language model application will likely involved\nchoosing between many different options of prompts, models, and even\nchains to use. When doing so, you will want to compare these different\noptions on different inputs in an easy, flexible, and intuitive way. LangChain provides the concept of a ModelLaboratory to test out and try\ndifferent models. from langchain . llms import Cohere , HuggingFaceHub , OpenAI from langchain . model_laboratory import ModelLaboratory from langchain . prompts import PromptTemplate llms = [ OpenAI ( temperature = 0 ) , Cohere ( model = \"command-xlarge-20221108\" , max_tokens = 20 , temperature = 0 ) , HuggingFaceHub ( repo_id = \"google/flan-t5-xl\" , model_kwargs = { \"temperature\" : 1 } ) , ] model_lab = ModelLaboratory . from_llms ( llms ) model_lab . compare ( \"What color is a flamingo?\" ) Input: What color is a flamingo? OpenAI Params: {'model': 'text-davinci-002', 'temperature': 0.0, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'best_of': 1} Flamingos are pink. Cohere Params: {'model': 'command-xlarge-20221108', 'max_tokens': 20, 'temperature': 0.0, 'k': 0, 'p': 1, 'frequency_penalty': 0, 'presence_penalty': 0} Pink HuggingFaceHub Params: {'repo_id': 'google/flan-t5-xl', 'temperature': 1} pink prompt = PromptTemplate ( template = \"What is the capital of {state}?\" , input_variables = [ \"state\" ] ) model_lab_with_prompt = ModelLaboratory . from_llms ( llms , prompt = prompt ) model_lab_with_prompt . compare ( \"New York\" ) Input: New York OpenAI Params: {'model': 'text-davinci-002', 'temperature': 0.0, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'best_of': 1} The capital of New York is Albany. Cohere Params: {'model': 'command-xlarge-20221108', 'max_tokens': 20, 'temperature': 0.0, 'k': 0, 'p': 1, 'frequency_penalty': 0, 'presence_penalty': 0} The capital of New York is Albany. HuggingFaceHub Params: {'repo_id': 'google/flan-t5-xl', 'temperature': 1} st john s from langchain . chains import SelfAskWithSearchChain from langchain . utilities import SerpAPIWrapper open_ai_llm = OpenAI ( temperature = 0 ) search = SerpAPIWrapper ( ) self_ask_with_search_openai = SelfAskWithSearchChain ( llm = open_ai_llm , search_chain = search , verbose = True ) cohere_llm = Cohere ( temperature = 0 , model = \"command-xlarge-20221108\" ) search = SerpAPIWrapper ( ) self_ask_with_search_cohere = SelfAskWithSearchChain ( llm = cohere_llm , search_chain = search , verbose = True ) chains = [ self_ask_with_search_openai , self_ask_with_search_cohere ] names = [ str ( open_ai_llm ) , str ( cohere_llm ) ] model_lab = ModelLaboratory ( chains , names = names ) model_lab . compare ( \"What is the hometown of the reigning men's U.S. Open champion?\" ) Input: What is the hometown of the reigning men's U.S. Open champion? OpenAI Params: {'model': 'text-davinci-002', 'temperature': 0.0, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'best_of': 1} > Entering new chain... What is the hometown of the reigning men's U.S. Open champion? Are follow up questions needed here: Yes. Follow up: Who is the reigning men's U.S. Open champion? Intermediate answer: Carlos Alcaraz. Follow up: Where is Carlos Alcaraz from? Intermediate answer: El Palmar, Spain. So the final answer is: El Palmar, Spain > Finished chain. So the final answer is: El Palmar, Spain Cohere Params: {'model': 'command-xlarge-20221108', 'max_tokens': 256, 'temperature': 0.0, 'k': 0, 'p': 1, 'frequency_penalty': 0, 'presence_penalty': 0} > Entering new chain... What is the hometown of the reigning men's U.S. Open champion? Are follow up questions needed here: Yes. Follow up: Who is the reigning men's U.S. Open champion? Intermediate answer: Carlos Alcaraz. So the final answer is: Carlos Alcaraz > Finished chain. So the final answer is: Carlos Alcaraz Previous Run LLMs locally Next Data anonymization with Microsoft Presidio"
    },
    {
        "title": "Data anonymization with Microsoft Presidio | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/guides/privacy/presidio_data_anonymization/",
        "text_content": "Privacy Data anonymization with Microsoft Presidio On this page Data anonymization with Microsoft Presidio Open In Colab Presidio (Origin from Latin\npraesidium ‘protection, garrison’) helps to ensure sensitive data is\nproperly managed and governed. It provides fast identification and\nanonymization modules for private entities in text and images such as\ncredit card numbers, names, locations, social security numbers,\nbitcoin wallets, US phone numbers, financial data and more. Use case ​ Data anonymization is crucial before passing information to a language\nmodel like GPT-4 because it helps protect privacy and maintain\nconfidentiality. If data is not anonymized, sensitive information such\nas names, addresses, contact numbers, or other identifiers linked to\nspecific individuals could potentially be learned and misused. Hence, by\nobscuring or removing this personally identifiable information (PII),\ndata can be used freely without compromising individuals’ privacy rights\nor breaching data protection laws and regulations. Overview ​ Anonynization consists of two steps: Identification: Identify all data fields that contain personally\nidentifiable information (PII). Replacement : Replace all PIIs with pseudo values or codes that\ndo not reveal any personal information about the individual but can\nbe used for reference. We’re not using regular encryption, because\nthe language model won’t be able to understand the meaning or\ncontext of the encrypted data. We use Microsoft Presidio together with Faker framework for\nanonymization purposes because of the wide range of functionalities they\nprovide. The full implementation is available in PresidioAnonymizer . Quickstart ​ Below you will find the use case on how to leverage anonymization in\nLangChain. # Install necessary packages # ! pip install langchain langchain-experimental openai presidio-analyzer presidio-anonymizer spacy Faker # ! python -m spacy download en_core_web_lg Let’s see how PII anonymization works using a sample sentence: from langchain_experimental . data_anonymizer import PresidioAnonymizer anonymizer = PresidioAnonymizer ( ) anonymizer . anonymize ( \"My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com\" ) 'My name is James Martinez, call me at (576)928-1972x679 or email me at lisa44@example.com' Using with LangChain Expression Language ​ With LCEL we can easily chain together anonymization with the rest of\nour application. # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv() text = \"\"\"Slim Shady recently lost his wallet. Inside is some cash and his credit card with the number 4916 0387 9536 0861. If you would find it, please call at 313-666-7440 or write an email here: real.slim.shady@gmail.com.\"\"\" from langchain . chat_models import ChatOpenAI from langchain . prompts . prompt import PromptTemplate anonymizer = PresidioAnonymizer ( ) template = \"\"\"Rewrite this text into an official, short email: {anonymized_text}\"\"\" prompt = PromptTemplate . from_template ( template ) llm = ChatOpenAI ( temperature = 0 ) chain = { \"anonymized_text\" : anonymizer . anonymize } | prompt | llm response = chain . invoke ( text ) print ( response . content ) Dear Sir/Madam, We regret to inform you that Mr. Dennis Cooper has recently misplaced his wallet. The wallet contains a sum of cash and his credit card, bearing the number 3588895295514977. Should you happen to come across the aforementioned wallet, kindly contact us immediately at (428)451-3494x4110 or send an email to perryluke@example.com. Your prompt assistance in this matter would be greatly appreciated. Yours faithfully, [Your Name] Customization ​ We can specify analyzed_fields to only anonymize particular types of\ndata. anonymizer = PresidioAnonymizer ( analyzed_fields = [ \"PERSON\" ] ) anonymizer . anonymize ( \"My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com\" ) 'My name is Shannon Steele, call me at 313-666-7440 or email me at real.slim.shady@gmail.com' As can be observed, the name was correctly identified and replaced with\nanother. The analyzed_fields attribute is responsible for what values\nare to be detected and substituted. We can add PHONE_NUMBER to the\nlist: anonymizer = PresidioAnonymizer ( analyzed_fields = [ \"PERSON\" , \"PHONE_NUMBER\" ] ) anonymizer . anonymize ( \"My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com\" ) 'My name is Wesley Flores, call me at (498)576-9526 or email me at real.slim.shady@gmail.com' If no analyzed_fields are specified, by default the anonymizer will\ndetect all supported formats. Below is the full list of them: ['PERSON', 'EMAIL_ADDRESS', 'PHONE_NUMBER', 'IBAN_CODE', 'CREDIT_CARD', 'CRYPTO', 'IP_ADDRESS', 'LOCATION', 'DATE_TIME', 'NRP', 'MEDICAL_LICENSE', 'URL', 'US_BANK_NUMBER', 'US_DRIVER_LICENSE', 'US_ITIN', 'US_PASSPORT', 'US_SSN'] Disclaimer: We suggest carefully defining the private data to be\ndetected - Presidio doesn’t work perfectly and it sometimes makes\nmistakes, so it’s better to have more control over the data. anonymizer = PresidioAnonymizer ( ) anonymizer . anonymize ( \"My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com\" ) 'My name is Carla Fisher, call me at 001-683-324-0721x0644 or email me at krausejeremy@example.com' It may be that the above list of detected fields is not sufficient. For\nexample, the already available PHONE_NUMBER field does not support\npolish phone numbers and confuses it with another field: anonymizer = PresidioAnonymizer ( ) anonymizer . anonymize ( \"My polish phone number is 666555444\" ) 'My polish phone number is QESQ21234635370499' You can then write your own recognizers and add them to the pool of\nthose present. How exactly to create recognizers is described in the Presidio\ndocumentation . # Define the regex pattern in a Presidio `Pattern` object: from presidio_analyzer import Pattern , PatternRecognizer polish_phone_numbers_pattern = Pattern ( name = \"polish_phone_numbers_pattern\" , regex = \"(?<!\\w)(\\(?(\\+|00)?48\\)?)?[ -]?\\d{3}[ -]?\\d{3}[ -]?\\d{3}(?!\\w)\" , score = 1 , ) # Define the recognizer with one or more patterns polish_phone_numbers_recognizer = PatternRecognizer ( supported_entity = \"POLISH_PHONE_NUMBER\" , patterns = [ polish_phone_numbers_pattern ] ) Now, we can add recognizer by calling add_recognizer method on the\nanonymizer: anonymizer . add_recognizer ( polish_phone_numbers_recognizer ) And voilà! With the added pattern-based recognizer, the anonymizer now\nhandles polish phone numbers. print ( anonymizer . anonymize ( \"My polish phone number is 666555444\" ) ) print ( anonymizer . anonymize ( \"My polish phone number is 666 555 444\" ) ) print ( anonymizer . anonymize ( \"My polish phone number is +48 666 555 444\" ) ) My polish phone number is <POLISH_PHONE_NUMBER> My polish phone number is <POLISH_PHONE_NUMBER> My polish phone number is <POLISH_PHONE_NUMBER> The problem is - even though we recognize polish phone numbers now, we\ndon’t have a method (operator) that would tell how to substitute a given\nfield - because of this, in the outpit we only provide string <POLISH_PHONE_NUMBER> We need to create a method to replace it\ncorrectly: from faker import Faker fake = Faker ( locale = \"pl_PL\" ) def fake_polish_phone_number ( _ = None ) : return fake . phone_number ( ) fake_polish_phone_number ( ) '665 631 080' We used Faker to create pseudo data. Now we can create an operator and\nadd it to the anonymizer. For complete information about operators and\ntheir creation, see the Presidio documentation for simple and custom anonymization. from presidio_anonymizer . entities import OperatorConfig new_operators = { \"POLISH_PHONE_NUMBER\" : OperatorConfig ( \"custom\" , { \"lambda\" : fake_polish_phone_number } ) } anonymizer . add_operators ( new_operators ) anonymizer . anonymize ( \"My polish phone number is 666555444\" ) 'My polish phone number is 538 521 657' Important considerations ​ Anonymizer detection rates ​ The level of anonymization and the precision of detection are just as\ngood as the quality of the recognizers implemented. Texts from different sources and in different languages have varying\ncharacteristics, so it is necessary to test the detection precision and\niteratively add recognizers and operators to achieve better and better\nresults. Microsoft Presidio gives a lot of freedom to refine anonymization. The\nlibrary’s author has provided his recommendations and a step-by-step\nguide for improving detection\nrates . Instance anonymization ​ PresidioAnonymizer has no built-in memory. Therefore, two occurrences\nof the entity in the subsequent texts will be replaced with two\ndifferent fake values: print ( anonymizer . anonymize ( \"My name is John Doe. Hi John Doe!\" ) ) print ( anonymizer . anonymize ( \"My name is John Doe. Hi John Doe!\" ) ) My name is Robert Morales. Hi Robert Morales! My name is Kelly Mccoy. Hi Kelly Mccoy! To preserve previous anonymization results, use PresidioReversibleAnonymizer , which has built-in memory: from langchain_experimental . data_anonymizer import PresidioReversibleAnonymizer anonymizer_with_memory = PresidioReversibleAnonymizer ( ) print ( anonymizer_with_memory . anonymize ( \"My name is John Doe. Hi John Doe!\" ) ) print ( anonymizer_with_memory . anonymize ( \"My name is John Doe. Hi John Doe!\" ) ) My name is Ashley Cervantes. Hi Ashley Cervantes! My name is Ashley Cervantes. Hi Ashley Cervantes! You can learn more about PresidioReversibleAnonymizer in the next\nsection. Previous Model comparison Next Reversible anonymization"
    },
    {
        "title": "Pydantic compatibility | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/guides/pydantic_compatibility",
        "text_content": "Pydantic compatibility On this page Pydantic compatibility Pydantic v2 was released in June, 2023 ( https://docs.pydantic.dev/2.0/blog/pydantic-v2-final/ ) v2 contains has a number of breaking changes ( https://docs.pydantic.dev/2.0/migration/ ) Pydantic v2 and v1 are under the same package name, so both versions cannot be installed at the same time LangChain Pydantic migration plan ​ As of langchain>=0.0.267 , LangChain will allow users to install either Pydantic V1 or V2. Internally LangChain will continue to use V1 . During this time, users can pin their pydantic version to v1 to avoid breaking changes, or start a partial\nmigration using pydantic v2 throughout their code, but avoiding mixing v1 and v2 code for LangChain (see below). User can either pin to pydantic v1, and upgrade their code in one go once LangChain has migrated to v2 internally, or they can start a partial migration to v2, but must avoid mixing v1 and v2 code for LangChain. Below are two examples of showing how to avoid mixing pydantic v1 and v2 code in\nthe case of inheritance and in the case of passing objects to LangChain. Example 1: Extending via inheritance YES from pydantic . v1 import root_validator , validator class CustomTool ( BaseTool ) : # BaseTool is v1 code x : int = Field ( default = 1 ) def _run ( * args , ** kwargs ) : return \"hello\" @validator ( 'x' ) # v1 code @classmethod def validate_x ( cls , x : int ) - > int : return 1 CustomTool ( name = 'custom_tool' , description = \"hello\" , x = 1 , ) Mixing Pydantic v2 primitives with Pydantic v1 primitives can raise cryptic errors NO from pydantic import Field , field_validator # pydantic v2 class CustomTool ( BaseTool ) : # BaseTool is v1 code x : int = Field ( default = 1 ) def _run ( * args , ** kwargs ) : return \"hello\" @field_validator ( 'x' ) # v2 code @classmethod def validate_x ( cls , x : int ) - > int : return 1 CustomTool ( name = 'custom_tool' , description = \"hello\" , x = 1 , ) Example 2: Passing objects to LangChain YES from langchain_core . tools import Tool from pydantic . v1 import BaseModel , Field # <-- Uses v1 namespace class CalculatorInput ( BaseModel ) : question : str = Field ( ) Tool . from_function ( # <-- tool uses v1 namespace func = lambda question : 'hello' , name = \"Calculator\" , description = \"useful for when you need to answer questions about math\" , args_schema = CalculatorInput ) NO from langchain_core . tools import Tool from pydantic import BaseModel , Field # <-- Uses v2 namespace class CalculatorInput ( BaseModel ) : question : str = Field ( ) Tool . from_function ( # <-- tool uses v1 namespace func = lambda question : 'hello' , name = \"Calculator\" , description = \"useful for when you need to answer questions about math\" , args_schema = CalculatorInput ) Previous QA with private data protection Next Safety"
    },
    {
        "title": "Safety | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/guides/safety/",
        "text_content": "Safety Safety One of the key concerns with using LLMs is that they may generate harmful or unethical text. This is an area of active research in the field. Here we present some built-in chains inspired by this research, which are intended to make the outputs of LLMs safer. Amazon Comprehend moderation chain : Use Amazon Comprehend to detect and handle Personally Identifiable Information (PII) and toxicity. Constitutional chain : Prompt the model with a set of principles which should guide the model behavior. Hugging Face prompt injection identification : Detect and handle prompt injection attacks. Logical Fallacy chain : Checks the model output against logical fallacies to correct any deviation. Moderation chain : Check if any output text is harmful and flag it. Previous Pydantic compatibility Next Amazon Comprehend Moderation Chain"
    },
    {
        "title": "WandB Tracing | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/providers/wandb_tracing",
        "text_content": "Providers More WandB Tracing WandB Tracing There are two recommended ways to trace your LangChains: Setting the LANGCHAIN_WANDB_TRACING environment variable to\n“true”. Using a context manager with tracing_enabled() to trace a particular\nblock of code. Note if the environment variable is set, all code will be traced,\nregardless of whether or not it’s within the context manager. import os os . environ [ \"LANGCHAIN_WANDB_TRACING\" ] = \"true\" # wandb documentation to configure wandb using env variables # https://docs.wandb.ai/guides/track/advanced/environment-variables # here we are configuring the wandb project name os . environ [ \"WANDB_PROJECT\" ] = \"langchain-tracing\" from langchain . agents import AgentType , initialize_agent , load_tools from langchain . callbacks import wandb_tracing_enabled from langchain . llms import OpenAI # Agent run with tracing. Ensure that OPENAI_API_KEY is set appropriately to run this example. llm = OpenAI ( temperature = 0 ) tools = load_tools ( [ \"llm-math\" ] , llm = llm ) agent = initialize_agent ( tools , llm , agent = AgentType . ZERO_SHOT_REACT_DESCRIPTION , verbose = True ) agent . run ( \"What is 2 raised to .123243 power?\" ) # this should be traced # A url with for the trace sesion like the following should print in your console: # https://wandb.ai/<wandb_entity>/<wandb_project>/runs/<run_id> # The url can be used to view the trace session in wandb. # Now, we unset the environment variable and use a context manager. if \"LANGCHAIN_WANDB_TRACING\" in os . environ : del os . environ [ \"LANGCHAIN_WANDB_TRACING\" ] # enable tracing using a context manager with wandb_tracing_enabled ( ) : agent . run ( \"What is 5 raised to .123243 power?\" ) # this should be traced agent . run ( \"What is 2 raised to .123243 power?\" ) # this should not be traced > Entering new AgentExecutor chain... I need to use a calculator to solve this. Action: Calculator Action Input: 5^.123243 Observation: Answer: 1.2193914912400514 Thought: I now know the final answer. Final Answer: 1.2193914912400514 > Finished chain. > Entering new AgentExecutor chain... I need to use a calculator to solve this. Action: Calculator Action Input: 2^.123243 Observation: Answer: 1.0891804557407723 Thought: I now know the final answer. Final Answer: 1.0891804557407723 > Finished chain. '1.0891804557407723' Previous Vespa Next Weights & Biases"
    },
    {
        "title": "Callbacks | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/callbacks/",
        "text_content": "Modules More Callbacks On this page Callbacks info Head to Integrations for documentation on built-in callbacks integrations with 3rd-party tools. LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks. You can subscribe to these events by using the callbacks argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail. Callback handlers ​ CallbackHandlers are objects that implement the CallbackHandler interface, which has a method for each event that can be subscribed to. The CallbackManager will call the appropriate method on each handler when the event is triggered. class BaseCallbackHandler : \"\"\"Base callback handler that can be used to handle callbacks from langchain.\"\"\" def on_llm_start ( self , serialized : Dict [ str , Any ] , prompts : List [ str ] , ** kwargs : Any ) - > Any : \"\"\"Run when LLM starts running.\"\"\" def on_chat_model_start ( self , serialized : Dict [ str , Any ] , messages : List [ List [ BaseMessage ] ] , ** kwargs : Any ) - > Any : \"\"\"Run when Chat Model starts running.\"\"\" def on_llm_new_token ( self , token : str , ** kwargs : Any ) - > Any : \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\" def on_llm_end ( self , response : LLMResult , ** kwargs : Any ) - > Any : \"\"\"Run when LLM ends running.\"\"\" def on_llm_error ( self , error : Union [ Exception , KeyboardInterrupt ] , ** kwargs : Any ) - > Any : \"\"\"Run when LLM errors.\"\"\" def on_chain_start ( self , serialized : Dict [ str , Any ] , inputs : Dict [ str , Any ] , ** kwargs : Any ) - > Any : \"\"\"Run when chain starts running.\"\"\" def on_chain_end ( self , outputs : Dict [ str , Any ] , ** kwargs : Any ) - > Any : \"\"\"Run when chain ends running.\"\"\" def on_chain_error ( self , error : Union [ Exception , KeyboardInterrupt ] , ** kwargs : Any ) - > Any : \"\"\"Run when chain errors.\"\"\" def on_tool_start ( self , serialized : Dict [ str , Any ] , input_str : str , ** kwargs : Any ) - > Any : \"\"\"Run when tool starts running.\"\"\" def on_tool_end ( self , output : str , ** kwargs : Any ) - > Any : \"\"\"Run when tool ends running.\"\"\" def on_tool_error ( self , error : Union [ Exception , KeyboardInterrupt ] , ** kwargs : Any ) - > Any : \"\"\"Run when tool errors.\"\"\" def on_text ( self , text : str , ** kwargs : Any ) - > Any : \"\"\"Run on arbitrary text.\"\"\" def on_agent_action ( self , action : AgentAction , ** kwargs : Any ) - > Any : \"\"\"Run on agent action.\"\"\" def on_agent_finish ( self , finish : AgentFinish , ** kwargs : Any ) - > Any : \"\"\"Run on agent end.\"\"\" Get started ​ LangChain provides a few built-in handlers that you can use to get started. These are available in the langchain/callbacks module. The most basic handler is the StdOutCallbackHandler , which simply logs all events to stdout . Note : when the verbose flag on the object is set to true, the StdOutCallbackHandler will be invoked even without being explicitly passed in. from langchain . callbacks import StdOutCallbackHandler from langchain . chains import LLMChain from langchain . llms import OpenAI from langchain . prompts import PromptTemplate handler = StdOutCallbackHandler ( ) llm = OpenAI ( ) prompt = PromptTemplate . from_template ( \"1 + {number} = \" ) # Constructor callback: First, let's explicitly set the StdOutCallbackHandler when initializing our chain chain = LLMChain ( llm = llm , prompt = prompt , callbacks = [ handler ] ) chain . run ( number = 2 ) # Use verbose flag: Then, let's use the `verbose` flag to achieve the same result chain = LLMChain ( llm = llm , prompt = prompt , verbose = True ) chain . run ( number = 2 ) # Request callbacks: Finally, let's use the request `callbacks` to achieve the same result chain = LLMChain ( llm = llm , prompt = prompt ) chain . run ( number = 2 , callbacks = [ handler ] ) > Entering new LLMChain chain... Prompt after formatting: 1 + 2 = > Finished chain. > Entering new LLMChain chain... Prompt after formatting: 1 + 2 = > Finished chain. > Entering new LLMChain chain... Prompt after formatting: 1 + 2 = > Finished chain. '\\n\\n3' Where to pass in callbacks ​ The callbacks argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) in two different places: Constructor callbacks : defined in the constructor, e.g. LLMChain(callbacks=[handler], tags=['a-tag']) , which will be used for all calls made on that object, and will be scoped to that object only, e.g. if you pass a handler to the LLMChain constructor, it will not be used by the Model attached to that chain. Request callbacks : defined in the run() / apply() methods used for issuing a request, e.g. chain.run(input, callbacks=[handler]) , which will be used for that specific request only, and all sub-requests that it contains (e.g. a call to an LLMChain triggers a call to a Model, which uses the same handler passed in the call() method). The verbose argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) as a constructor argument, e.g. LLMChain(verbose=True) , and it is equivalent to passing a ConsoleCallbackHandler to the callbacks argument of that object and all child objects. This is useful for debugging, as it will log all events to the console. When do you want to use each of these? ​ Constructor callbacks are most useful for use cases such as logging, monitoring, etc., which are not specific to a single request , but rather to the entire chain. For example, if you want to log all the requests made to an LLMChain , you would pass a handler to the constructor. Request callbacks are most useful for use cases such as streaming, where you want to stream the output of a single request to a specific websocket connection, or other similar use cases. For example, if you want to stream the output of a single request to a websocket, you would pass a handler to the call() method Previous Multiple Memory classes Next Callbacks"
    },
    {
        "title": "anthropic-iterative-search | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/anthropic-iterative-search",
        "text_content": "Templates anthropic-iterative-search On this page anthropic-iterative-search This template will create a virtual research assistant with the ability to search Wikipedia to find answers to your questions. It is heavily inspired by this notebook . Environment Setup ​ Set the ANTHROPIC_API_KEY environment variable to access the Anthropic models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package anthropic-iterative-search If you want to add this to an existing project, you can just run: langchain app add anthropic-iterative-search And add the following code to your server.py file: from anthropic_iterative_search import chain as anthropic_iterative_search_chain add_routes ( app , anthropic_iterative_search_chain , path = \"/anthropic-iterative-search\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/anthropic-iterative-search/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/anthropic-iterative-search\" ) Previous Templates Next basic-critique-revise"
    },
    {
        "title": "basic-critique-revise | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/basic-critique-revise",
        "text_content": "Templates basic-critique-revise On this page basic-critique-revise Iteratively generate schema candidates and revise them based on errors. Environment Setup ​ This template uses OpenAI function calling, so you will need to set the OPENAI_API_KEY environment variable in order to use this template. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U \"langchain-cli[serve]\" To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package basic-critique-revise If you want to add this to an existing project, you can just run: langchain app add basic-critique-revise And add the following code to your server.py file: from basic_critique_revise import chain as basic_critique_revise_chain add_routes ( app , basic_critique_revise_chain , path = \"/basic-critique-revise\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/basic-critique-revise/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/basic-critique-revise\" ) Previous anthropic-iterative-search Next Bedrock JCVD 🕺🥋"
    },
    {
        "title": "Bedrock JCVD 🕺🥋 | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/bedrock-jcvd",
        "text_content": "Templates Bedrock JCVD 🕺🥋 On this page Bedrock JCVD 🕺🥋 Overview ​ LangChain template that uses Anthropic's Claude on Amazon Bedrock to behave like JCVD. I am the Fred Astaire of Chatbots! 🕺 ' Environment Setup ​ AWS Credentials ​ This template uses Boto3 , the AWS SDK for Python, to call Amazon Bedrock . You must configure both AWS credentials and an AWS Region in order to make requests. For information on how to do this, see AWS Boto3 documentation (Developer Guide > Credentials). Foundation Models ​ By default, this template uses Anthropic's Claude v2 ( anthropic.claude-v2 ). To request access to a specific model, check out the Amazon Bedrock User Guide (Model access) To use a different model, set the environment variable BEDROCK_JCVD_MODEL_ID . A list of base models is available in the Amazon Bedrock User Guide (Use the API > API operations > Run inference > Base Model IDs). The full list of available models (including base and custom models ) is available in the Amazon Bedrock Console under Foundation Models or by calling aws bedrock list-foundation-models . Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package bedrock-jcvd If you want to add this to an existing project, you can just run: langchain app add bedrock-jcvd And add the following code to your server.py file: from bedrock_jcvd import chain as bedrock_jcvd_chain add_routes ( app , bedrock_jcvd_chain , path = \"/bedrock-jcvd\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs . We can also access the playground at http://127.0.0.1:8000/bedrock-jcvd/playground Previous basic-critique-revise Next cassandra-entomology-rag"
    },
    {
        "title": "cassandra-entomology-rag | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/cassandra-entomology-rag",
        "text_content": "Templates cassandra-entomology-rag On this page cassandra-entomology-rag This template will perform RAG using Apache Cassandra® or Astra DB through CQL ( Cassandra vector store class) Environment Setup ​ For the setup, you will require: an Astra Vector Database. You must have a Database Administrator token , specifically the string starting with AstraCS:... . Database ID . an OpenAI API Key . (More info here ) You may also use a regular Cassandra cluster. In this case, provide the USE_CASSANDRA_CLUSTER entry as shown in .env.template and the subsequent environment variables to specify how to connect to it. The connection parameters and secrets must be provided through environment variables. Refer to .env.template for the required variables. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package cassandra-entomology-rag If you want to add this to an existing project, you can just run: langchain app add cassandra-entomology-rag And add the following code to your server.py file: from cassandra_entomology_rag import chain as cassandra_entomology_rag_chain add_routes ( app , cassandra_entomology_rag_chain , path = \"/cassandra-entomology-rag\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/cassandra-entomology-rag/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/cassandra-entomology-rag\" ) Reference ​ Stand-alone repo with LangServe chain: here . Previous Bedrock JCVD 🕺🥋 Next cassandra-synonym-caching"
    },
    {
        "title": "cassandra-synonym-caching | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/cassandra-synonym-caching",
        "text_content": "Templates cassandra-synonym-caching On this page cassandra-synonym-caching This template provides a simple chain template showcasing the usage of LLM Caching backed by Apache Cassandra® or Astra DB through CQL. Environment Setup ​ To set up your environment, you will need the following: an Astra Vector Database (free tier is fine!). You need a Database Administrator token , in particular the string starting with AstraCS:... ; likewise, get your Database ID ready, you will have to enter it below; an OpenAI API Key . (More info here , note that out-of-the-box this demo supports OpenAI unless you tinker with the code.) Note: you can alternatively use a regular Cassandra cluster: to do so, make sure you provide the USE_CASSANDRA_CLUSTER entry as shown in .env.template and the subsequent environment variables to specify how to connect to it. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package cassandra-synonym-caching If you want to add this to an existing project, you can just run: langchain app add cassandra-synonym-caching And add the following code to your server.py file: from cassandra_synonym_caching import chain as cassandra_synonym_caching_chain add_routes ( app , cassandra_synonym_caching_chain , path = \"/cassandra-synonym-caching\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/cassandra-synonym-caching/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/cassandra-synonym-caching\" ) Reference ​ Stand-alone LangServe template repo: here . Previous cassandra-entomology-rag Next Chain-of-Note (Wikipedia)"
    },
    {
        "title": "Chain-of-Note (Wikipedia) | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/chain-of-note-wiki",
        "text_content": "Templates Chain-of-Note (Wikipedia) On this page Chain-of-Note (Wikipedia) Implements Chain-of-Note as described in https://arxiv.org/pdf/2311.09210.pdf by Yu, et al. Uses Wikipedia for retrieval. Check out the prompt being used here https://smith.langchain.com/hub/bagatur/chain-of-note-wiki . Environment Setup ​ Uses Anthropic claude-2 chat model. Set Anthropic API key: export ANTHROPIC_API_KEY = \"...\" Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U \"langchain-cli[serve]\" To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package chain-of-note-wiki If you want to add this to an existing project, you can just run: langchain app add chain-of-note-wiki And add the following code to your server.py file: from chain_of_note_wiki import chain as chain_of_note_wiki_chain add_routes ( app , chain_of_note_wiki_chain , path = \"/chain-of-note-wiki\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/chain-of-note-wiki/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/chain-of-note-wiki\" ) Previous cassandra-synonym-caching Next Chat Bot Feedback Template"
    },
    {
        "title": "Chat Bot Feedback Template | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/chat-bot-feedback",
        "text_content": "Templates Chat Bot Feedback Template On this page Chat Bot Feedback Template This template shows how to evaluate your chat bot without explicit user feedback. It defines a simple chat bot in chain.py and custom evaluator that scores bot response effectiveness based on the subsequent user response. You can apply this run evaluator to your own chat bot by calling with_config on the chat bot before serving. You can also directly deploy your chat app using this template. Chat bots are one of the most common interfaces for deploying LLMs. The quality of chat bots varies, making continuous development important. But users are wont to leave explicit feedback through mechanisms like thumbs-up or thumbs-down buttons. Furthermore, traditional analytics such as \"session length\" or \"conversation length\" often lack clarity. However, multi-turn conversations with a chat bot can provide a wealth of information, which we can transform into metrics for fine-tuning, evaluation, and product analytics. Taking Chat Langchain as a case study, only about 0.04% of all queries receive explicit feedback. Yet, approximately 70% of the queries are follow-ups to previous questions. A significant portion of these follow-up queries continue useful information we can use to infer the quality of the previous AI response. This template helps solve this \"feedback scarcity\" problem. Below is an example invocation of this chat bot: When the user responds to this ( link ), the response evaluator is invoked, resulting in the following evaluationrun: As shown, the evaluator sees that the user is increasingly frustrated, indicating that the prior response was not effective LangSmith Feedback ​ LangSmith is a platform for building production-grade LLM applications. Beyond its debugging and offline evaluation features, LangSmith helps you capture both user and model-assisted feedback to refine your LLM application. This template uses an LLM to generate feedback for your application, which you can use to continuously improve your service. For more examples on collecting feedback using LangSmith, consult the documentation . Evaluator Implementation ​ The user feedback is inferred by custom RunEvaluator . This evaluator is called using the EvaluatorCallbackHandler , which run it in a separate thread to avoid interfering with the chat bot's runtime. You can use this custom evaluator on any compatible chat bot by calling the following function on your LangChain object: my_chain . with_config ( callbacks = [ EvaluatorCallbackHandler ( evaluators = [ ResponseEffectivenessEvaluator ( evaluate_response_effectiveness ) ] ) ] , ) The evaluator instructs an LLM, specifically gpt-3.5-turbo , to evaluate the AI's most recent chat message based on the user's followup response. It generates a score and accompanying reasoning that is converted to feedback in LangSmith, applied to the value provided as the last_run_id . The prompt used within the LLM is available on the hub . Feel free to customize it with things like additional app context (such as the goal of the app or the types of questions it should respond to) or \"symptoms\" you'd like the LLM to focus on. This evaluator also utilizes OpenAI's function-calling API to ensure a more consistent, structured output for the grade. Environment Variables ​ Ensure that OPENAI_API_KEY is set to use OpenAI models. Also, configure LangSmith by setting your LANGSMITH_API_KEY . export OPENAI_API_KEY = sk- .. . export LANGSMITH_API_KEY = .. . export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_PROJECT = my-project # Set to the project you want to save to Usage ​ If deploying via LangServe , we recommend configuring the server to return callback events as well. This will ensure the backend traces are included in whatever traces you generate using the RemoteRunnable . from chat_bot_feedback . chain import chain add_routes ( app , chain , path = \"/chat-bot-feedback\" , include_callback_events = True ) With the server running, you can use the following code snippet to stream the chat bot responses for a 2 turn conversation. from functools import partial from typing import Dict , Optional , Callable , List from langserve import RemoteRunnable from langchain . callbacks . manager import tracing_v2_enabled from langchain . schema import BaseMessage , AIMessage , HumanMessage # Update with the URL provided by your LangServe server chain = RemoteRunnable ( \"http://127.0.0.1:8031/chat-bot-feedback\" ) def stream_content ( text : str , chat_history : Optional [ List [ BaseMessage ] ] = None , last_run_id : Optional [ str ] = None , on_chunk : Callable = None , ) : results = [ ] with tracing_v2_enabled ( ) as cb : for chunk in chain . stream ( { \"text\" : text , \"chat_history\" : chat_history , \"last_run_id\" : last_run_id } , ) : on_chunk ( chunk ) results . append ( chunk ) last_run_id = cb . latest_run . id if cb . latest_run else None return last_run_id , \"\" . join ( results ) chat_history = [ ] text = \"Where are my keys?\" last_run_id , response_message = stream_content ( text , on_chunk = partial ( print , end = \"\" ) ) print ( ) chat_history . extend ( [ HumanMessage ( content = text ) , AIMessage ( content = response_message ) ] ) text = \"I CAN'T FIND THEM ANYWHERE\" # The previous response will likely receive a low score, # as the user's frustration appears to be escalating. last_run_id , response_message = stream_content ( text , chat_history = chat_history , last_run_id = str ( last_run_id ) , on_chunk = partial ( print , end = \"\" ) , ) print ( ) chat_history . extend ( [ HumanMessage ( content = text ) , AIMessage ( content = response_message ) ] ) This uses the tracing_v2_enabled callback manager to get the run ID of the call, which we provide in subsequent calls in the same chat thread, so the evaluator can assign feedback to the appropriate trace. Conclusion ​ This template provides a simple chat bot definition you can directly deploy using LangServe. It defines a custom evaluator to log evaluation feedback for the bot without any explicit user ratings. This is an effective way to augment your analytics and to better select data points for fine-tuning and evaluation. Previous Chain-of-Note (Wikipedia) Next cohere-librarian"
    },
    {
        "title": "cohere-librarian | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/cohere-librarian",
        "text_content": "Templates cohere-librarian On this page cohere-librarian This template turns Cohere into a librarian. It demonstrates the use of a router to switch between chains that can handle different things: a vector database with Cohere embeddings; a chat bot that has a prompt with some information about the library; and finally a RAG chatbot that has access to the internet. For a fuller demo of the book recomendation, consider replacing books_with_blurbs.csv with a larger sample from the following dataset: https://www.kaggle.com/datasets/jdobrow/57000-books-with-metadata-and-blurbs/ . Environment Setup ​ Set the COHERE_API_KEY environment variable to access the Cohere models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package cohere-librarian If you want to add this to an existing project, you can just run: langchain app add cohere-librarian And add the following code to your server.py file: from cohere_librarian . chain import chain as cohere_librarian_chain add_routes ( app , cohere_librarian_chain , path = \"/cohere-librarian\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://localhost:8000/docs We can access the playground at http://localhost:8000/cohere-librarian/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/cohere-librarian\" ) Previous Chat Bot Feedback Template Next csv-agent"
    },
    {
        "title": "csv-agent | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/csv-agent",
        "text_content": "Templates csv-agent On this page csv-agent This template uses a csv agent with tools (Python REPL) and memory (vectorstore) for interaction (question-answering) with text data. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. To set up the environment, the ingest.py script should be run to handle the ingestion into a vectorstore. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package csv-agent If you want to add this to an existing project, you can just run: langchain app add csv-agent And add the following code to your server.py file: from csv_agent . agent import agent_executor as csv_agent_chain add_routes ( app , csv_agent_chain , path = \"/csv-agent\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/csv-agent/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/csv-agent\" ) Previous cohere-librarian Next elastic-query-generator"
    },
    {
        "title": "elastic-query-generator | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/elastic-query-generator",
        "text_content": "Templates elastic-query-generator On this page elastic-query-generator This template allows interacting with Elasticsearch analytics databases in natural language using LLMs. It builds search queries via the Elasticsearch DSL API (filters and aggregations). Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. Installing Elasticsearch ​ There are a number of ways to run Elasticsearch. However, one recommended way is through Elastic Cloud. Create a free trial account on Elastic Cloud . With a deployment, update the connection string. Password and connection (elasticsearch url) can be found on the deployment console. Note that the Elasticsearch client must have permissions for index listing, mapping description, and search queries. Populating with data ​ If you want to populate the DB with some example info, you can run python ingest.py . This will create a customers index. In this package, we specify indexes to generate queries against, and we specify [\"customers\"] . This is specific to setting up your Elastic index. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package elastic-query-generator If you want to add this to an existing project, you can just run: langchain app add elastic-query-generator And add the following code to your server.py file: from elastic_query_generator . chain import chain as elastic_query_generator_chain add_routes ( app , elastic_query_generator_chain , path = \"/elastic-query-generator\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/elastic-query-generator/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/elastic-query-generator\" ) Previous csv-agent Next extraction-anthropic-functions"
    },
    {
        "title": "extraction-anthropic-functions | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/extraction-anthropic-functions",
        "text_content": "Templates extraction-anthropic-functions On this page extraction-anthropic-functions This template enables Anthropic function calling . This can be used for various tasks, such as extraction or tagging. The function output schema can be set in chain.py . Environment Setup ​ Set the ANTHROPIC_API_KEY environment variable to access the Anthropic models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package extraction-anthropic-functions If you want to add this to an existing project, you can just run: langchain app add extraction-anthropic-functions And add the following code to your server.py file: from extraction_anthropic_functions import chain as extraction_anthropic_functions_chain add_routes ( app , extraction_anthropic_functions_chain , path = \"/extraction-anthropic-functions\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/extraction-anthropic-functions/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/extraction-anthropic-functions\" ) By default, the package will extract the title and author of papers from the information you specify in chain.py . This template will use Claude2 by default. Previous elastic-query-generator Next extraction-openai-functions"
    },
    {
        "title": "extraction-openai-functions | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/extraction-openai-functions",
        "text_content": "Templates extraction-openai-functions On this page extraction-openai-functions This template uses OpenAI function calling for extraction of structured output from unstructured input text. The extraction output schema can be set in chain.py . Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package extraction-openai-functions If you want to add this to an existing project, you can just run: langchain app add extraction-openai-functions And add the following code to your server.py file: from extraction_openai_functions import chain as extraction_openai_functions_chain add_routes ( app , extraction_openai_functions_chain , path = \"/extraction-openai-functions\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/extraction-openai-functions/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/extraction-openai-functions\" ) By default, this package is set to extract the title and author of papers, as specified in the chain.py file. LLM is leveraged by the OpenAI function by default. Previous extraction-anthropic-functions Next guardrails-output-parser"
    },
    {
        "title": "guardrails-output-parser | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/guardrails-output-parser",
        "text_content": "Templates guardrails-output-parser On this page guardrails-output-parser This template uses guardrails-ai to validate LLM output. The GuardrailsOutputParser is set in chain.py . The default example protects against profanity. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package guardrails-output-parser If you want to add this to an existing project, you can just run: langchain app add guardrails-output-parser And add the following code to your server.py file: from guardrails_output_parser . chain import chain as guardrails_output_parser_chain add_routes ( app , guardrails_output_parser_chain , path = \"/guardrails-output-parser\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/guardrails-output-parser/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/guardrails-output-parser\" ) If Guardrails does not find any profanity, then the translated output is returned as is. If Guardrails does find profanity, then an empty string is returned. Previous extraction-openai-functions Next Hybrid Search in Weaviate"
    },
    {
        "title": "Hybrid Search in Weaviate | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/hybrid-search-weaviate",
        "text_content": "Templates Hybrid Search in Weaviate On this page Hybrid Search in Weaviate This template shows you how to use the hybrid search feature in Weaviate. Hybrid search combines multiple search algorithms to improve the accuracy and relevance of search results. Weaviate uses both sparse and dense vectors to represent the meaning and context of search queries and documents. The results use a combination of bm25 and vector search ranking to return the top results. Configurations ​ Connect to your hosted Weaviate Vectorstore by setting a few env variables in chain.py : WEAVIATE_ENVIRONMENT WEAVIATE_API_KEY You will also need to set your OPENAI_API_KEY to use the OpenAI models. Get Started ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package hybrid-search-weaviate If you want to add this to an existing project, you can just run: langchain app add hybrid-search-weaviate And add the following code to your server.py file: from hybrid_search_weaviate import chain as hybrid_search_weaviate_chain add_routes ( app , hybrid_search_weaviate_chain , path = \"/hybrid-search-weaviate\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/hybrid-search-weaviate/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/hybrid-search-weaviate\" ) Previous guardrails-output-parser Next hyde"
    },
    {
        "title": "hyde | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/hyde",
        "text_content": "Templates hyde On this page hyde This template HyDE with RAG. Hyde is a retrieval method that stands for Hypothetical Document Embeddings (HyDE). It is a method used to enhance retrieval by generating a hypothetical document for an incoming query. The document is then embedded, and that embedding is utilized to look up real documents that are similar to the hypothetical document. The underlying concept is that the hypothetical document may be closer in the embedding space than the query. For a more detailed description, see the paper here . Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package hyde If you want to add this to an existing project, you can just run: langchain app add hyde And add the following code to your server.py file: from hyde . chain import chain as hyde_chain add_routes ( app , hyde_chain , path = \"/hyde\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/hyde/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/hyde\" ) Previous Hybrid Search in Weaviate Next Templates"
    },
    {
        "title": "llama2-functions | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/llama2-functions",
        "text_content": "Templates llama2-functions On this page llama2-functions This template performs extraction of structured data from unstructured data using a LLaMA2 model that supports a specified JSON output schema . The extraction schema can be set in chain.py . Environment Setup ​ This will use a LLaMA2-13b model hosted by Replicate . Ensure that REPLICATE_API_TOKEN is set in your environment. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package llama2-functions If you want to add this to an existing project, you can just run: langchain app add llama2-functions And add the following code to your server.py file: from llama2_functions import chain as llama2_functions_chain add_routes ( app , llama2_functions_chain , path = \"/llama2-functions\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/llama2-functions/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/llama2-functions\" ) Previous Templates Next mongo-parent-document-retrieval"
    },
    {
        "title": "mongo-parent-document-retrieval | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/mongo-parent-document-retrieval",
        "text_content": "Templates mongo-parent-document-retrieval On this page mongo-parent-document-retrieval This template performs RAG using MongoDB and OpenAI.\nIt does a more advanced form of RAG called Parent-Document Retrieval. In this form of retrieval, a large document is first split into medium sized chunks.\nFrom there, those medium size chunks are split into small chunks.\nEmbeddings are created for the small chunks.\nWhen a query comes in, an embedding is created for that query and compared to the small chunks.\nBut rather than passing the small chunks directly to the LLM for generation, the medium-sized chunks\nfrom whence the smaller chunks came are passed.\nThis helps enable finer-grained search, but then passing of larger context (which can be useful during generation). Environment Setup ​ You should export two environment variables, one being your MongoDB URI, the other being your OpenAI API KEY.\nIf you do not have a MongoDB URI, see the Setup Mongo section at the bottom for instructions on how to do so. export MONGO_URI = .. . export OPENAI_API_KEY = .. . Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package mongo-parent-document-retrieval If you want to add this to an existing project, you can just run: langchain app add mongo-parent-document-retrieval And add the following code to your server.py file: from mongo_parent_document_retrieval import chain as mongo_parent_document_retrieval_chain add_routes ( app , mongo_parent_document_retrieval_chain , path = \"/mongo-parent-document-retrieval\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you DO NOT already have a Mongo Search Index you want to connect to, see MongoDB Setup section below before proceeding.\nNote that because Parent Document Retrieval uses a different indexing strategy, it's likely you will want to run this new setup. If you DO have a MongoDB Search index you want to connect to, edit the connection details in mongo_parent_document_retrieval/chain.py If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/mongo-parent-document-retrieval/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/mongo-parent-document-retrieval\" ) For additional context, please refer to this notebook . MongoDB Setup ​ Use this step if you need to setup your MongoDB account and ingest data.\nWe will first follow the standard MongoDB Atlas setup instructions here . Create an account (if not already done) Create a new project (if not already done) Locate your MongoDB URI. This can be done by going to the deployement overview page and connecting to you database We then look at the drivers available Among which we will see our URI listed Let's then set that as an environment variable locally: export MONGO_URI = .. . Let's also set an environment variable for OpenAI (which we will use as an LLM) export OPENAI_API_KEY = .. . Let's now ingest some data! We can do that by moving into this directory and running the code in ingest.py , eg: python ingest.py Note that you can (and should!) change this to ingest data of your choice We now need to set up a vector index on our data. We can first connect to the cluster where our database lives We can then navigate to where all our collections are listed We can then find the collection we want and look at the search indexes for that collection That should likely be empty, and we want to create a new one: We will use the JSON editor to create it And we will paste the following JSON in: { \"mappings\": { \"dynamic\": true, \"fields\": { \"doc_level\": [ { \"type\": \"token\" } ], \"embedding\": { \"dimensions\": 1536, \"similarity\": \"cosine\", \"type\": \"knnVector\" } } } } From there, hit \"Next\" and then \"Create Search Index\". It will take a little bit but you should then have an index over your data! Previous llama2-functions Next neo4j-advanced-rag"
    },
    {
        "title": "neo4j-advanced-rag | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/neo4j-advanced-rag",
        "text_content": "Templates neo4j-advanced-rag On this page neo4j-advanced-rag This template allows you to balance precise embeddings and context retention by implementing advanced retrieval strategies. Strategies ​ Typical RAG : Traditional method where the exact data indexed is the data retrieved. Parent retriever : Instead of indexing entire documents, data is divided into smaller chunks, referred to as Parent and Child documents. Child documents are indexed for better representation of specific concepts, while parent documents is retrieved to ensure context retention. Hypothetical Questions : Documents are processed to determine potential questions they might answer. These questions are then indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention. Summaries : Instead of indexing the entire document, a summary of the document is created and indexed. Similarly, the parent document is retrieved in a RAG application. Environment Setup ​ You need to define the following environment variables OPENAI_API_KEY=<YOUR_OPENAI_API_KEY> NEO4J_URI=<YOUR_NEO4J_URI> NEO4J_USERNAME=<YOUR_NEO4J_USERNAME> NEO4J_PASSWORD=<YOUR_NEO4J_PASSWORD> Populating with data ​ If you want to populate the DB with some example data, you can run python ingest.py .\nThe script process and stores sections of the text from the file dune.txt into a Neo4j graph database.\nFirst, the text is divided into larger chunks (\"parents\") and then further subdivided into smaller chunks (\"children\"), where both parent and child chunks overlap slightly to maintain context.\nAfter storing these chunks in the database, embeddings for the child nodes are computed using OpenAI's embeddings and stored back in the graph for future retrieval or analysis.\nFor every parent node, hypothetical questions and summaries are generated, embedded, and added to the database.\nAdditionally, a vector index for each retrieval strategy is created for efficient querying of these embeddings. Note that ingestion can take a minute or two due to LLMs velocity of generating hypothetical questions and summaries. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U \"langchain-cli[serve]\" To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package neo4j-advanced-rag If you want to add this to an existing project, you can just run: langchain app add neo4j-advanced-rag And add the following code to your server.py file: from neo4j_advanced_rag import chain as neo4j_advanced_chain add_routes ( app , neo4j_advanced_chain , path = \"/neo4j-advanced-rag\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/neo4j-advanced-rag/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/neo4j-advanced-rag\" ) Previous mongo-parent-document-retrieval Next neo4j-cypher-ft"
    },
    {
        "title": "neo4j-cypher-ft | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/neo4j-cypher-ft",
        "text_content": "Templates neo4j-cypher-ft On this page neo4j-cypher-ft This template allows you to interact with a Neo4j graph database using natural language, leveraging OpenAI's LLM. Its main function is to convert natural language questions into Cypher queries (the language used to query Neo4j databases), execute these queries, and provide natural language responses based on the query's results. The package utilizes a full-text index for efficient mapping of text values to database entries, thereby enhancing the generation of accurate Cypher statements. In the provided example, the full-text index is used to map names of people and movies from the user's query to corresponding database entries. Environment Setup ​ The following environment variables need to be set: OPENAI_API_KEY=<YOUR_OPENAI_API_KEY> NEO4J_URI=<YOUR_NEO4J_URI> NEO4J_USERNAME=<YOUR_NEO4J_USERNAME> NEO4J_PASSWORD=<YOUR_NEO4J_PASSWORD> Additionally, if you wish to populate the DB with some example data, you can run python ingest.py .\nThis script will populate the database with sample movie data and create a full-text index named entity , which is used to map person and movies from user input to database values for precise Cypher statement generation. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package neo4j-cypher-ft If you want to add this to an existing project, you can just run: langchain app add neo4j-cypher-ft And add the following code to your server.py file: from neo4j_cypher_ft import chain as neo4j_cypher_ft_chain add_routes ( app , neo4j_cypher_ft_chain , path = \"/neo4j-cypher-ft\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/neo4j-cypher-ft/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/neo4j-cypher-ft\" ) Previous neo4j-advanced-rag Next neo4j-cypher-memory"
    },
    {
        "title": "neo4j-cypher-memory | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/neo4j-cypher-memory",
        "text_content": "Templates neo4j-cypher-memory On this page neo4j-cypher-memory This template allows you to have conversations with a Neo4j graph database in natural language, using an OpenAI LLM.\nIt transforms a natural language question into a Cypher query (used to fetch data from Neo4j databases), executes the query, and provides a natural language response based on the query results.\nAdditionally, it features a conversational memory module that stores the dialogue history in the Neo4j graph database.\nThe conversation memory is uniquely maintained for each user session, ensuring personalized interactions.\nTo facilitate this, please supply both the user_id and session_id when using the conversation chain. Environment Setup ​ Define the following environment variables: OPENAI_API_KEY=<YOUR_OPENAI_API_KEY> NEO4J_URI=<YOUR_NEO4J_URI> NEO4J_USERNAME=<YOUR_NEO4J_USERNAME> NEO4J_PASSWORD=<YOUR_NEO4J_PASSWORD> Neo4j database setup ​ There are a number of ways to set up a Neo4j database. Neo4j Aura ​ Neo4j AuraDB is a fully managed cloud graph database service.\nCreate a free instance on Neo4j Aura .\nWhen you initiate a free database instance, you'll receive credentials to access the database. Populating with data ​ If you want to populate the DB with some example data, you can run python ingest.py .\nThis script will populate the database with sample movie data. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package neo4j-cypher-memory If you want to add this to an existing project, you can just run: langchain app add neo4j-cypher-memory And add the following code to your server.py file: from neo4j_cypher_memory import chain as neo4j_cypher_memory_chain add_routes ( app , neo4j_cypher_memory_chain , path = \"/neo4j-cypher-memory\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/neo4j_cypher_memory/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/neo4j-cypher-memory\" ) Previous neo4j-cypher-ft Next neo4j_cypher"
    },
    {
        "title": "neo4j_cypher | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/neo4j-cypher",
        "text_content": "Templates neo4j_cypher On this page neo4j_cypher This template allows you to interact with a Neo4j graph database in natural language, using an OpenAI LLM. It transforms a natural language question into a Cypher query (used to fetch data from Neo4j databases), executes the query, and provides a natural language response based on the query results. Environment Setup ​ Define the following environment variables: OPENAI_API_KEY=<YOUR_OPENAI_API_KEY> NEO4J_URI=<YOUR_NEO4J_URI> NEO4J_USERNAME=<YOUR_NEO4J_USERNAME> NEO4J_PASSWORD=<YOUR_NEO4J_PASSWORD> Neo4j database setup ​ There are a number of ways to set up a Neo4j database. Neo4j Aura ​ Neo4j AuraDB is a fully managed cloud graph database service.\nCreate a free instance on Neo4j Aura .\nWhen you initiate a free database instance, you'll receive credentials to access the database. Populating with data ​ If you want to populate the DB with some example data, you can run python ingest.py .\nThis script will populate the database with sample movie data. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package neo4j-cypher If you want to add this to an existing project, you can just run: langchain app add neo4j-cypher And add the following code to your server.py file: from neo4j_cypher import chain as neo4j_cypher_chain add_routes ( app , neo4j_cypher_chain , path = \"/neo4j-cypher\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/neo4j_cypher/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/neo4j-cypher\" ) Previous neo4j-cypher-memory Next neo4j-generation"
    },
    {
        "title": "neo4j-generation | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/neo4j-generation",
        "text_content": "Templates neo4j-generation On this page neo4j-generation This template pairs LLM-based knowledge graph extraction with Neo4j AuraDB, a fully managed cloud graph database. You can create a free instance on Neo4j Aura . When you initiate a free database instance, you'll receive credentials to access the database. This template is flexible and allows users to guide the extraction process by specifying a list of node labels and relationship types. For more details on the functionality and capabilities of this package, please refer to this blog post . Environment Setup ​ You need to set the following environment variables: OPENAI_API_KEY=<YOUR_OPENAI_API_KEY> NEO4J_URI=<YOUR_NEO4J_URI> NEO4J_USERNAME=<YOUR_NEO4J_USERNAME> NEO4J_PASSWORD=<YOUR_NEO4J_PASSWORD> Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package neo4j-generation If you want to add this to an existing project, you can just run: langchain app add neo4j-generation And add the following code to your server.py file: from neo4j_generation . chain import chain as neo4j_generation_chain add_routes ( app , neo4j_generation_chain , path = \"/neo4j-generation\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/neo4j-generation/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/neo4j-generation\" ) Previous neo4j_cypher Next neo4j-parent"
    },
    {
        "title": "neo4j-parent | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/neo4j-parent",
        "text_content": "Templates neo4j-parent On this page neo4j-parent This template allows you to balance precise embeddings and context retention by splitting documents into smaller chunks and retrieving their original or larger text information. Using a Neo4j vector index, the package queries child nodes using vector similarity search and retrieves the corresponding parent's text by defining an appropriate retrieval_query parameter. Environment Setup ​ You need to define the following environment variables OPENAI_API_KEY=<YOUR_OPENAI_API_KEY> NEO4J_URI=<YOUR_NEO4J_URI> NEO4J_USERNAME=<YOUR_NEO4J_USERNAME> NEO4J_PASSWORD=<YOUR_NEO4J_PASSWORD> Populating with data ​ If you want to populate the DB with some example data, you can run python ingest.py .\nThe script process and stores sections of the text from the file dune.txt into a Neo4j graph database.\nFirst, the text is divided into larger chunks (\"parents\") and then further subdivided into smaller chunks (\"children\"), where both parent and child chunks overlap slightly to maintain context.\nAfter storing these chunks in the database, embeddings for the child nodes are computed using OpenAI's embeddings and stored back in the graph for future retrieval or analysis.\nAdditionally, a vector index named retrieval is created for efficient querying of these embeddings. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package neo4j-parent If you want to add this to an existing project, you can just run: langchain app add neo4j-parent And add the following code to your server.py file: from neo4j_parent import chain as neo4j_parent_chain add_routes ( app , neo4j_parent_chain , path = \"/neo4j-parent\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/neo4j-parent/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/neo4j-parent\" ) Previous neo4j-generation Next neo4j-vector-memory"
    },
    {
        "title": "neo4j-vector-memory | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/neo4j-vector-memory",
        "text_content": "Templates neo4j-vector-memory On this page neo4j-vector-memory This template allows you to integrate an LLM with a vector-based retrieval system using Neo4j as the vector store.\nAdditionally, it uses the graph capabilities of the Neo4j database to store and retrieve the dialogue history of a specific user's session.\nHaving the dialogue history stored as a graph allows for seamless conversational flows but also gives you the ability to analyze user behavior and text chunk retrieval through graph analytics. Environment Setup ​ You need to define the following environment variables OPENAI_API_KEY=<YOUR_OPENAI_API_KEY> NEO4J_URI=<YOUR_NEO4J_URI> NEO4J_USERNAME=<YOUR_NEO4J_USERNAME> NEO4J_PASSWORD=<YOUR_NEO4J_PASSWORD> Populating with data ​ If you want to populate the DB with some example data, you can run python ingest.py .\nThe script process and stores sections of the text from the file dune.txt into a Neo4j graph database.\nAdditionally, a vector index named dune is created for efficient querying of these embeddings. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package neo4j-vector-memory If you want to add this to an existing project, you can just run: langchain app add neo4j-vector-memory And add the following code to your server.py file: from neo4j_vector_memory import chain as neo4j_vector_memory_chain add_routes ( app , neo4j_vector_memory_chain , path = \"/neo4j-vector-memory\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/neo4j-vector-memory/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/neo4j-vector-memory\" ) Previous neo4j-parent Next OpenAI Functions Agent - Gmail"
    },
    {
        "title": "OpenAI Functions Agent - Gmail | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/openai-functions-agent-gmail",
        "text_content": "Templates OpenAI Functions Agent - Gmail On this page OpenAI Functions Agent - Gmail Ever struggled to reach inbox zero? Using this template, you can create and customize your very own AI assistant to manage your Gmail account. Using the default Gmail tools, it can read, search through, and draft emails to respond on your behalf. It also has access to a Tavily search engine so it can search for relevant information about any topics or people in the email thread before writing, ensuring the drafts include all the relevant information needed to sound well-informed. The details ​ This assistant uses OpenAI's function calling support to reliably select and invoke the tools you've provided This template also imports directly from langchain-core and langchain-community where appropriate. We have restructured LangChain to let you select the specific integrations needed for your use case. While you can still import from langchain (we are making this transition backwards-compatible), we have separated the homes of most of the classes to reflect ownership and to make your dependency lists lighter. Most of the integrations you need can be found in the langchain-community package, and if you are just using the core expression language API's, you can even build solely based on langchain-core . Environment Setup ​ The following environment variables need to be set: Set the OPENAI_API_KEY environment variable to access the OpenAI models. Set the TAVILY_API_KEY environment variable to access Tavily search. Create a credentials.json file containing your OAuth client ID from Gmail. To customize authentication, see the Customize Auth section below. Note: The first time you run this app, it will force you to go through a user authentication flow. (Optional): Set GMAIL_AGENT_ENABLE_SEND to true (or modify the agent.py file in this template) to give it access to the \"Send\" tool. This will give your assistant permissions to send emails on your behalf without your explicit review, which is not recommended. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package openai-functions-agent-gmail If you want to add this to an existing project, you can just run: langchain app add openai-functions-agent-gmail And add the following code to your server.py file: from openai_functions_agent import agent_executor as openai_functions_agent_chain add_routes ( app , openai_functions_agent_chain , path = \"/openai-functions-agent-gmail\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/openai-functions-agent-gmail/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/openai-functions-agent-gmail\" ) Customize Auth ​ from langchain.tools.gmail.utils import build_resource_service, get_gmail_credentials # Can review scopes here https://developers.google.com/gmail/api/auth/scopes # For instance, readonly scope is 'https://www.googleapis.com/auth/gmail.readonly' credentials = get_gmail_credentials( token_file=\"token.json\", scopes=[\"https://mail.google.com/\"], client_secrets_file=\"credentials.json\", ) api_resource = build_resource_service(credentials=credentials) toolkit = GmailToolkit(api_resource=api_resource) Previous neo4j-vector-memory Next openai-functions-agent"
    },
    {
        "title": "openai-functions-agent | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/openai-functions-agent",
        "text_content": "Templates openai-functions-agent On this page openai-functions-agent This template creates an agent that uses OpenAI function calling to communicate its decisions on what actions to take. This example creates an agent that can optionally look up information on the internet using Tavily's search engine. Environment Setup ​ The following environment variables need to be set: Set the OPENAI_API_KEY environment variable to access the OpenAI models. Set the TAVILY_API_KEY environment variable to access Tavily. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package openai-functions-agent If you want to add this to an existing project, you can just run: langchain app add openai-functions-agent And add the following code to your server.py file: from openai_functions_agent import agent_executor as openai_functions_agent_chain add_routes ( app , openai_functions_agent_chain , path = \"/openai-functions-agent\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/openai-functions-agent/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/openai-functions-agent\" ) Previous OpenAI Functions Agent - Gmail Next openai-functions-tool-retrieval-agent"
    },
    {
        "title": "openai-functions-tool-retrieval-agent | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/openai-functions-tool-retrieval-agent",
        "text_content": "Templates openai-functions-tool-retrieval-agent On this page openai-functions-tool-retrieval-agent The novel idea introduced in this template is the idea of using retrieval to select the set of tools to use to answer an agent query. This is useful when you have many many tools to select from. You cannot put the description of all the tools in the prompt (because of context length issues) so instead you dynamically select the N tools you do want to consider using at run time. In this template we will create a somewhat contrived example. We will have one legitimate tool (search) and then 99 fake tools which are just nonsense. We will then add a step in the prompt template that takes the user input and retrieves tool relevant to the query. This template is based on this Agent How-To . Environment Setup ​ The following environment variables need to be set: Set the OPENAI_API_KEY environment variable to access the OpenAI models. Set the TAVILY_API_KEY environment variable to access Tavily. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package openai-functions-tool-retrieval-agent If you want to add this to an existing project, you can just run: langchain app add openai-functions-tool-retrieval-agent And add the following code to your server.py file: from openai_functions_tool_retrieval_agent import chain as openai_functions_tool_retrieval_agent_chain add_routes ( app , openai_functions_tool_retrieval_agent_chain , path = \"/openai-functions-tool-retrieval-agent\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/openai-functions-tool-retrieval-agent/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/openai-functions-tool-retrieval-agent\" ) Previous openai-functions-agent Next pii-protected-chatbot"
    },
    {
        "title": "pii-protected-chatbot | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/pii-protected-chatbot",
        "text_content": "Templates pii-protected-chatbot On this page pii-protected-chatbot This template creates a chatbot that flags any incoming PII and doesn't pass it to the LLM. Environment Setup ​ The following environment variables need to be set: Set the OPENAI_API_KEY environment variable to access the OpenAI models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U \"langchain-cli[serve]\" To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package pii-protected-chatbot If you want to add this to an existing project, you can just run: langchain app add pii-protected-chatbot And add the following code to your server.py file: from pii_protected_chatbot . chain import chain as pii_protected_chatbot add_routes ( app , pii_protected_chatbot , path = \"/openai-functions-agent\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/pii_protected_chatbot/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/pii_protected_chatbot\" ) Previous openai-functions-tool-retrieval-agent Next pirate-speak-configurable"
    },
    {
        "title": "pirate-speak-configurable | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/pirate-speak-configurable",
        "text_content": "Templates pirate-speak-configurable On this page pirate-speak-configurable This template converts user input into pirate speak. It shows how you can allow configurable_alternatives in the Runnable, allowing you to select from\nOpenAI, Anthropic, or Cohere as your LLM Provider in the playground (or via API). Environment Setup ​ Set the following environment variables to access all 3 configurable alternative\nmodel providers: OPENAI_API_KEY ANTHROPIC_API_KEY COHERE_API_KEY Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package pirate-speak-configurable If you want to add this to an existing project, you can just run: langchain app add pirate-speak-configurable And add the following code to your server.py file: from pirate_speak_configurable import chain as pirate_speak_configurable_chain add_routes ( app , pirate_speak_configurable_chain , path = \"/pirate-speak-configurable\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/pirate-speak-configurable/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/pirate-speak-configurable\" ) Previous pii-protected-chatbot Next pirate-speak"
    },
    {
        "title": "pirate-speak | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/pirate-speak",
        "text_content": "Templates pirate-speak On this page pirate-speak This template converts user input into pirate speak. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package pirate-speak If you want to add this to an existing project, you can just run: langchain app add pirate-speak And add the following code to your server.py file: from pirate_speak . chain import chain as pirate_speak_chain add_routes ( app , pirate_speak_chain , path = \"/pirate-speak\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/pirate-speak/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/pirate-speak\" ) Previous pirate-speak-configurable Next plate-chain"
    },
    {
        "title": "plate-chain | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/plate-chain",
        "text_content": "Templates plate-chain On this page plate-chain This template enables parsing of data from laboratory plates. In the context of biochemistry or molecular biology, laboratory plates are commonly used tools to hold samples in a grid-like format. This can parse the resulting data into standardized (e.g., JSON) format for further processing. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. Usage ​ To utilize plate-chain, you must have the LangChain CLI installed: pip install -U langchain-cli Creating a new LangChain project and installing plate-chain as the only package can be done with: langchain app new my-app --package plate-chain If you wish to add this to an existing project, simply run: langchain app add plate-chain Then add the following code to your server.py file: from plate_chain import chain as plate_chain add_routes ( app , plate_chain , path = \"/plate-chain\" ) (Optional) For configuring LangSmith, which helps trace, monitor and debug LangChain applications, use the following code: export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you're in this directory, you can start a LangServe instance directly by: langchain serve This starts the FastAPI app with a server running locally at http://localhost:8000 All templates can be viewed at http://127.0.0.1:8000/docs Access the playground at http://127.0.0.1:8000/plate-chain/playground You can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/plate-chain\" ) Previous pirate-speak Next propositional-retrieval"
    },
    {
        "title": "propositional-retrieval | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/propositional-retrieval",
        "text_content": "Templates propositional-retrieval On this page propositional-retrieval This template demonstrates the multi-vector indexing strategy proposed by Chen, et. al.'s Dense X Retrieval: What Retrieval Granularity Should We Use? . The prompt, which you can try out on the hub , directs an LLM to generate de-contextualized \"propositions\" which can be vectorized to increase the retrieval accuracy. You can see the full definition in proposal_chain.py . Storage ​ For this demo, we index a simple academic paper using the RecursiveUrlLoader, and store all retriever information locally (using chroma and a bytestore stored on the local filesystem). You can modify the storage layer in storage.py . Environment Setup ​ Set the OPENAI_API_KEY environment variable to access gpt-3.5 and the OpenAI Embeddings classes. Indexing ​ Create the index by running the following: poetry install poetry run python propositional_retrieval / ingest . py Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package propositional-retrieval If you want to add this to an existing project, you can just run: langchain app add propositional-retrieval And add the following code to your server.py file: from propositional_retrieval import chain add_routes ( app , chain , path = \"/propositional-retrieval\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/propositional-retrieval/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/propositional-retrieval\" ) Previous plate-chain Next rag-astradb"
    },
    {
        "title": "rag-astradb | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-astradb",
        "text_content": "Templates rag-astradb On this page rag-astradb This template will perform RAG using Astra DB ( AstraDB vector store class) Environment Setup ​ An Astra DB database is required; free tier is fine. You need the database API endpoint (such as https://0123...-us-east1.apps.astra.datastax.com ) ... ... and a token ( AstraCS:... ). Also, an OpenAI API Key is required. Note that out-of-the-box this demo supports OpenAI only, unless you tinker with the code. Provide the connection parameters and secrets through environment variables. Please refer to .env.template for the variable names. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U \"langchain-cli[serve]\" To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-astradb If you want to add this to an existing project, you can just run: langchain app add rag-astradb And add the following code to your server.py file: from astradb_entomology_rag import chain as astradb_entomology_rag_chain add_routes ( app , astradb_entomology_rag_chain , path = \"/rag-astradb\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-astradb/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-astradb\" ) Reference ​ Stand-alone repo with LangServe chain: here . Previous propositional-retrieval Next rag-aws-bedrock"
    },
    {
        "title": "rag-aws-bedrock | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-aws-bedrock",
        "text_content": "Templates rag-aws-bedrock On this page rag-aws-bedrock This template is designed to connect with the AWS Bedrock service, a managed server that offers a set of foundation models. It primarily uses the Anthropic Claude for text generation and Amazon Titan for text embedding, and utilizes FAISS as the vectorstore. For additional context on the RAG pipeline, refer to this notebook . Environment Setup ​ Before you can use this package, ensure that you have configured boto3 to work with your AWS account. For details on how to set up and configure boto3 , visit this page . In addition, you need to install the faiss-cpu package to work with the FAISS vector store: pip install faiss-cpu You should also set the following environment variables to reflect your AWS profile and region (if you're not using the default AWS profile and us-east-1 region): AWS_DEFAULT_REGION AWS_PROFILE Usage ​ First, install the LangChain CLI: pip install -U langchain-cli To create a new LangChain project and install this as the only package: langchain app new my-app --package rag-aws-bedrock To add this package to an existing project: langchain app add rag-aws-bedrock Then add the following code to your server.py file: from rag_aws_bedrock import chain as rag_aws_bedrock_chain add_routes ( app , rag_aws_bedrock_chain , path = \"/rag-aws-bedrock\" ) (Optional) If you have access to LangSmith, you can configure it to trace, monitor, and debug LangChain applications. If you don't have access, you can skip this section. export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server running locally at http://localhost:8000 You can see all templates at http://127.0.0.1:8000/docs and access the playground at http://127.0.0.1:8000/rag-aws-bedrock/playground . You can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-aws-bedrock\" ) Previous rag-astradb Next rag-aws-kendra"
    },
    {
        "title": "rag-aws-kendra | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-aws-kendra",
        "text_content": "Templates rag-aws-kendra On this page rag-aws-kendra This template is an application that utilizes Amazon Kendra, a machine learning powered search service, and Anthropic Claude for text generation. The application retrieves documents using a Retrieval chain to answer questions from your documents. It uses the boto3 library to connect with the Bedrock service. For more context on building RAG applications with Amazon Kendra, check this page . Environment Setup ​ Please ensure to setup and configure boto3 to work with your AWS account. You can follow the guide here . You should also have a Kendra Index set up before using this template. You can use this Cloudformation template to create a sample index. This includes sample data containing AWS online documentation for Amazon Kendra, Amazon Lex, and Amazon SageMaker. Alternatively, you can use your own Amazon Kendra index if you have indexed your own dataset. The following environment variables need to be set: AWS_DEFAULT_REGION - This should reflect the correct AWS region. Default is us-east-1 . AWS_PROFILE - This should reflect your AWS profile. Default is default . KENDRA_INDEX_ID - This should have the Index ID of the Kendra index. Note that the Index ID is a 36 character alphanumeric value that can be found in the index detail page. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-aws-kendra If you want to add this to an existing project, you can just run: langchain app add rag-aws-kendra And add the following code to your server.py file: from rag_aws_kendra . chain import chain as rag_aws_kendra_chain add_routes ( app , rag_aws_kendra_chain , path = \"/rag-aws-kendra\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-aws-kendra/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-aws-kendra\" ) Previous rag-aws-bedrock Next rag-chroma-multi-modal-multi-vector"
    },
    {
        "title": "rag-chroma-multi-modal-multi-vector | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-chroma-multi-modal-multi-vector",
        "text_content": "Templates rag-chroma-multi-modal-multi-vector On this page rag-chroma-multi-modal-multi-vector Multi-modal LLMs enable visual assistants that can perform question-answering about images. This template create a visual assistant for slide decks, which often contain visuals such as graphs or figures. It uses GPT-4V to create image summaries for each slide, embeds the summaries, and stores them in Chroma. Given a question, relevat slides are retrieved and passed to GPT-4V for answer synthesis. Input ​ Supply a slide deck as pdf in the /docs directory. By default, this template has a slide deck about Q3 earnings from DataDog, a public techologyy company. Example questions to ask can be: How many customers does Datadog have? What is Datadog platform % Y/Y growth in FY20, FY21, and FY22? To create an index of the slide deck, run: poetry install python ingest.py Storage ​ Here is the process the template will use to create an index of the slides (see blog ): Extract the slides as a collection of images Use GPT-4V to summarize each image Embed the image summaries using text embeddings with a link to the original images Retrieve relevant image based on similarity between the image summary and the user input question Pass those images to GPT-4V for answer synthesis By default, this will use LocalFileStore to store images and Chroma to store summaries. For production, it may be desirable to use a remote option such as Redis. You can set the local_file_store flag in chain.py and ingest.py to switch between the two options. For Redis, the template will use UpstashRedisByteStore . We will use Upstash to store the images, which offers Redis with a REST API. Simply login here and create a database. This will give you a REST API with: UPSTASH_URL UPSTASH_TOKEN Set UPSTASH_URL and UPSTASH_TOKEN as environment variables to access your database. We will use Chroma to store and index the image summaries, which will be created locally in the template directory. LLM ​ The app will retrieve images based on similarity between the text input and the image summary, and pass the images to GPT-4V. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI GPT-4V. Set UPSTASH_URL and UPSTASH_TOKEN as environment variables to access your database if you use UpstashRedisByteStore . Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-chroma-multi-modal-multi-vector If you want to add this to an existing project, you can just run: langchain app add rag-chroma-multi-modal-multi-vector And add the following code to your server.py file: from rag_chroma_multi_modal_multi_vector import chain as rag_chroma_multi_modal_chain_mv add_routes ( app , rag_chroma_multi_modal_chain_mv , path = \"/rag-chroma-multi-modal-multi-vector\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-chroma-multi-modal-multi-vector/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-chroma-multi-modal-multi-vector\" ) Previous rag-aws-kendra Next rag-chroma-multi-modal"
    },
    {
        "title": "rag-chroma-multi-modal | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-chroma-multi-modal",
        "text_content": "Templates rag-chroma-multi-modal On this page rag-chroma-multi-modal Multi-modal LLMs enable visual assistants that can perform question-answering about images. This template create a visual assistant for slide decks, which often contain visuals such as graphs or figures. It uses OpenCLIP embeddings to embed all of the slide images and stores them in Chroma. Given a question, relevat slides are retrieved and passed to GPT-4V for answer synthesis. Input ​ Supply a slide deck as pdf in the /docs directory. By default, this template has a slide deck about Q3 earnings from DataDog, a public techologyy company. Example questions to ask can be: How many customers does Datadog have? What is Datadog platform % Y/Y growth in FY20, FY21, and FY22? To create an index of the slide deck, run: poetry install python ingest.py Storage ​ This template will use OpenCLIP multi-modal embeddings to embed the images. You can select different embedding model options (see results here ). The first time you run the app, it will automatically download the multimodal embedding model. By default, LangChain will use an embedding model with moderate performance but lower memory requirments, ViT-H-14 . You can choose alternative OpenCLIPEmbeddings models in rag_chroma_multi_modal/ingest.py : vectorstore_mmembd = Chroma( collection_name=\"multi-modal-rag\", persist_directory=str(re_vectorstore_path), embedding_function=OpenCLIPEmbeddings( model_name=\"ViT-H-14\", checkpoint=\"laion2b_s32b_b79k\" ), ) LLM ​ The app will retrieve images based on similarity between the text input and the image, which are both mapped to multi-modal embedding space. It will then pass the images to GPT-4V. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI GPT-4V. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-chroma-multi-modal If you want to add this to an existing project, you can just run: langchain app add rag-chroma-multi-modal And add the following code to your server.py file: from rag_chroma_multi_modal import chain as rag_chroma_multi_modal_chain add_routes ( app , rag_chroma_multi_modal_chain , path = \"/rag-chroma-multi-modal\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-chroma-multi-modal/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-chroma-multi-modal\" ) Previous rag-chroma-multi-modal-multi-vector Next rag-chroma-private"
    },
    {
        "title": "rag-chroma-private | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-chroma-private",
        "text_content": "Templates rag-chroma-private On this page rag-chroma-private This template performs RAG with no reliance on external APIs. It utilizes Ollama the LLM, GPT4All for embeddings, and Chroma for the vectorstore. The vectorstore is created in chain.py and by default indexes a popular blog posts on Agents for question-answering. Environment Setup ​ To set up the environment, you need to download Ollama. Follow the instructions here . You can choose the desired LLM with Ollama. This template uses llama2:7b-chat , which can be accessed using ollama pull llama2:7b-chat . There are many other options available here . This package also uses GPT4All embeddings. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-chroma-private If you want to add this to an existing project, you can just run: langchain app add rag-chroma-private And add the following code to your server.py file: from rag_chroma_private import chain as rag_chroma_private_chain add_routes ( app , rag_chroma_private_chain , path = \"/rag-chroma-private\" ) (Optional) Let's now configure LangSmith. LangSmith will help us trace, monitor and debug LangChain applications. LangSmith is currently in private beta, you can sign up here . If you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-chroma-private/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-chroma-private\" ) The package will create and add documents to the vector database in chain.py . By default, it will load a popular blog post on agents. However, you can choose from a large number of document loaders here . Previous rag-chroma-multi-modal Next rag-chroma"
    },
    {
        "title": "rag-chroma | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-chroma",
        "text_content": "Templates rag-chroma On this page rag-chroma This template performs RAG using Chroma and OpenAI. The vectorstore is created in chain.py and by default indexes a popular blog posts on Agents for question-answering. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-chroma If you want to add this to an existing project, you can just run: langchain app add rag-chroma And add the following code to your server.py file: from rag_chroma import chain as rag_chroma_chain add_routes ( app , rag_chroma_chain , path = \"/rag-chroma\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-chroma/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-chroma\" ) Previous rag-chroma-private Next rag-codellama-fireworks"
    },
    {
        "title": "rag-codellama-fireworks | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-codellama-fireworks",
        "text_content": "Templates rag-codellama-fireworks On this page rag-codellama-fireworks This template performs RAG on a codebase. It uses codellama-34b hosted by Fireworks' LLM inference API . Environment Setup ​ Set the FIREWORKS_API_KEY environment variable to access the Fireworks models. You can obtain it from here . Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-codellama-fireworks If you want to add this to an existing project, you can just run: langchain app add rag-codellama-fireworks And add the following code to your server.py file: from rag_codellama_fireworks import chain as rag_codellama_fireworks_chain add_routes ( app , rag_codellama_fireworks_chain , path = \"/rag-codellama-fireworks\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-codellama-fireworks/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-codellama-fireworks\" ) Previous rag-chroma Next rag-conversation-zep"
    },
    {
        "title": "rag-conversation-zep | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-conversation-zep",
        "text_content": "Templates rag-conversation-zep On this page rag-conversation-zep This template demonstrates building a RAG conversation app using Zep. Included in this template: Populating a Zep Document Collection with a set of documents (a Collection is analogous to an index in other Vector Databases). Using Zep's integrated embedding functionality to embed the documents as vectors. Configuring a LangChain ZepVectorStore Retriever to retrieve documents using Zep's built, hardware accelerated in Maximal Marginal Relevance (MMR) re-ranking. Prompts, a simple chat history data structure, and other components required to build a RAG conversation app. The RAG conversation chain. About Zep - Fast, scalable building blocks for LLM Apps ​ Zep is an open source platform for productionizing LLM apps. Go from a prototype built in LangChain or LlamaIndex, or a custom app, to production in minutes without rewriting code. Key Features: Fast! Zep’s async extractors operate independently of the your chat loop, ensuring a snappy user experience. Long-term memory persistence, with access to historical messages irrespective of your summarization strategy. Auto-summarization of memory messages based on a configurable message window. A series of summaries are stored, providing flexibility for future summarization strategies. Hybrid search over memories and metadata, with messages automatically embedded on creation. Entity Extractor that automatically extracts named entities from messages and stores them in the message metadata. Auto-token counting of memories and summaries, allowing finer-grained control over prompt assembly. Python and JavaScript SDKs. Zep project: https://github.com/getzep/zep | Docs: https://docs.getzep.com/ Environment Setup ​ Set up a Zep service by following the Quick Start Guide . Ingesting Documents into a Zep Collection ​ Run python ingest.py to ingest the test documents into a Zep Collection. Review the file to modify the Collection name and document source. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U \"langchain-cli[serve]\" To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-conversation-zep If you want to add this to an existing project, you can just run: langchain app add rag-conversation-zep And add the following code to your server.py file: from rag_conversation_zep import chain as rag_conversation_zep_chain add_routes ( app , rag_conversation_zep_chain , path = \"/rag-conversation-zep\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-conversation-zep/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-conversation-zep\" ) Previous rag-codellama-fireworks Next rag-conversation"
    },
    {
        "title": "rag-conversation | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-conversation",
        "text_content": "Templates rag-conversation On this page rag-conversation This template is used for conversational retrieval , which is one of the most popular LLM use-cases. It passes both a conversation history and retrieved documents into an LLM for synthesis. Environment Setup ​ This template uses Pinecone as a vectorstore and requires that PINECONE_API_KEY , PINECONE_ENVIRONMENT , and PINECONE_INDEX are set. Set the OPENAI_API_KEY environment variable to access the OpenAI models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-conversation If you want to add this to an existing project, you can just run: langchain app add rag-conversation And add the following code to your server.py file: from rag_conversation import chain as rag_conversation_chain add_routes ( app , rag_conversation_chain , path = \"/rag-conversation\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-conversation/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-conversation\" ) Previous rag-conversation-zep Next rag-elasticsearch"
    },
    {
        "title": "rag-elasticsearch | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-elasticsearch",
        "text_content": "Templates rag-elasticsearch On this page rag-elasticsearch This template performs RAG using ElasticSearch . It relies on sentence transformer MiniLM-L6-v2 for embedding passages and questions. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. To connect to your Elasticsearch instance, use the following environment variables: export ELASTIC_CLOUD_ID = < ClOUD_ID > export ELASTIC_USERNAME = < ClOUD_USERNAME > export ELASTIC_PASSWORD = < ClOUD_PASSWORD > For local development with Docker, use: export ES_URL = \"http://localhost:9200\" And run an Elasticsearch instance in Docker with docker run -p 9200 :9200 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" -e \"xpack.security.http.ssl.enabled=false\" docker.elastic.co/elasticsearch/elasticsearch:8.9.0 Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-elasticsearch If you want to add this to an existing project, you can just run: langchain app add rag-elasticsearch And add the following code to your server.py file: from rag_elasticsearch import chain as rag_elasticsearch_chain add_routes ( app , rag_elasticsearch_chain , path = \"/rag-elasticsearch\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-elasticsearch/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-elasticsearch\" ) For loading the fictional workplace documents, run the following command from the root of this repository: python ingest.py However, you can choose from a large number of document loaders here . Previous rag-conversation Next rag-fusion"
    },
    {
        "title": "rag-fusion | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-fusion",
        "text_content": "Templates rag-fusion On this page rag-fusion This template enables RAG fusion using a re-implementation of the project found here . It performs multiple query generation and Reciprocal Rank Fusion to re-rank search results. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-fusion If you want to add this to an existing project, you can just run: langchain app add rag-fusion And add the following code to your server.py file: from rag_fusion . chain import chain as rag_fusion_chain add_routes ( app , rag_fusion_chain , path = \"/rag-fusion\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-fusion/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-fusion\" ) Previous rag-elasticsearch Next rag-gemini-multi-modal"
    },
    {
        "title": "rag-gemini-multi-modal | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-gemini-multi-modal",
        "text_content": "Templates rag-gemini-multi-modal On this page rag-gemini-multi-modal Multi-modal LLMs enable visual assistants that can perform question-answering about images. This template create a visual assistant for slide decks, which often contain visuals such as graphs or figures. It uses OpenCLIP embeddings to embed all of the slide images and stores them in Chroma. Given a question, relevat slides are retrieved and passed to Google Gemini for answer synthesis. Input ​ Supply a slide deck as pdf in the /docs directory. By default, this template has a slide deck about Q3 earnings from DataDog, a public techologyy company. Example questions to ask can be: How many customers does Datadog have? What is Datadog platform % Y/Y growth in FY20, FY21, and FY22? To create an index of the slide deck, run: poetry install python ingest.py Storage ​ This template will use OpenCLIP multi-modal embeddings to embed the images. You can select different embedding model options (see results here ). The first time you run the app, it will automatically download the multimodal embedding model. By default, LangChain will use an embedding model with moderate performance but lower memory requirments, ViT-H-14 . You can choose alternative OpenCLIPEmbeddings models in rag_chroma_multi_modal/ingest.py : vectorstore_mmembd = Chroma( collection_name=\"multi-modal-rag\", persist_directory=str(re_vectorstore_path), embedding_function=OpenCLIPEmbeddings( model_name=\"ViT-H-14\", checkpoint=\"laion2b_s32b_b79k\" ), ) LLM ​ The app will retrieve images using multi-modal embeddings, and pass them to Google Gemini. Environment Setup ​ Set your GOOGLE_API_KEY environment variable in order to access Gemini. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-gemini-multi-modal If you want to add this to an existing project, you can just run: langchain app add rag-gemini-multi-modal And add the following code to your server.py file: from rag_gemini_multi_modal import chain as rag_gemini_multi_modal_chain add_routes ( app , rag_gemini_multi_modal_chain , path = \"/rag-gemini-multi-modal\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-gemini-multi-modal/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-gemini-multi-modal\" ) Previous rag-fusion Next rag-google-cloud-sensitive-data-protection"
    },
    {
        "title": "rag-google-cloud-sensitive-data-protection | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-google-cloud-sensitive-data-protection",
        "text_content": "Templates rag-google-cloud-sensitive-data-protection On this page rag-google-cloud-sensitive-data-protection This template is an application that utilizes Google Vertex AI Search, a machine learning powered search service, and\nPaLM 2 for Chat (chat-bison). The application uses a Retrieval chain to answer questions based on your documents. This template is an application that utilizes Google Sensitive Data Protection, a service for detecting and redacting\nsensitive data in text, and PaLM 2 for Chat (chat-bison), although you can use any model. For more context on using Sensitive Data Protection,\ncheck here . Environment Setup ​ Before using this template, please ensure that you enable the DLP API and Vertex AI API in your Google Cloud\nproject. For some common environment troubleshooting steps related to Google Cloud, see the bottom\nof this readme. Set the following environment variables: GOOGLE_CLOUD_PROJECT_ID - Your Google Cloud project ID. MODEL_TYPE - The model type for Vertex AI Search (e.g. chat-bison ) Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-google-cloud-sensitive-data-protection If you want to add this to an existing project, you can just run: langchain app add rag-google-cloud-sensitive-data-protection And add the following code to your server.py file: from rag_google_cloud_sensitive_data_protection . chain import chain as rag_google_cloud_sensitive_data_protection_chain add_routes ( app , rag_google_cloud_sensitive_data_protection_chain , path = \"/rag-google-cloud-sensitive-data-protection\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground\nat http://127.0.0.1:8000/rag-google-cloud-vertexai-search/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-google-cloud-sensitive-data-protection\" ) # Troubleshooting Google Cloud You can set your `gcloud` credentials with their CLI using `gcloud auth application-default login` You can set your `gcloud` project with the following commands ```bash gcloud config set project <your project> gcloud auth application-default set-quota-project <your project> export GOOGLE_CLOUD_PROJECT_ID=<your project> Previous rag-gemini-multi-modal Next rag-google-cloud-vertexai-search"
    },
    {
        "title": "rag-google-cloud-vertexai-search | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-google-cloud-vertexai-search",
        "text_content": "Templates rag-google-cloud-vertexai-search On this page rag-google-cloud-vertexai-search This template is an application that utilizes Google Vertex AI Search, a machine learning powered search service, and\nPaLM 2 for Chat (chat-bison). The application uses a Retrieval chain to answer questions based on your documents. For more context on building RAG applications with Vertex AI Search,\ncheck here . Environment Setup ​ Before using this template, please ensure that you are authenticated with Vertex AI Search. See the authentication\nguide: here . You will also need to create: A search application here A data store here A suitable dataset to test this template with is the Alphabet Earnings Reports, which you can\nfind here . The data is also available\nat gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs . Set the following environment variables: GOOGLE_CLOUD_PROJECT_ID - Your Google Cloud project ID. DATA_STORE_ID - The ID of the data store in Vertex AI Search, which is a 36-character alphanumeric value found on\nthe data store details page. MODEL_TYPE - The model type for Vertex AI Search. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-google-cloud-vertexai-search If you want to add this to an existing project, you can just run: langchain app add rag-google-cloud-vertexai-search And add the following code to your server.py file: from rag_google_cloud_vertexai_search . chain import chain as rag_google_cloud_vertexai_search_chain add_routes ( app , rag_google_cloud_vertexai_search_chain , path = \"/rag-google-cloud-vertexai-search\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground\nat http://127.0.0.1:8000/rag-google-cloud-vertexai-search/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-google-cloud-vertexai-search\" ) Previous rag-google-cloud-sensitive-data-protection Next rag-gpt-crawler"
    },
    {
        "title": "rag-gpt-crawler | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-gpt-crawler",
        "text_content": "Templates rag-gpt-crawler On this page rag-gpt-crawler GPT-crawler will crawl websites to produce files for use in custom GPTs or other apps (RAG). This template uses gpt-crawler to build a RAG app Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. Crawling ​ Run GPT-crawler to extact content from a set of urls, using the config file in GPT-crawler repo. Here is example config for LangChain use-case docs: export const config: Config = { url: \"https://python.langchain.com/docs/use_cases/\", match: \"https://python.langchain.com/docs/use_cases/**\", selector: \".docMainContainer_gTbr\", maxPagesToCrawl: 10, outputFileName: \"output.json\", }; Then, run this as described in the gpt-crawler README: npm start And copy the output.json file into the folder containing this README. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-gpt-crawler If you want to add this to an existing project, you can just run: langchain app add rag-gpt-crawler And add the following code to your server.py file: from rag_chroma import chain as rag_gpt_crawler add_routes ( app , rag_gpt_crawler , path = \"/rag-gpt-crawler\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-gpt-crawler/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-gpt-crawler\" ) Previous rag-google-cloud-vertexai-search Next rag-matching-engine"
    },
    {
        "title": "rag-matching-engine | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-matching-engine",
        "text_content": "Templates rag-matching-engine On this page rag-matching-engine This template performs RAG using Google Cloud Platform's Vertex AI with the matching engine. It will utilize a previously created index to retrieve relevant documents or contexts based on user-provided questions. Environment Setup ​ An index should be created before running the code. The process to create this index can be found here . Environment variables for Vertex should be set: PROJECT_ID ME_REGION GCS_BUCKET ME_INDEX_ID ME_ENDPOINT_ID Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-matching-engine If you want to add this to an existing project, you can just run: langchain app add rag-matching-engine And add the following code to your server.py file: from rag_matching_engine import chain as rag_matching_engine_chain add_routes ( app , rag_matching_engine_chain , path = \"/rag-matching-engine\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-matching-engine/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-matching-engine\" ) For more details on how to connect to the template, refer to the Jupyter notebook rag_matching_engine . Previous rag-gpt-crawler Next rag-momento-vector-index"
    },
    {
        "title": "rag-momento-vector-index | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-momento-vector-index",
        "text_content": "Templates rag-momento-vector-index On this page rag-momento-vector-index This template performs RAG using Momento Vector Index (MVI) and OpenAI. MVI: the most productive, easiest to use, serverless vector index for your data. To get started with MVI, simply sign up for an account. There's no need to handle infrastructure, manage servers, or be concerned about scaling. MVI is a service that scales automatically to meet your needs. Combine with other Momento services such as Momento Cache to cache prompts and as a session store or Momento Topics as a pub/sub system to broadcast events to your application. To sign up and access MVI, visit the Momento Console . Environment Setup ​ This template uses Momento Vector Index as a vectorstore and requires that MOMENTO_API_KEY , and MOMENTO_INDEX_NAME are set. Go to the console to get an API key. Set the OPENAI_API_KEY environment variable to access the OpenAI models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-momento-vector-index If you want to add this to an existing project, you can just run: langchain app add rag-momento-vector-index And add the following code to your server.py file: from rag_momento_vector_index import chain as rag_momento_vector_index_chain add_routes ( app , rag_momento_vector_index_chain , path = \"/rag-momento-vector-index\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-momento-vector-index/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-momento-vector-index\" ) Indexing Data ​ We have included a sample module to index data. That is available at rag_momento_vector_index/ingest.py . You will see a commented out line in chain.py that invokes this. Uncomment to use. Previous rag-matching-engine Next rag-mongo"
    },
    {
        "title": "rag-mongo | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-mongo",
        "text_content": "Templates rag-mongo On this page rag-mongo This template performs RAG using MongoDB and OpenAI. Environment Setup ​ You should export two environment variables, one being your MongoDB URI, the other being your OpenAI API KEY.\nIf you do not have a MongoDB URI, see the Setup Mongo section at the bottom for instructions on how to do so. export MONGO_URI = .. . export OPENAI_API_KEY = .. . Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-mongo If you want to add this to an existing project, you can just run: langchain app add rag-mongo And add the following code to your server.py file: from rag_mongo import chain as rag_mongo_chain add_routes ( app , rag_mongo_chain , path = \"/rag-mongo\" ) If you want to set up an ingestion pipeline, you can add the following code to your server.py file: from rag_mongo import ingest as rag_mongo_ingest add_routes ( app , rag_mongo_ingest , path = \"/rag-mongo-ingest\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you DO NOT already have a Mongo Search Index you want to connect to, see MongoDB Setup section below before proceeding. If you DO have a MongoDB Search index you want to connect to, edit the connection details in rag_mongo/chain.py If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-mongo/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-mongo\" ) For additional context, please refer to this notebook . MongoDB Setup ​ Use this step if you need to setup your MongoDB account and ingest data.\nWe will first follow the standard MongoDB Atlas setup instructions here . Create an account (if not already done) Create a new project (if not already done) Locate your MongoDB URI. This can be done by going to the deployement overview page and connecting to you database We then look at the drivers available Among which we will see our URI listed Let's then set that as an environment variable locally: export MONGO_URI = .. . Let's also set an environment variable for OpenAI (which we will use as an LLM) export OPENAI_API_KEY = .. . Let's now ingest some data! We can do that by moving into this directory and running the code in ingest.py , eg: python ingest.py Note that you can (and should!) change this to ingest data of your choice We now need to set up a vector index on our data. We can first connect to the cluster where our database lives We can then navigate to where all our collections are listed We can then find the collection we want and look at the search indexes for that collection That should likely be empty, and we want to create a new one: We will use the JSON editor to create it And we will paste the following JSON in: { \"mappings\": { \"dynamic\": true, \"fields\": { \"embedding\": { \"dimensions\": 1536, \"similarity\": \"cosine\", \"type\": \"knnVector\" } } } } From there, hit \"Next\" and then \"Create Search Index\". It will take a little bit but you should then have an index over your data! Previous rag-momento-vector-index Next RAG with Multiple Indexes (Fusion)"
    },
    {
        "title": "RAG with Multiple Indexes (Fusion) | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-multi-index-fusion",
        "text_content": "Templates RAG with Multiple Indexes (Fusion) On this page RAG with Multiple Indexes (Fusion) A QA application that queries multiple domain-specific retrievers and selects the most relevant documents from across all retrieved results. Environment Setup ​ This application queries PubMed, ArXiv, Wikipedia, and Kay AI (for SEC filings). You will need to create a free Kay AI account and get your API key here .\nThen set environment variable: export KAY_API_KEY = \"<YOUR_API_KEY>\" Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-multi-index-fusion If you want to add this to an existing project, you can just run: langchain app add rag-multi-index-fusion And add the following code to your server.py file: from rag_multi_index_fusion import chain as rag_multi_index_fusion_chain add_routes ( app , rag_multi_index_fusion_chain , path = \"/rag-multi-index-fusion\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-multi-index-fusion/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-multi-index-fusion\" ) Previous rag-mongo Next RAG with Multiple Indexes (Routing)"
    },
    {
        "title": "RAG with Multiple Indexes (Routing) | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-multi-index-router",
        "text_content": "Templates RAG with Multiple Indexes (Routing) On this page RAG with Multiple Indexes (Routing) A QA application that routes between different domain-specific retrievers given a user question. Environment Setup ​ This application queries PubMed, ArXiv, Wikipedia, and Kay AI (for SEC filings). You will need to create a free Kay AI account and get your API key here .\nThen set environment variable: export KAY_API_KEY = \"<YOUR_API_KEY>\" Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-multi-index-router If you want to add this to an existing project, you can just run: langchain app add rag-multi-index-router And add the following code to your server.py file: from rag_multi_index_router import chain as rag_multi_index_router_chain add_routes ( app , rag_multi_index_router_chain , path = \"/rag-multi-index-router\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-multi-index-router/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-multi-index-router\" ) Previous RAG with Multiple Indexes (Fusion) Next rag-multi-modal-local"
    },
    {
        "title": "rag-multi-modal-local | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-multi-modal-local",
        "text_content": "Templates rag-multi-modal-local On this page rag-multi-modal-local Visual search is a famililar application to many with iPhones or Android devices. It allows user to serch photos using natural language. With the release of open source, multi-modal LLMs it's possible to build this kind of application for yourself for your own private photo collection. This template demonstrates how to perform private visual search and question-answering over a collection of your photos. It uses OpenCLIP embeddings to embed all of the photos and stores them in Chroma. Given a question, relevat photos are retrieved and passed to an open source multi-modal LLM of your choice for answer synthesis. Input ​ Supply a set of photos in the /docs directory. By default, this template has a toy collection of 3 food pictures. Example questions to ask can be: What kind of soft serve did I have? In practice, a larger corpus of images can be tested. To create an index of the images, run: poetry install python ingest.py Storage ​ This template will use OpenCLIP multi-modal embeddings to embed the images. You can select different embedding model options (see results here ). The first time you run the app, it will automatically download the multimodal embedding model. By default, LangChain will use an embedding model with moderate performance but lower memory requirments, ViT-H-14 . You can choose alternative OpenCLIPEmbeddings models in rag_chroma_multi_modal/ingest.py : vectorstore_mmembd = Chroma( collection_name=\"multi-modal-rag\", persist_directory=str(re_vectorstore_path), embedding_function=OpenCLIPEmbeddings( model_name=\"ViT-H-14\", checkpoint=\"laion2b_s32b_b79k\" ), ) LLM ​ This template will use Ollama . Download the latest version of Ollama: https://ollama.ai/ Pull the an open source multi-modal LLM: e.g., https://ollama.ai/library/bakllava ollama pull bakllava The app is by default configured for bakllava . But you can change this in chain.py and ingest.py for different downloaded models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-chroma-multi-modal If you want to add this to an existing project, you can just run: langchain app add rag-chroma-multi-modal And add the following code to your server.py file: from rag_chroma_multi_modal import chain as rag_chroma_multi_modal_chain add_routes ( app , rag_chroma_multi_modal_chain , path = \"/rag-chroma-multi-modal\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-chroma-multi-modal/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-chroma-multi-modal\" ) Previous RAG with Multiple Indexes (Routing) Next rag-multi-modal-mv-local"
    },
    {
        "title": "rag-multi-modal-mv-local | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-multi-modal-mv-local",
        "text_content": "Templates rag-multi-modal-mv-local On this page rag-multi-modal-mv-local Visual search is a famililar application to many with iPhones or Android devices. It allows user to serch photos using natural language. With the release of open source, multi-modal LLMs it's possible to build this kind of application for yourself for your own private photo collection. This template demonstrates how to perform private visual search and question-answering over a collection of your photos. It uses an open source multi-modal LLM of your choice to create image summaries for each photos, embeds the summaries, and stores them in Chroma. Given a question, relevat photos are retrieved and passed to the multi-modal LLM for answer synthesis. Input ​ Supply a set of photos in the /docs directory. By default, this template has a toy collection of 3 food pictures. The app will look up and summarize photos based upon provided keywords or questions: What kind of ice cream did I have? In practice, a larger corpus of images can be tested. To create an index of the images, run: poetry install python ingest.py Storage ​ Here is the process the template will use to create an index of the slides (see blog ): Given a set of images It uses a local multi-modal LLM ( bakllava ) to summarize each image Embeds the image summaries with a link to the original images Given a user question, it will relevant image(s) based on similarity between the image summary and user input (using Ollama embeddings) It will pass those images to bakllava for answer synthesis By default, this will use LocalFileStore to store images and Chroma to store summaries. LLM and Embedding Models ​ We will use Ollama for generating image summaries, embeddings, and the final image QA. Download the latest version of Ollama: https://ollama.ai/ Pull an open source multi-modal LLM: e.g., https://ollama.ai/library/bakllava Pull an open source embedding model: e.g., https://ollama.ai/library/llama2:7b ollama pull bakllava ollama pull llama2:7b The app is by default configured for bakllava . But you can change this in chain.py and ingest.py for different downloaded models. The app will retrieve images based on similarity between the text input and the image summary, and pass the images to bakllava . Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-multi-modal-mv-local If you want to add this to an existing project, you can just run: langchain app add rag-multi-modal-mv-local And add the following code to your server.py file: from rag_multi_modal_mv_local import chain as rag_multi_modal_mv_local_chain add_routes ( app , rag_multi_modal_mv_local_chain , path = \"/rag-multi-modal-mv-local\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-multi-modal-mv-local/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-multi-modal-mv-local\" ) Previous rag-multi-modal-local Next rag-ollama-multi-query"
    },
    {
        "title": "rag-ollama-multi-query | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-ollama-multi-query",
        "text_content": "Templates rag-ollama-multi-query On this page rag-ollama-multi-query This template performs RAG using Ollama and OpenAI with a multi-query retriever. The multi-query retriever is an example of query transformation, generating multiple queries from different perspectives based on the user's input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries for answer synthesis. We use a private, local LLM for the narrow task of query generation to avoid excessive calls to a larger LLM API. See an example trace for Ollama LLM performing the query expansion here . But we use OpenAI for the more challenging task of answer syntesis (full trace example here ). Environment Setup ​ To set up the environment, you need to download Ollama. Follow the instructions here . You can choose the desired LLM with Ollama. This template uses zephyr , which can be accessed using ollama pull zephyr . There are many other options available here . Set the OPENAI_API_KEY environment variable to access the OpenAI models. Usage ​ To use this package, you should first install the LangChain CLI: pip install -U langchain-cli To create a new LangChain project and install this package, do: langchain app new my-app --package rag-ollama-multi-query To add this package to an existing project, run: langchain app add rag-ollama-multi-query And add the following code to your server.py file: from rag_ollama_multi_query import chain as rag_ollama_multi_query_chain add_routes ( app , rag_ollama_multi_query_chain , path = \"/rag-ollama-multi-query\" ) (Optional) Now, let's configure LangSmith. LangSmith will help us trace, monitor, and debug LangChain applications. LangSmith is currently in private beta, you can sign up here . If you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server running locally at http://localhost:8000 You can see all templates at http://127.0.0.1:8000/docs You can access the playground at http://127.0.0.1:8000/rag-ollama-multi-query/playground To access the template from code, use: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-ollama-multi-query\" ) Previous rag-multi-modal-mv-local Next rag-opensearch"
    },
    {
        "title": "rag-opensearch | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-opensearch",
        "text_content": "Templates rag-opensearch On this page rag-opensearch This Template performs RAG using OpenSearch . Environment Setup ​ Set the following environment variables. OPENAI_API_KEY -  To access OpenAI Embeddings and Models. And optionally set the OpenSearch ones if not using defaults: OPENSEARCH_URL - URL of the hosted OpenSearch Instance OPENSEARCH_USERNAME - User name for the OpenSearch instance OPENSEARCH_PASSWORD - Password for the OpenSearch instance OPENSEARCH_INDEX_NAME - Name of the index To run the default OpenSeach instance in docker, you can use the command docker run -p 9200 :9200 -p 9600 :9600 -e \"discovery.type=single-node\" --name opensearch-node -d opensearchproject/opensearch:latest Note: To load dummy index named langchain-test with dummy documents, run python dummy_index_setup.py in the package Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-opensearch If you want to add this to an existing project, you can just run: langchain app add rag-opensearch And add the following code to your server.py file: from rag_opensearch import chain as rag_opensearch_chain add_routes ( app , rag_opensearch_chain , path = \"/rag-opensearch\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-opensearch/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-opensearch\" ) Previous rag-ollama-multi-query Next rag-pinecone-multi-query"
    },
    {
        "title": "rag-pinecone-multi-query | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-pinecone-multi-query",
        "text_content": "Templates rag-pinecone-multi-query On this page rag-pinecone-multi-query This template performs RAG using Pinecone and OpenAI with a multi-query retriever. It uses an LLM to generate multiple queries from different perspectives based on the user's input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries for answer synthesis. Environment Setup ​ This template uses Pinecone as a vectorstore and requires that PINECONE_API_KEY , PINECONE_ENVIRONMENT , and PINECONE_INDEX are set. Set the OPENAI_API_KEY environment variable to access the OpenAI models. Usage ​ To use this package, you should first install the LangChain CLI: pip install -U langchain-cli To create a new LangChain project and install this package, do: langchain app new my-app --package rag-pinecone-multi-query To add this package to an existing project, run: langchain app add rag-pinecone-multi-query And add the following code to your server.py file: from rag_pinecone_multi_query import chain as rag_pinecone_multi_query_chain add_routes ( app , rag_pinecone_multi_query_chain , path = \"/rag-pinecone-multi-query\" ) (Optional) Now, let's configure LangSmith. LangSmith will help us trace, monitor, and debug LangChain applications. LangSmith is currently in private beta, you can sign up here . If you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server running locally at http://localhost:8000 You can see all templates at http://127.0.0.1:8000/docs You can access the playground at http://127.0.0.1:8000/rag-pinecone-multi-query/playground To access the template from code, use: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-pinecone-multi-query\" ) Previous rag-opensearch Next rag-pinecone-rerank"
    },
    {
        "title": "rag-pinecone-rerank | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-pinecone-rerank",
        "text_content": "Templates rag-pinecone-rerank On this page rag-pinecone-rerank This template performs RAG using Pinecone and OpenAI along with Cohere to perform re-ranking on returned documents. Re-ranking provides a way to rank retrieved documents using specified filters or criteria. Environment Setup ​ This template uses Pinecone as a vectorstore and requires that PINECONE_API_KEY , PINECONE_ENVIRONMENT , and PINECONE_INDEX are set. Set the OPENAI_API_KEY environment variable to access the OpenAI models. Set the COHERE_API_KEY environment variable to access the Cohere ReRank. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-pinecone-rerank If you want to add this to an existing project, you can just run: langchain app add rag-pinecone-rerank And add the following code to your server.py file: from rag_pinecone_rerank import chain as rag_pinecone_rerank_chain add_routes ( app , rag_pinecone_rerank_chain , path = \"/rag-pinecone-rerank\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-pinecone-rerank/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-pinecone-rerank\" ) Previous rag-pinecone-multi-query Next rag-pinecone"
    },
    {
        "title": "rag-pinecone | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-pinecone",
        "text_content": "Templates rag-pinecone On this page rag-pinecone This template performs RAG using Pinecone and OpenAI. Environment Setup ​ This template uses Pinecone as a vectorstore and requires that PINECONE_API_KEY , PINECONE_ENVIRONMENT , and PINECONE_INDEX are set. Set the OPENAI_API_KEY environment variable to access the OpenAI models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-pinecone If you want to add this to an existing project, you can just run: langchain app add rag-pinecone And add the following code to your server.py file: from rag_pinecone import chain as rag_pinecone_chain add_routes ( app , rag_pinecone_chain , path = \"/rag-pinecone\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-pinecone/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-pinecone\" ) Previous rag-pinecone-rerank Next rag-redis"
    },
    {
        "title": "rag-redis | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-redis",
        "text_content": "Templates rag-redis On this page rag-redis This template performs RAG using Redis (vector database) and OpenAI (LLM) on financial 10k filings docs for Nike. It relies on the sentence transformer all-MiniLM-L6-v2 for embedding chunks of the pdf and user questions. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models: export OPENAI_API_KEY = < YOUR OPENAI API KEY > Set the following Redis environment variables: export REDIS_HOST = < YOUR REDIS HOST > export REDIS_PORT = < YOUR REDIS PORT > export REDIS_USER = < YOUR REDIS USER NAME > export REDIS_PASSWORD = < YOUR REDIS PASSWORD > Supported Settings ​ We use a variety of environment variables to configure this application Environment Variable Description Default Value DEBUG Enable or disable Langchain debugging logs True REDIS_HOST Hostname for the Redis server \"localhost\" REDIS_PORT Port for the Redis server 6379 REDIS_USER User for the Redis server \"\" REDIS_PASSWORD Password for the Redis server \"\" REDIS_URL Full URL for connecting to Redis None , Constructed from user, password, host, and port if not provided INDEX_NAME Name of the vector index \"rag-redis\" Usage ​ To use this package, you should first have the LangChain CLI and Pydantic installed in a Python virtual environment: pip install -U langchain-cli pydantic == 1.10 .13 To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-redis If you want to add this to an existing project, you can just run: langchain app add rag-redis And add the following code snippet to your app/server.py file: from rag_redis . chain import chain as rag_redis_chain add_routes ( app , rag_redis_chain , path = \"/rag-redis\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-redis/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-redis\" ) Previous rag-pinecone Next rag-self-query"
    },
    {
        "title": "rag-self-query | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-self-query",
        "text_content": "Templates rag-self-query On this page rag-self-query This template performs RAG using the self-query retrieval technique. The main idea is to let an LLM convert unstructured queries into structured queries. See the docs for more on how this works . Environment Setup ​ In this template we'll use OpenAI models and an Elasticsearch vector store, but the approach generalizes to all LLMs/ChatModels and a number of vector stores . Set the OPENAI_API_KEY environment variable to access the OpenAI models. To connect to your Elasticsearch instance, use the following environment variables: export ELASTIC_CLOUD_ID = < ClOUD_ID > export ELASTIC_USERNAME = < ClOUD_USERNAME > export ELASTIC_PASSWORD = < ClOUD_PASSWORD > For local development with Docker, use: export ES_URL = \"http://localhost:9200\" docker run -p 9200 :9200 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" -e \"xpack.security.http.ssl.enabled=false\" docker.elastic.co/elasticsearch/elasticsearch:8.9.0 Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U \"langchain-cli[serve]\" To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-self-query If you want to add this to an existing project, you can just run: langchain app add rag-self-query And add the following code to your server.py file: from rag_self_query import chain add_routes ( app , chain , path = \"/rag-elasticsearch\" ) To populate the vector store with the sample data, from the root of the directory run: python ingest.py (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-elasticsearch/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-self-query\" ) Previous rag-redis Next rag-semi-structured"
    },
    {
        "title": "rag-semi-structured | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-semi-structured",
        "text_content": "Templates rag-semi-structured On this page rag-semi-structured This template performs RAG on semi-structured data, such as a PDF with text and tables. See this cookbook as a reference. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. This uses Unstructured for PDF parsing, which requires some system-level package installations. On Mac, you can install the necessary packages with the following: brew install tesseract poppler Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-semi-structured If you want to add this to an existing project, you can just run: langchain app add rag-semi-structured And add the following code to your server.py file: from rag_semi_structured import chain as rag_semi_structured_chain add_routes ( app , rag_semi_structured_chain , path = \"/rag-semi-structured\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-semi-structured/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-semi-structured\" ) For more details on how to connect to the template, refer to the Jupyter notebook rag_semi_structured . Previous rag-self-query Next rag-singlestoredb"
    },
    {
        "title": "rag-singlestoredb | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-singlestoredb",
        "text_content": "Templates rag-singlestoredb On this page rag-singlestoredb This template performs RAG using SingleStoreDB and OpenAI. Environment Setup ​ This template uses SingleStoreDB as a vectorstore and requires that SINGLESTOREDB_URL is set. It should take the form admin:password@svc-xxx.svc.singlestore.com:port/db_name Set the OPENAI_API_KEY environment variable to access the OpenAI models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-singlestoredb If you want to add this to an existing project, you can just run: langchain app add rag-singlestoredb And add the following code to your server.py file: from rag_singlestoredb import chain as rag_singlestoredb_chain add_routes ( app , rag_singlestoredb_chain , path = \"/rag-singlestoredb\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-singlestoredb/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-singlestoredb\" ) Previous rag-semi-structured Next rag_supabase"
    },
    {
        "title": "rag_supabase | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-supabase",
        "text_content": "Templates rag_supabase On this page rag_supabase This template performs RAG with Supabase. Supabase is an open-source Firebase alternative. It is built on top of PostgreSQL , a free and open-source relational database management system (RDBMS) and uses pgvector to store embeddings within your tables. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. To get your OPENAI_API_KEY , navigate to API keys on your OpenAI account and create a new secret key. To find your SUPABASE_URL and SUPABASE_SERVICE_KEY , head to your Supabase project's API settings . SUPABASE_URL corresponds to the Project URL SUPABASE_SERVICE_KEY corresponds to the service_role API key export SUPABASE_URL = export SUPABASE_SERVICE_KEY = export OPENAI_API_KEY = Setup Supabase Database ​ Use these steps to setup your Supabase database if you haven't already. Head over to https://database.new to provision your Supabase database. In the studio, jump to the SQL editor and run the following script to enable pgvector and setup your database as a vector store: -- Enable the pgvector extension to work with embedding vectors create extension if not exists vector ; -- Create a table to store your documents create table documents ( id uuid primary key , content text , -- corresponds to Document.pageContent metadata jsonb , -- corresponds to Document.metadata embedding vector ( 1536 ) -- 1536 works for OpenAI embeddings, change as needed ) ; -- Create a function to search for documents create function match_documents ( query_embedding vector ( 1536 ) , filter jsonb default '{}' ) returns table ( id uuid , content text , metadata jsonb , similarity float ) language plpgsql as $$ #variable_conflict use_column begin return query select id , content , metadata , 1 - ( documents . embedding <=> query_embedding ) as similarity from documents where metadata @ > filter order by documents . embedding <=> query_embedding ; end ; $$ ; Setup Environment Variables ​ Since we are using SupabaseVectorStore and OpenAIEmbeddings , we need to load their API keys. Usage ​ First, install the LangChain CLI: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-supabase If you want to add this to an existing project, you can just run: langchain app add rag-supabase And add the following code to your server.py file: from rag_supabase . chain import chain as rag_supabase_chain add_routes ( app , rag_supabase_chain , path = \"/rag-supabase\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-supabase/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-supabase\" ) TODO: Add details about setting up the Supabase database Previous rag-singlestoredb Next rag-timescale-conversation"
    },
    {
        "title": "rag-timescale-conversation | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-timescale-conversation",
        "text_content": "Templates rag-timescale-conversation On this page rag-timescale-conversation This template is used for conversational retrieval , which is one of the most popular LLM use-cases. It passes both a conversation history and retrieved documents into an LLM for synthesis. Environment Setup ​ This template uses Timescale Vector as a vectorstore and requires that TIMESCALES_SERVICE_URL . Signup for a 90-day trial here if you don't yet have an account. To load the sample dataset, set LOAD_SAMPLE_DATA=1 . To load your own dataset see the section below. Set the OPENAI_API_KEY environment variable to access the OpenAI models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U \"langchain-cli[serve]\" To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-timescale-conversation If you want to add this to an existing project, you can just run: langchain app add rag-timescale-conversation And add the following code to your server.py file: from rag_timescale_conversation import chain as rag_timescale_conversation_chain add_routes ( app , rag_timescale_conversation_chain , path = \"/rag-timescale_conversation\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-timescale-conversation/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-timescale-conversation\" ) See the rag_conversation.ipynb notebook for example usage. Loading your own dataset ​ To load your own dataset you will have to create a load_dataset function. You can see an example, in the load_ts_git_dataset function defined in the load_sample_dataset.py file. You can then run this as a\nstandalone function (e.g. in a bash script) or add it to chain.py (but then you should run it just once). Previous rag_supabase Next RAG with Timescale Vector using hybrid search"
    },
    {
        "title": "RAG with Timescale Vector using hybrid search | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-timescale-hybrid-search-time",
        "text_content": "Templates RAG with Timescale Vector using hybrid search On this page RAG with Timescale Vector using hybrid search This template shows how to use timescale-vector with the self-query retriver to perform hybrid search on similarity and time.\nThis is useful any time your data has a strong time-based component. Some examples of such data are: News articles (politics, business, etc) Blog posts, documentation or other published material (public or private). Social media posts Changelogs of any kind Messages Such items are often searched by both similarity and time. For example: Show me all news about Toyota trucks from 2022. Timescale Vector provides superior performance when searching for embeddings within a particular timeframe by leveraging automatic table partitioning to isolate data for particular time-ranges. Langchain's self-query retriever allows deducing time-ranges (as well as other search criteria) from the text of user queries. What is Timescale Vector? ​ Timescale Vector is PostgreSQL++ for AI applications. Timescale Vector enables you to efficiently store and query billions of vector embeddings in PostgreSQL . Enhances pgvector with faster and more accurate similarity search on 1B+ vectors via DiskANN inspired indexing algorithm. Enables fast time-based vector search via automatic time-based partitioning and indexing. Provides a familiar SQL interface for querying vector embeddings and relational data. Timescale Vector is cloud PostgreSQL for AI that scales with you from POC to production: Simplifies operations by enabling you to store relational metadata, vector embeddings, and time-series data in a single database. Benefits from rock-solid PostgreSQL foundation with enterprise-grade feature liked streaming backups and replication, high-availability and row-level security. Enables a worry-free experience with enterprise-grade security and compliance. How to access Timescale Vector ​ Timescale Vector is available on Timescale , the cloud PostgreSQL platform. (There is no self-hosted version at this time.) LangChain users get a 90-day free trial for Timescale Vector. To get started, signup to Timescale, create a new database and follow this notebook! See the installation instructions for more details on using Timescale Vector in python. Environment Setup ​ This template uses Timescale Vector as a vectorstore and requires that TIMESCALES_SERVICE_URL . Signup for a 90-day trial here if you don't yet have an account. To load the sample dataset, set LOAD_SAMPLE_DATA=1 . To load your own dataset see the section below. Set the OPENAI_API_KEY environment variable to access the OpenAI models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-timescale-hybrid-search-time If you want to add this to an existing project, you can just run: langchain app add rag-timescale-hybrid-search-time And add the following code to your server.py file: from rag_timescale_hybrid_search . chain import chain as rag_timescale_hybrid_search_chain add_routes ( app , rag_timescale_hybrid_search_chain , path = \"/rag-timescale-hybrid-search\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-timescale-hybrid-search/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-timescale-hybrid-search\" ) Loading your own dataset ​ To load your own dataset you will have to modify the code in the DATASET SPECIFIC CODE section of chain.py .\nThis code defines the name of the collection, how to load the data, and the human-language description of both the\ncontents of the collection and all of the metadata. The human-language descriptions are used by the self-query retriever\nto help the LLM convert the question into filters on the metadata when searching the data in Timescale-vector. Previous rag-timescale-conversation Next rag-vectara-multiquery"
    },
    {
        "title": "rag-vectara-multiquery | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-vectara-multiquery",
        "text_content": "Templates rag-vectara-multiquery On this page rag-vectara-multiquery This template performs multiquery RAG with vectara. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. Also, ensure the following environment variables are set: VECTARA_CUSTOMER_ID VECTARA_CORPUS_ID VECTARA_API_KEY Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-vectara If you want to add this to an existing project, you can just run: langchain app add rag-vectara And add the following code to your server.py file: from rag_vectara import chain as rag_vectara_chain add_routes ( app , rag_vectara_chain , path = \"/rag-vectara\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"vectara-demo\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-vectara/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-vectara\" ) Previous RAG with Timescale Vector using hybrid search Next rag-vectara"
    },
    {
        "title": "rag-vectara | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-vectara",
        "text_content": "Templates rag-vectara On this page rag-vectara This template performs RAG with vectara. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. Also, ensure the following environment variables are set: VECTARA_CUSTOMER_ID VECTARA_CORPUS_ID VECTARA_API_KEY Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-vectara If you want to add this to an existing project, you can just run: langchain app add rag-vectara And add the following code to your server.py file: from rag_vectara import chain as rag_vectara_chain add_routes ( app , rag_vectara_chain , path = \"/rag-vectara\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"vectara-demo\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-vectara/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-vectara\" ) Previous rag-vectara-multiquery Next rag-weaviate"
    },
    {
        "title": "rag-weaviate | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rag-weaviate",
        "text_content": "Templates rag-weaviate On this page rag-weaviate This template performs RAG with Weaviate. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. Also, ensure the following environment variables are set: WEAVIATE_ENVIRONMENT WEAVIATE_API_KEY Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rag-weaviate If you want to add this to an existing project, you can just run: langchain app add rag-weaviate And add the following code to your server.py file: from rag_weaviate import chain as rag_weaviate_chain add_routes ( app , rag_weaviate_chain , path = \"/rag-weaviate\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rag-weaviate/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rag-weaviate\" ) Previous rag-vectara Next research-assistant"
    },
    {
        "title": "research-assistant | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/research-assistant",
        "text_content": "Templates research-assistant On this page research-assistant This template implements a version of GPT Researcher that you can use\nas a starting point for a research agent. Environment Setup ​ The default template relies on ChatOpenAI and DuckDuckGo, so you will need the\nfollowing environment variable: OPENAI_API_KEY And to use the Tavily LLM-optimized search engine, you will need: TAVILY_API_KEY Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package research-assistant If you want to add this to an existing project, you can just run: langchain app add research-assistant And add the following code to your server.py file: from research_assistant import chain as research_assistant_chain add_routes ( app , research_assistant_chain , path = \"/research-assistant\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/research-assistant/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/research-assistant\" ) Previous rag-weaviate Next retrieval-agent"
    },
    {
        "title": "retrieval-agent | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/retrieval-agent",
        "text_content": "Templates retrieval-agent On this page retrieval-agent This package uses Azure OpenAI to do retrieval using an agent architecture.\nBy default, this does retrieval over Arxiv. Environment Setup ​ Since we are using Azure OpenAI, we will need to set the following environment variables: export AZURE_OPENAI_API_BASE = .. . export AZURE_OPENAI_API_VERSION = .. . export AZURE_OPENAI_API_KEY = .. . export AZURE_OPENAI_DEPLOYMENT_NAME = .. . Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package retrieval-agent If you want to add this to an existing project, you can just run: langchain app add retrieval-agent And add the following code to your server.py file: from retrieval_agent import chain as retrieval_agent_chain add_routes ( app , retrieval_agent_chain , path = \"/retrieval-agent\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/retrieval-agent/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/retrieval-agent\" ) Previous research-assistant Next rewrite_retrieve_read"
    },
    {
        "title": "rewrite_retrieve_read | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/rewrite-retrieve-read",
        "text_content": "Templates rewrite_retrieve_read On this page rewrite_retrieve_read This template implemenets a method for query transformation (re-writing) in the paper Query Rewriting for Retrieval-Augmented Large Language Models to optimize for RAG. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package rewrite_retrieve_read If you want to add this to an existing project, you can just run: langchain app add rewrite_retrieve_read And add the following code to your server.py file: from rewrite_retrieve_read . chain import chain as rewrite_retrieve_read_chain add_routes ( app , rewrite_retrieve_read_chain , path = \"/rewrite-retrieve-read\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/rewrite_retrieve_read/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/rewrite_retrieve_read\" ) Previous retrieval-agent Next self-query-qdrant"
    },
    {
        "title": "self-query-qdrant | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/self-query-qdrant",
        "text_content": "Templates self-query-qdrant On this page self-query-qdrant This template performs self-querying using Qdrant and OpenAI. By default, it uses an artificial dataset of 10 documents, but you can replace it with your own dataset. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. Set the QDRANT_URL to the URL of your Qdrant instance. If you use Qdrant Cloud you have to set the QDRANT_API_KEY environment variable as well. If you do not set any of them,\nthe template will try to connect a local Qdrant instance at http://localhost:6333 . export QDRANT_URL = export QDRANT_API_KEY = export OPENAI_API_KEY = Usage ​ To use this package, install the LangChain CLI first: pip install -U \"langchain-cli[serve]\" Create a new LangChain project and install this package as the only one: langchain app new my-app --package self-query-qdrant To add this to an existing project, run: langchain app add self-query-qdrant Defaults ​ Before you launch the server, you need to create a Qdrant collection and index the documents.\nIt can be done by running the following command: from self_query_qdrant . chain import initialize initialize ( ) Add the following code to your app/server.py file: from self_query_qdrant . chain import chain add_routes ( app , chain , path = \"/self-query-qdrant\" ) The default dataset consists 10 documents about dishes, along with their price and restaurant information.\nYou can find the documents in the packages/self-query-qdrant/self_query_qdrant/defaults.py file.\nHere is one of the documents: from langchain . schema import Document Document ( page_content = \"Spaghetti with meatballs and tomato sauce\" , metadata = { \"price\" : 12.99 , \"restaurant\" : { \"name\" : \"Olive Garden\" , \"location\" : [ \"New York\" , \"Chicago\" , \"Los Angeles\" ] , } , } , ) The self-querying allows performing semantic search over the documents, with some additional filtering\nbased on the metadata. For example, you can search for the dishes that cost less than $15 and are served in New York. Customization ​ All the examples above assume that you want to launch the template with just the defaults.\nIf you want to customize the template, you can do it by passing the parameters to the create_chain function\nin the app/server.py file: from langchain . llms import Cohere from langchain . embeddings import HuggingFaceEmbeddings from langchain . chains . query_constructor . schema import AttributeInfo from self_query_qdrant . chain import create_chain chain = create_chain ( llm = Cohere ( ) , embeddings = HuggingFaceEmbeddings ( ) , document_contents = \"Descriptions of cats, along with their names and breeds.\" , metadata_field_info = [ AttributeInfo ( name = \"name\" , description = \"Name of the cat\" , type = \"string\" ) , AttributeInfo ( name = \"breed\" , description = \"Cat's breed\" , type = \"string\" ) , ] , collection_name = \"cats\" , ) The same goes for the initialize function that creates a Qdrant collection and indexes the documents: from langchain . schema import Document from langchain . embeddings import HuggingFaceEmbeddings from self_query_qdrant . chain import initialize initialize ( embeddings = HuggingFaceEmbeddings ( ) , collection_name = \"cats\" , documents = [ Document ( page_content = \"A mean lazy old cat who destroys furniture and eats lasagna\" , metadata = { \"name\" : \"Garfield\" , \"breed\" : \"Tabby\" } , ) , . . . ] ) The template is flexible and might be used for different sets of documents easily. LangSmith ​ (Optional) If you have access to LangSmith, configure it to help trace, monitor and debug LangChain applications. If you don't have access, skip this section. export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve Local Server ​ This will start the FastAPI app with a server running locally at http://localhost:8000 You can see all templates at http://127.0.0.1:8000/docs Access the playground at http://127.0.0.1:8000/self-query-qdrant/playground Access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/self-query-qdrant\" ) Previous rewrite_retrieve_read Next self-query-supabase"
    },
    {
        "title": "self-query-supabase | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/self-query-supabase",
        "text_content": "Templates self-query-supabase On this page self-query-supabase This templates allows natural language structured quering of Supabase. Supabase is an open-source alternative to Firebase, built on top of PostgreSQL . It uses pgvector to store embeddings within your tables. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. To get your OPENAI_API_KEY , navigate to API keys on your OpenAI account and create a new secret key. To find your SUPABASE_URL and SUPABASE_SERVICE_KEY , head to your Supabase project's API settings . SUPABASE_URL corresponds to the Project URL SUPABASE_SERVICE_KEY corresponds to the service_role API key export SUPABASE_URL = export SUPABASE_SERVICE_KEY = export OPENAI_API_KEY = Setup Supabase Database ​ Use these steps to setup your Supabase database if you haven't already. Head over to https://database.new to provision your Supabase database. In the studio, jump to the SQL editor and run the following script to enable pgvector and setup your database as a vector store: -- Enable the pgvector extension to work with embedding vectors create extension if not exists vector ; -- Create a table to store your documents create table documents ( id uuid primary key , content text , -- corresponds to Document.pageContent metadata jsonb , -- corresponds to Document.metadata embedding vector ( 1536 ) -- 1536 works for OpenAI embeddings, change as needed ) ; -- Create a function to search for documents create function match_documents ( query_embedding vector ( 1536 ) , filter jsonb default '{}' ) returns table ( id uuid , content text , metadata jsonb , similarity float ) language plpgsql as $$ #variable_conflict use_column begin return query select id , content , metadata , 1 - ( documents . embedding <=> query_embedding ) as similarity from documents where metadata @ > filter order by documents . embedding <=> query_embedding ; end ; $$ ; Usage ​ To use this package, install the LangChain CLI first: pip install -U langchain-cli Create a new LangChain project and install this package as the only one: langchain app new my-app --package self-query-supabase To add this to an existing project, run: langchain app add self-query-supabase Add the following code to your server.py file: from self_query_supabase . chain import chain as self_query_supabase_chain add_routes ( app , self_query_supabase_chain , path = \"/self-query-supabase\" ) (Optional) If you have access to LangSmith, configure it to help trace, monitor and debug LangChain applications. If you don't have access, skip this section. export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server running locally at http://localhost:8000 You can see all templates at http://127.0.0.1:8000/docs Access the playground at http://127.0.0.1:8000/self-query-supabase/playground Access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/self-query-supabase\" ) TODO: Instructions to set up the Supabase database and install the package. Previous self-query-qdrant Next skeleton-of-thought"
    },
    {
        "title": "skeleton-of-thought | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/skeleton-of-thought",
        "text_content": "Templates skeleton-of-thought On this page skeleton-of-thought Implements \"Skeleton of Thought\" from this paper. This technique makes it possible to generate longer generations more quickly by first generating a skeleton, then generating each point of the outline. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. To get your OPENAI_API_KEY , navigate to API keys on your OpenAI account and create a new secret key. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package skeleton-of-thought If you want to add this to an existing project, you can just run: langchain app add skeleton-of-thought And add the following code to your server.py file: from skeleton_of_thought import chain as skeleton_of_thought_chain add_routes ( app , skeleton_of_thought_chain , path = \"/skeleton-of-thought\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/skeleton-of-thought/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/skeleton-of-thought\" ) Previous self-query-supabase Next solo-performance-prompting-agent"
    },
    {
        "title": "solo-performance-prompting-agent | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/solo-performance-prompting-agent",
        "text_content": "Templates solo-performance-prompting-agent On this page solo-performance-prompting-agent This template creates an agent that transforms a single LLM into a cognitive synergist  by engaging in multi-turn self-collaboration with multiple personas.\nA cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. This template will use the DuckDuckGo search API. Environment Setup ​ This template will use OpenAI by default.\nBe sure that OPENAI_API_KEY is set in your environment. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package solo-performance-prompting-agent If you want to add this to an existing project, you can just run: langchain app add solo-performance-prompting-agent And add the following code to your server.py file: from solo_performance_prompting_agent . agent import agent_executor as solo_performance_prompting_agent_chain add_routes ( app , solo_performance_prompting_agent_chain , path = \"/solo-performance-prompting-agent\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/solo-performance-prompting-agent/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/solo-performance-prompting-agent\" ) Previous skeleton-of-thought Next sql-llama2"
    },
    {
        "title": "sql-llama2 | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/sql-llama2",
        "text_content": "Templates sql-llama2 On this page sql-llama2 This template enables a user to interact with a SQL database using natural language. It uses LLamA2-13b hosted by Replicate , but can be adapted to any API that supports LLaMA2 including Fireworks . The template includes an example database of 2023 NBA rosters. For more information on how to build this database, see here . Environment Setup ​ Ensure the REPLICATE_API_TOKEN is set in your environment. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package sql-llama2 If you want to add this to an existing project, you can just run: langchain app add sql-llama2 And add the following code to your server.py file: from sql_llama2 import chain as sql_llama2_chain add_routes ( app , sql_llama2_chain , path = \"/sql-llama2\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/sql-llama2/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/sql-llama2\" ) Previous solo-performance-prompting-agent Next sql-llamacpp"
    },
    {
        "title": "sql-llamacpp | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/sql-llamacpp",
        "text_content": "Templates sql-llamacpp On this page sql-llamacpp This template enables a user to interact with a SQL database using natural language. It uses Mistral-7b via llama.cpp to run inference locally on a Mac laptop. Environment Setup ​ To set up the environment, use the following steps: wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh bash Miniforge3-MacOSX-arm64.sh conda create -n llama python = 3.9 .16 conda activate /Users/rlm/miniforge3/envs/llama CMAKE_ARGS = \"-DLLAMA_METAL=on\" FORCE_CMAKE = 1 pip install -U llama-cpp-python --no-cache-dir Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package sql-llamacpp If you want to add this to an existing project, you can just run: langchain app add sql-llamacpp And add the following code to your server.py file: from sql_llamacpp import chain as sql_llamacpp_chain add_routes ( app , sql_llamacpp_chain , path = \"/sql-llamacpp\" ) The package will download the Mistral-7b model from here . You can select other files and specify their download path (browse here ). This package includes an example DB of 2023 NBA rosters. You can see instructions to build this DB here . (Optional) Configure LangSmith for tracing, monitoring and debugging LangChain applications. LangSmith is currently in private beta, you can sign up here . If you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server running locally at http://localhost:8000 You can see all templates at http://127.0.0.1:8000/docs You can access the playground at http://127.0.0.1:8000/sql-llamacpp/playground You can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/sql-llamacpp\" ) Previous sql-llama2 Next sql-ollama"
    },
    {
        "title": "sql-ollama | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/sql-ollama",
        "text_content": "Templates sql-ollama On this page sql-ollama This template enables a user to interact with a SQL database using natural language. It uses Zephyr-7b via Ollama to run inference locally on a Mac laptop. Environment Setup ​ Before using this template, you need to set up Ollama and SQL database. Follow instructions here to download Ollama. Download your LLM of interest: This package uses zephyr : ollama pull zephyr You can choose from many LLMs here This package includes an example DB of 2023 NBA rosters. You can see instructions to build this DB here . Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package sql-ollama If you want to add this to an existing project, you can just run: langchain app add sql-ollama And add the following code to your server.py file: from sql_ollama import chain as sql_ollama_chain add_routes ( app , sql_ollama_chain , path = \"/sql-ollama\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/sql-ollama/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/sql-ollama\" ) Previous sql-llamacpp Next sql-pgvector"
    },
    {
        "title": "sql-pgvector | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/sql-pgvector",
        "text_content": "Templates sql-pgvector On this page sql-pgvector This template enables user to use pgvector for combining postgreSQL with semantic search / RAG. It uses PGVector extension as shown in the RAG empowered SQL cookbook Environment Setup ​ If you are using ChatOpenAI as your LLM, make sure the OPENAI_API_KEY is set in your environment. You can change both the LLM and embeddings model inside chain.py And you can configure configure the following environment variables\nfor use by the template (defaults are in parentheses) POSTGRES_USER (postgres) POSTGRES_PASSWORD (test) POSTGRES_DB (vectordb) POSTGRES_HOST (localhost) POSTGRES_PORT (5432) If you don't have a postgres instance, you can run one locally in docker: docker run \\ --name some-postgres \\ -e POSTGRES_PASSWORD = test \\ -e POSTGRES_USER = postgres \\ -e POSTGRES_DB = vectordb \\ -p 5432 :5432 \\ postgres:16 And to start again later, use the --name defined above: docker start some-postgres PostgreSQL Database setup ​ Apart from having pgvector extension enabled, you will need to do some setup before being able to run semantic search within your SQL queries. In order to run RAG over your postgreSQL database you will need to generate the embeddings for the specific columns you want. This process is covered in the RAG empowered SQL cookbook , but the overall approach consist of: Querying for unique values in the column Generating embeddings for those values Store the embeddings in a separate column or in an auxiliary table. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package sql-pgvector If you want to add this to an existing project, you can just run: langchain app add sql-pgvector And add the following code to your server.py file: from sql_pgvector import chain as sql_pgvector_chain add_routes ( app , sql_pgvector_chain , path = \"/sql-pgvector\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/sql-pgvector/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/sql-pgvector\" ) Previous sql-ollama Next sql-research-assistant"
    },
    {
        "title": "sql-research-assistant | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/sql-research-assistant",
        "text_content": "Templates sql-research-assistant On this page sql-research-assistant This package does research over a SQL database Usage ​ This package relies on multiple models, which have the following dependencies: OpenAI: set the OPENAI_API_KEY environment variables Ollama: install and run Ollama llama2 (on Ollama): ollama pull llama2 (otherwise you will get 404 errors from Ollama) To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package sql-research-assistant If you want to add this to an existing project, you can just run: langchain app add sql-research-assistant And add the following code to your server.py file: from sql_research_assistant import chain as sql_research_assistant_chain add_routes ( app , sql_research_assistant_chain , path = \"/sql-research-assistant\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/sql-research-assistant/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/sql-research-assistant\" ) Previous sql-pgvector Next stepback-qa-prompting"
    },
    {
        "title": "stepback-qa-prompting | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/stepback-qa-prompting",
        "text_content": "Templates stepback-qa-prompting On this page stepback-qa-prompting This template replicates the \"Step-Back\" prompting technique that improves performance on complex questions by first asking a \"step back\" question. This technique can be combined with regular question-answering applications by doing retrieval on both the original and step-back question. Read more about this in the paper here and an excellent blog post by Cobus Greyling here We will modify the prompts slightly to work better with chat models in this template. Environment Setup ​ Set the OPENAI_API_KEY environment variable to access the OpenAI models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package stepback-qa-prompting If you want to add this to an existing project, you can just run: langchain app add stepback-qa-prompting And add the following code to your server.py file: from stepback_qa_prompting . chain import chain as stepback_qa_prompting_chain add_routes ( app , stepback_qa_prompting_chain , path = \"/stepback-qa-prompting\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/stepback-qa-prompting/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/stepback-qa-prompting\" ) Previous sql-research-assistant Next summarize-anthropic"
    },
    {
        "title": "summarize-anthropic | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/summarize-anthropic",
        "text_content": "Templates summarize-anthropic On this page summarize-anthropic This template uses Anthropic's Claude2 to summarize long documents. It leverages a large context window of 100k tokens, allowing for summarization of documents over 100 pages. You can see the summarization prompt in chain.py . Environment Setup ​ Set the ANTHROPIC_API_KEY environment variable to access the Anthropic models. Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package summarize-anthropic If you want to add this to an existing project, you can just run: langchain app add summarize-anthropic And add the following code to your server.py file: from summarize_anthropic import chain as summarize_anthropic_chain add_routes ( app , summarize_anthropic_chain , path = \"/summarize-anthropic\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/summarize-anthropic/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/summarize-anthropic\" ) Previous stepback-qa-prompting Next vertexai-chuck-norris"
    },
    {
        "title": "vertexai-chuck-norris | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/vertexai-chuck-norris",
        "text_content": "Templates vertexai-chuck-norris On this page vertexai-chuck-norris This template makes jokes about Chuck Norris using Vertex AI PaLM2. Environment Setup ​ First, make sure you have a Google Cloud project with\nan active billing account, and have the gcloud CLI installed . Configure application default credentials : gcloud auth application-default login To set a default Google Cloud project to use, run this command and set the project ID of the project you want to use: gcloud config set project [ PROJECT-ID ] Enable the Vertex AI API for the project: gcloud services enable aiplatform.googleapis.com Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package pirate-speak If you want to add this to an existing project, you can just run: langchain app add vertexai-chuck-norris And add the following code to your server.py file: from vertexai_chuck_norris . chain import chain as vertexai_chuck_norris_chain add_routes ( app , vertexai_chuck_norris_chain , path = \"/vertexai-chuck-norris\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/vertexai-chuck-norris/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/vertexai-chuck-norris\" ) Previous summarize-anthropic Next xml-agent"
    },
    {
        "title": "xml-agent | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/templates/xml-agent",
        "text_content": "Templates xml-agent On this page xml-agent This package creates an agent that uses XML syntax to communicate its decisions of what actions to take. It uses Anthropic's Claude models for writing XML syntax and can optionally look up things on the internet using DuckDuckGo. Environment Setup ​ Two environment variables need to be set: ANTHROPIC_API_KEY : Required for using Anthropic Usage ​ To use this package, you should first have the LangChain CLI installed: pip install -U langchain-cli To create a new LangChain project and install this as the only package, you can do: langchain app new my-app --package xml-agent If you want to add this to an existing project, you can just run: langchain app add xml-agent And add the following code to your server.py file: from xml_agent import agent_executor as xml_agent_chain add_routes ( app , xml_agent_chain , path = \"/xml-agent\" ) (Optional) Let's now configure LangSmith.\nLangSmith will help us trace, monitor and debug LangChain applications.\nLangSmith is currently in private beta, you can sign up here .\nIf you don't have access, you can skip this section export LANGCHAIN_TRACING_V2 = true export LANGCHAIN_API_KEY = < your-api-key > export LANGCHAIN_PROJECT = < your-project > # if not specified, defaults to \"default\" If you are inside this directory, then you can spin up a LangServe instance directly by: langchain serve This will start the FastAPI app with a server is running locally at http://localhost:8000 We can see all templates at http://127.0.0.1:8000/docs We can access the playground at http://127.0.0.1:8000/xml-agent/playground We can access the template from code with: from langserve . client import RemoteRunnable runnable = RemoteRunnable ( \"http://localhost:8000/xml-agent\" ) Previous vertexai-chuck-norris"
    },
    {
        "title": "Welcome Contributors | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/contributing/",
        "text_content": "Welcome Contributors On this page Welcome Contributors Hi there! Thank you for even being interested in contributing to LangChain.\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether they involve new features, improved infrastructure, better documentation, or bug fixes. 🗺️ Guidelines ​ 👩‍💻 Ways to contribute ​ There are many ways to contribute to LangChain. Here are some common ways people contribute: Documentation : Help improve our docs, including this one! Code : Help us write code, fix bugs, or improve our infrastructure. Integrations : Help us integrate with your favorite vendors and tools. 🚩GitHub Issues ​ Our issues page is kept up to date with bugs, improvements, and feature requests. There is a taxonomy of labels to help with sorting and discovery of issues of interest. Please use these to help organize issues. If you start working on an issue, please assign it to yourself. If you are adding an issue, please try to keep it focused on a single, modular bug/improvement/feature.\nIf two issues are related, or blocking, please link them rather than combining them. We will try to keep these issues as up-to-date as possible, though\nwith the rapid rate of development in this field some may get out of date.\nIf you notice this happening, please let us know. 🙋Getting Help ​ Our goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please\ncontact a maintainer! Not only do we want to help get you unblocked, but we also want to make sure that the process is\nsmooth for future contributors. In a similar vein, we do enforce certain linting, formatting, and documentation standards in the codebase.\nIf you are finding these difficult (or even just annoying) to work with, feel free to contact a maintainer for help -\nwe do not want these to get in the way of getting good code into the codebase. Next Contribute Code"
    },
    {
        "title": "Contribute Code | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/contributing/code",
        "text_content": "Contribute Code On this page Contribute Code To contribute to this project, please follow the \"fork and pull request\" workflow.\nPlease do not try to push directly to this repo unless you are a maintainer. Please follow the checked-in pull request template when opening pull requests. Note related issues and tag relevant\nmaintainers. Pull requests cannot land without passing the formatting, linting, and testing checks first. See Testing and Formatting and Linting for how to run these checks locally. It's essential that we maintain great documentation and testing. If you: Fix a bug Add a relevant unit or integration test when possible. These live in tests/unit_tests and tests/integration_tests . Make an improvement Update any affected example notebooks and documentation. These live in docs . Update unit and integration tests when relevant. Add a feature Add a demo notebook in docs/docs/ . Add unit and integration tests. We are a small, progress-oriented team. If there's something you'd like to add or change, opening a pull request is the\nbest way to get our attention. 🚀 Quick Start ​ This quick start guide explains how to run the repository locally.\nFor a development container , see the .devcontainer folder . Dependency Management: Poetry and other env/dependency managers ​ This project utilizes Poetry v1.6.1+ as a dependency manager. ❗Note: Before installing Poetry , if you use Conda , create and activate a new Conda env (e.g. conda create -n langchain python=3.9 ) Install Poetry: documentation on how to install it . ❗Note: If you use Conda or Pyenv as your environment/package manager, after installing Poetry,\ntell Poetry to use the virtualenv python environment ( poetry config virtualenvs.prefer-active-python true ) Different packages ​ This repository contains multiple packages: langchain-core : Base interfaces for key abstractions as well as logic for combining them in chains (LangChain Expression Language). langchain-community : Third-party integrations of various components. langchain : Chains, agents, and retrieval logic that makes up the cognitive architecture of your applications. langchain-experimental : Components and chains that are experimental, either in the sense that the techniques are novel and still being tested, or they require giving the LLM more access than would be possible in most production systems. Partner integrations: Partner packages in libs/partners that are independently version controlled. Each of these has its own development environment. Docs are run from the top-level makefile, but development\nis split across separate test & release flows. For this quickstart, start with langchain-community: cd libs/community Local Development Dependencies ​ Install langchain-community development requirements (for running langchain, running examples, linting, formatting, tests, and coverage): poetry install --with lint,typing,test,test_integration Then verify dependency installation: make test If during installation you receive a WheelFileValidationError for debugpy , please make sure you are running\nPoetry v1.6.1+. This bug was present in older versions of Poetry (e.g. 1.4.1) and has been resolved in newer releases.\nIf you are still seeing this bug on v1.6.1, you may also try disabling \"modern installation\"\n( poetry config installer.modern-installation false ) and re-installing requirements.\nSee this debugpy issue for more details. Testing ​ In langchain , langchain-community , and langchain-experimental , some test dependencies are optional; see section about optional dependencies . Unit tests cover modular logic that does not require calls to outside APIs.\nIf you add new logic, please add a unit test. To run unit tests: make test To run unit tests in Docker: make docker_tests There are also integration tests and code-coverage available. Only develop langchain_core or langchain_experimental ​ If you are only developing langchain_core or langchain_experimental , you can simply install the dependencies for the respective projects and run tests: cd libs/core poetry install --with test make test Or: cd libs/experimental poetry install --with test make test Formatting and Linting ​ Run these locally before submitting a PR; the CI system will check also. Code Formatting ​ Formatting for this project is done via ruff . To run formatting for docs, cookbook and templates: make format To run formatting for a library, run the same command from the relevant library directory: cd libs/ { LIBRARY } make format Additionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command: make format_diff This is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase. Linting ​ Linting for this project is done via a combination of ruff and mypy . To run linting for docs, cookbook and templates: make lint To run linting for a library, run the same command from the relevant library directory: cd libs/ { LIBRARY } make lint In addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command: make lint_diff This can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase. We recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed. Spellcheck ​ Spellchecking for this project is done via codespell .\nNote that codespell finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words. To check spelling for this project: make spell_check To fix spelling in place: make spell_fix If codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the pyproject.toml file. [ tool . codespell ] . . . # Add here: ignore - words - list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure' Working with Optional Dependencies ​ langchain , langchain-community , and langchain-experimental rely on optional dependencies to keep these packages lightweight. langchain-core and partner packages do not use optional dependencies in this way. You only need to add a new dependency if a unit test relies on the package.\nIf your package is only required for integration tests , then you can skip these\nsteps and leave all pyproject.toml and poetry.lock files alone. If you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed. Users who do not have the dependency installed should be able to import your code without\nany side effects (no warnings, no errors, no exceptions). To introduce the dependency to the pyproject.toml file correctly, please do the following: Add the dependency to the main group as an optional dependency poetry add --optional [ package_name ] Open pyproject.toml and add the dependency to the extended_testing extra Relock the poetry file to update the extra. poetry lock --no-update Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code. Please use the @pytest.mark.requires(package_name) decorator for any tests that require the dependency. Adding a Jupyter Notebook ​ If you are adding a Jupyter Notebook example, you'll want to install the optional dev dependencies. To install dev dependencies: poetry install --with dev Launch a notebook: poetry run jupyter notebook When you run poetry install , the langchain package is installed as editable in the virtualenv, so your new logic can be imported into the notebook. Previous Welcome Contributors Next Testing"
    },
    {
        "title": "Testing | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/contributing/testing",
        "text_content": "Testing On this page Testing All of our packages have unit tests and integration tests, and we favor unit tests over integration tests. Unit tests run on every pull request, so they should be fast and reliable. Integration tests run once a day, and they require more setup, so they should be reserved for confirming interface points with external services. Unit Tests ​ Unit tests cover modular logic that does not require calls to outside APIs.\nIf you add new logic, please add a unit test. To install dependencies for unit tests: poetry install --with test To run unit tests: make test To run unit tests in Docker: make docker_tests To run a specific test: TEST_FILE = tests/unit_tests/test_imports.py make test Integration Tests ​ Integration tests cover logic that requires making calls to outside APIs (often integration with other services).\nIf you add support for a new external API, please add a new integration test. Warning: Almost no tests should be integration tests. Tests that require making network connections make it difficult for other\ndevelopers to test the code. Instead favor relying on responses library and/or mock.patch to mock\nrequests using small fixtures. To install dependencies for integration tests: poetry install --with test,test_integration To run integration tests: make integration_tests Prepare ​ The integration tests use several search engines and databases. The tests\naim to verify the correct behavior of the engines and databases according to\ntheir specifications and requirements. To run some integration tests, such as tests located in tests/integration_tests/vectorstores/ , you will need to install the following\nsoftware: Docker Python 3.8.1 or later Any new dependencies should be added by running: # add package and install it after adding: poetry add tiktoken@latest --group \"test_integration\" && poetry install --with test_integration Before running any tests, you should start a specific Docker container that has all the\nnecessary dependencies installed. For instance, we use the elasticsearch.yml container\nfor test_elasticsearch.py : cd tests/integration_tests/vectorstores/docker-compose docker-compose -f elasticsearch.yml up For environments that requires more involving preparation, look for *.sh . For instance, opensearch.sh builds a required docker image and then launch opensearch. Prepare environment variables for local testing: ​ copy tests/integration_tests/.env.example to tests/integration_tests/.env set variables in tests/integration_tests/.env file, e.g OPENAI_API_KEY Additionally, it's important to note that some integration tests may require certain\nenvironment variables to be set, such as OPENAI_API_KEY . Be sure to set any required\nenvironment variables before running the tests to ensure they run correctly. Recording HTTP interactions with pytest-vcr ​ Some of the integration tests in this repository involve making HTTP requests to\nexternal services. To prevent these requests from being made every time the tests are\nrun, we use pytest-vcr to record and replay HTTP interactions. When running tests in a CI/CD pipeline, you may not want to modify the existing\ncassettes. You can use the --vcr-record=none command-line option to disable recording\nnew cassettes. Here's an example: pytest --log-cli-level = 10 tests/integration_tests/vectorstores/test_pinecone.py --vcr-record = none pytest tests/integration_tests/vectorstores/test_elasticsearch.py --vcr-record = none Run some tests with coverage: ​ pytest tests/integration_tests/vectorstores/test_elasticsearch.py --cov = langchain --cov-report = html start \"\" htmlcov/index.html || open htmlcov/index.html Coverage ​ Code coverage (i.e. the amount of code that is covered by unit tests) helps identify areas of the code that are potentially more or less brittle. Coverage requires the dependencies for integration tests: poetry install --with test_integration To get a report of current coverage, run the following: make coverage Previous Contribute Code Next Contribute Documentation"
    },
    {
        "title": "Contribute Documentation | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/contributing/documentation",
        "text_content": "Contribute Documentation On this page Contribute Documentation The docs directory contains Documentation and API Reference. Documentation is built using Quarto and Docusaurus 2 . API Reference are largely autogenerated by sphinx from the code and are hosted by Read the Docs .\nFor that reason, we ask that you add good documentation to all classes and methods. Similar to linting, we recognize documentation can be annoying. If you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed. Build Documentation Locally ​ Install dependencies ​ Quarto - package that converts Jupyter notebooks ( .ipynb files) into mdx files for serving in Docusaurus. poetry install from the monorepo root Building ​ In the following commands, the prefix api_ indicates that those are operations for the API Reference. Before building the documentation, it is always a good idea to clean the build directory: make docs_clean make api_docs_clean Next, you can build the documentation as outlined below: make docs_build make api_docs_build Finally, run the link checker to ensure all links are valid: make docs_linkcheck make api_docs_linkcheck Linting and Formatting ​ The docs are linted from the monorepo root. To lint the docs, run the following from there: poetry install --with lint,typing make lint If you have formatting-related errors, you can fix them automatically with: make format Verify Documentation changes ​ After pushing documentation changes to the repository, you can preview and verify that the changes are\nwhat you wanted by clicking the View deployment or Visit Preview buttons on the pull request Conversation page.\nThis will take you to a preview of the documentation changes.\nThis preview is created by Vercel . Previous Testing Next Package Versioning"
    },
    {
        "title": "📕 Package Versioning | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/contributing/packages",
        "text_content": "Package Versioning On this page 📕 Package Versioning As of now, LangChain has an ad hoc release process: releases are cut with high frequency by\na maintainer and published to PyPI .\nThe different packages are versioned slightly differently. langchain-core ​ langchain-core is currently on version 0.1.x . As langchain-core contains the base abstractions and runtime for the whole LangChain ecosystem, we will communicate any breaking changes with advance notice and version bumps. The exception for this is anything in langchain_core.beta . The reason for langchain_core.beta is that given the rate of change of the field, being able to move quickly is still a priority, and this module is our attempt to do so. Minor version increases will occur for: Breaking changes for any public interfaces NOT in langchain_core.beta Patch version increases will occur for: Bug fixes New features Any changes to private interfaces Any changes to langchain_core.beta langchain ​ langchain is currently on version 0.0.x All changes will be accompanied by a patch version increase. Any changes to public interfaces are nearly always done in a backwards compatible way and will be communicated ahead of time when they are not backwards compatible. We are targeting January 2024 for a release of langchain v0.1, at which point langchain will adopt the same versioning policy as langchain-core . langchain-community ​ langchain-community is currently on version 0.0.x All changes will be accompanied by a patch version increase. langchain-experimental ​ langchain-experimental is currently on version 0.0.x All changes will be accompanied by a patch version increase. Partner Packages ​ Partner packages are versioned independently. 🌟 Recognition If your contribution has made its way into a release, we will want to give you credit on Twitter (only if you want though)!\nIf you have a Twitter account you would like us to mention, please let us know in the PR or through another means. Previous Contribute Documentation Next Contribute Integrations"
    },
    {
        "title": "Contribute Integrations | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/contributing/integrations",
        "text_content": "Contribute Integrations On this page Contribute Integrations To begin, make sure you have all the dependencies outlined in guide on Contributing Code . There are a few different places you can contribute integrations for LangChain: Community : For lighter-weight integrations that are primarily maintained by LangChain and the Open Source Community. Partner Packages : For independent packages that are co-maintained by LangChain and a partner. For the most part, new integrations should be added to the Community package. Partner packages require more maintenance as separate packages, so please confirm with the LangChain team before creating a new partner package. In the following sections, we'll walk through how to contribute to each of these packages from a fake company, Parrot Link AI . Community Package ​ The langchain-community package is in libs/community and contains most integrations. It is installed by users with pip install langchain-community , and exported members can be imported with code like from langchain_community . chat_models import ParrotLinkLLM from langchain_community . llms import ChatParrotLink from langchain_community . vectorstores import ParrotLinkVectorStore The community package relies on manually-installed dependent packages, so you will see errors if you try to import a package that is not installed. In our fake example, if you tried to import ParrotLinkLLM without installing parrot-link-sdk , you will see an ImportError telling you to install it when trying to use it. Let's say we wanted to implement a chat model for Parrot Link AI. We would create a new file in libs/community/langchain_community/chat_models/parrot_link.py with the following code: from langchain_core . language_models . chat_models import BaseChatModel class ChatParrotLink ( BaseChatModel ) : \"\"\"ChatParrotLink chat model. Example: .. code-block:: python from langchain_parrot_link import ChatParrotLink model = ChatParrotLink() \"\"\" . . . And we would write tests in: Unit tests: libs/community/tests/unit_tests/chat_models/test_parrot_link.py Integration tests: libs/community/tests/integration_tests/chat_models/test_parrot_link.py And add documentation to: docs/docs/integrations/chat/parrot_link.ipynb `docs/docs/ Partner Packages ​ Partner packages are in libs/partners/* and are installed by users with pip install langchain-{partner} , and exported members can be imported with code like from langchain_ { partner } import X Set up a new package ​ To set up a new partner package, use the latest version of the LangChain CLI. You can install or update it with: pip install -U langchain-cli Let's say you want to create a new partner package working for a company called Parrot Link AI. Then, run the following command to create a new partner package: cd libs/partners langchain-cli integration new > Name: parrot-link > Name of integration in PascalCase [ ParrotLink ] : ParrotLink This will create a new package in libs/partners/parrot-link with the following structure: libs/partners/parrot-link/ langchain_parrot_link/ # folder containing your package ... tests/ ... docs/ # bootstrapped docs notebooks, must be moved to /docs in monorepo root ... scripts/ # scripts for CI ... LICENSE README.md # fill out with information about your package Makefile # default commands for CI pyproject.toml # package metadata, mostly managed by Poetry poetry.lock # package lockfile, managed by Poetry .gitignore Implement your package ​ First, add any dependencies your package needs, such as your company's SDK: poetry add parrot-link-sdk If you need separate dependencies for type checking, you can add them to the typing group with: poetry add --group typing types-parrot-link-sdk Then, implement your package in libs/partners/parrot-link/langchain_parrot_link . By default, this will include stubs for a Chat Model, an LLM, and/or a Vector Store. You should delete any of the files you won't use and remove them from __init__.py . Write Unit and Integration Tests ​ Some basic tests are generated in the tests/ directory. You should add more tests to cover your package's functionality. For information on running and implementing tests, see the Testing guide . Write documentation ​ Documentation is generated from Jupyter notebooks in the docs/ directory. You should move the generated notebooks to the relevant docs/docs/integrations directory in the monorepo root. Additional steps ​ Contributor steps: Add secret names to manual integrations workflow in .github/workflows/_integration_test.yml Add secrets to release workflow (for pre-release testing) in .github/workflows/_release.yml Maintainer steps (Contributors should not do these): set up pypi and test pypi projects add credential secrets to Github Actions add package to conda-forge Previous Package Versioning"
    },
    {
        "title": "Prompts | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/model_io/prompts",
        "text_content": "Modules Model I/O Prompts Prompts A prompt for a language model is a set of instructions or input provided by a user to\nguide the model's response, helping it understand the context and generate relevant\nand coherent language-based output, such as answering questions, completing sentences,\nor engaging in a conversation. LangChain provides several classes and functions to help construct and work with prompts. Prompt templates : Parametrized model inputs Example selectors : Dynamically select examples to include in prompts Previous Model I/O Next Prompt templates"
    },
    {
        "title": "Output parsers | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/model_io/output_parsers",
        "text_content": "Modules Model I/O Output parsers On this page Output parsers Language models output text. But many times you may want to get more\nstructured information than just text back. This is where output parsers\ncome in. Output parsers are classes that help structure language model responses.\nThere are two main methods an output parser must implement: “Get format instructions”: A method which returns a string\ncontaining instructions for how the output of a language model\nshould be formatted. “Parse”: A method which takes in a string (assumed to be the\nresponse from a language model) and parses it into some structure. And then one optional one: “Parse with prompt”: A method which takes in a string (assumed to be\nthe response from a language model) and a prompt (assumed to be the\nprompt that generated such a response) and parses it into some\nstructure. The prompt is largely provided in the event the\nOutputParser wants to retry or fix the output in some way, and needs\ninformation from the prompt to do so. Get started ​ Below we go over the main type of output parser, the PydanticOutputParser . from langchain . llms import OpenAI from langchain . output_parsers import PydanticOutputParser from langchain . prompts import PromptTemplate from langchain_core . pydantic_v1 import BaseModel , Field , validator model = OpenAI ( model_name = \"gpt-3.5-turbo-instruct\" , temperature = 0.0 ) # Define your desired data structure. class Joke ( BaseModel ) : setup : str = Field ( description = \"question to set up a joke\" ) punchline : str = Field ( description = \"answer to resolve the joke\" ) # You can add custom validation logic easily with Pydantic. @validator ( \"setup\" ) def question_ends_with_question_mark ( cls , field ) : if field [ - 1 ] != \"?\" : raise ValueError ( \"Badly formed question!\" ) return field # Set up a parser + inject instructions into the prompt template. parser = PydanticOutputParser ( pydantic_object = Joke ) prompt = PromptTemplate ( template = \"Answer the user query.\\n{format_instructions}\\n{query}\\n\" , input_variables = [ \"query\" ] , partial_variables = { \"format_instructions\" : parser . get_format_instructions ( ) } , ) # And a query intended to prompt a language model to populate the data structure. prompt_and_model = prompt | model output = prompt_and_model . invoke ( { \"query\" : \"Tell me a joke.\" } ) parser . invoke ( output ) Joke(setup='Why did the chicken cross the road?', punchline='To get to the other side!') LCEL ​ Output parsers implement the Runnable\ninterface , the basic building\nblock of the LangChain Expression Language\n(LCEL) . This means they support invoke , ainvoke , stream , astream , batch , abatch , astream_log calls. Output parsers accept a string or BaseMessage as input and can return\nan arbitrary type. parser . invoke ( output ) Joke(setup='Why did the chicken cross the road?', punchline='To get to the other side!') Instead of manually invoking the parser, we also could’ve just added it\nto our Runnable sequence: chain = prompt | model | parser chain . invoke ( { \"query\" : \"Tell me a joke.\" } ) Joke(setup='Why did the chicken cross the road?', punchline='To get to the other side!') While all parsers support the streaming interface, only certain parsers\ncan stream through partially parsed objects, since this is highly\ndependent on the output type. Parsers which cannot construct partial\nobjects will simply yield the fully parsed output. The SimpleJsonOutputParser for example can stream through partial\noutputs: from langchain . output_parsers . json import SimpleJsonOutputParser json_prompt = PromptTemplate . from_template ( \"Return a JSON object with an `answer` key that answers the following question: {question}\" ) json_parser = SimpleJsonOutputParser ( ) json_chain = json_prompt | model | json_parser list ( json_chain . stream ( { \"question\" : \"Who invented the microscope?\" } ) ) [{}, {'answer': ''}, {'answer': 'Ant'}, {'answer': 'Anton'}, {'answer': 'Antonie'}, {'answer': 'Antonie van'}, {'answer': 'Antonie van Lee'}, {'answer': 'Antonie van Leeu'}, {'answer': 'Antonie van Leeuwen'}, {'answer': 'Antonie van Leeuwenho'}, {'answer': 'Antonie van Leeuwenhoek'}] While the PydanticOutputParser cannot: list ( chain . stream ( { \"query\" : \"Tell me a joke.\" } ) ) [Joke(setup='Why did the chicken cross the road?', punchline='To get to the other side!')] Previous Tracking token usage Next List parser"
    },
    {
        "title": "LangChain Expression Language (LCEL) | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language",
        "text_content": "LangChain Expression Language LangChain Expression Language (LCEL) LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.\nLCEL was designed from day 1 to support putting prototypes in production, with no code changes , from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL: Streaming support When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens. Async support Any chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server. Optimized parallel execution Whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency. Retries and fallbacks Configure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost. Access intermediate results For more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every LangServe server. Input and output schemas Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe. Seamless LangSmith tracing integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability. Seamless LangServe deployment integration Any chain created with LCEL can be easily deployed using LangServe . Previous Security Next Get started"
    },
    {
        "title": "Model I/O | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/model_io",
        "text_content": "Modules Model I/O On this page Model I/O The core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model. Prompts : Templatize, dynamically select, and manage model inputs Chat models : Models that are backed by a language model but take a list of Chat Messages as input and return a Chat Message LLMs : Models that take a text string as input and return a text string Output parsers : Extract information from model outputs LLMs vs Chat models ​ LLMs and chat models are subtly but importantly different. LLMs in LangChain refer to pure text completion models.\nThe APIs they wrap take a string prompt as input and output a string completion. OpenAI's GPT-3 is implemented as an LLM.\nChat models are often backed by LLMs but tuned specifically for having conversations.\nAnd, crucially, their provider APIs use a different interface than pure text completion models. Instead of a single string,\nthey take a list of chat messages as input. Usually these messages are labeled with the speaker (usually one of \"System\",\n\"AI\", and \"Human\"). And they return an AI chat message as output. GPT-4 and Anthropic's Claude-2 are both implemented as chat models. Previous Modules Next Model I/O"
    },
    {
        "title": "Modules | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules",
        "text_content": "Modules On this page Modules LangChain provides standard, extendable interfaces and external integrations for the following main modules: Model I/O ​ Interface with language models Retrieval ​ Interface with application-specific data Agents ​ Let chains choose which tools to use given high-level directives Additional ​ Chains ​ Common, building block compositions Memory ​ Persist application state between runs of a chain Callbacks ​ Log and stream intermediate steps of any chain Previous LangChain Expression Language (LCEL) Next Model I/O"
    },
    {
        "title": "RAG | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/cookbook/retrieval",
        "text_content": "LangChain Expression Language Cookbook RAG On this page RAG Let’s look at adding in a retrieval step to a prompt and LLM, which adds\nup to a “retrieval-augmented generation” chain !pip install langchain openai faiss - cpu tiktoken from operator import itemgetter from langchain . chat_models import ChatOpenAI from langchain . embeddings import OpenAIEmbeddings from langchain . prompts import ChatPromptTemplate from langchain . vectorstores import FAISS from langchain_core . output_parsers import StrOutputParser from langchain_core . runnables import RunnableLambda , RunnablePassthrough vectorstore = FAISS . from_texts ( [ \"harrison worked at kensho\" ] , embedding = OpenAIEmbeddings ( ) ) retriever = vectorstore . as_retriever ( ) template = \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\" prompt = ChatPromptTemplate . from_template ( template ) model = ChatOpenAI ( ) chain = ( { \"context\" : retriever , \"question\" : RunnablePassthrough ( ) } | prompt | model | StrOutputParser ( ) ) chain . invoke ( \"where did harrison work?\" ) 'Harrison worked at Kensho.' template = \"\"\"Answer the question based only on the following context: {context} Question: {question} Answer in the following language: {language} \"\"\" prompt = ChatPromptTemplate . from_template ( template ) chain = ( { \"context\" : itemgetter ( \"question\" ) | retriever , \"question\" : itemgetter ( \"question\" ) , \"language\" : itemgetter ( \"language\" ) , } | prompt | model | StrOutputParser ( ) ) chain . invoke ( { \"question\" : \"where did harrison work\" , \"language\" : \"italian\" } ) 'Harrison ha lavorato a Kensho.' Conversational Retrieval Chain ​ We can easily add in conversation history. This primarily means adding\nin chat_message_history from langchain . schema import format_document from langchain_core . messages import AIMessage , HumanMessage , get_buffer_string from langchain_core . runnables import RunnableParallel from langchain . prompts . prompt import PromptTemplate _template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language. Chat History: {chat_history} Follow Up Input: {question} Standalone question:\"\"\" CONDENSE_QUESTION_PROMPT = PromptTemplate . from_template ( _template ) template = \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\" ANSWER_PROMPT = ChatPromptTemplate . from_template ( template ) DEFAULT_DOCUMENT_PROMPT = PromptTemplate . from_template ( template = \"{page_content}\" ) def _combine_documents ( docs , document_prompt = DEFAULT_DOCUMENT_PROMPT , document_separator = \"\\n\\n\" ) : doc_strings = [ format_document ( doc , document_prompt ) for doc in docs ] return document_separator . join ( doc_strings ) _inputs = RunnableParallel ( standalone_question = RunnablePassthrough . assign ( chat_history = lambda x : get_buffer_string ( x [ \"chat_history\" ] ) ) | CONDENSE_QUESTION_PROMPT | ChatOpenAI ( temperature = 0 ) | StrOutputParser ( ) , ) _context = { \"context\" : itemgetter ( \"standalone_question\" ) | retriever | _combine_documents , \"question\" : lambda x : x [ \"standalone_question\" ] , } conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI ( ) conversational_qa_chain . invoke ( { \"question\" : \"where did harrison work?\" , \"chat_history\" : [ ] , } ) AIMessage(content='Harrison was employed at Kensho.') conversational_qa_chain . invoke ( { \"question\" : \"where did he work?\" , \"chat_history\" : [ HumanMessage ( content = \"Who wrote this notebook?\" ) , AIMessage ( content = \"Harrison\" ) , ] , } ) AIMessage(content='Harrison worked at Kensho.') With Memory and returning source documents ​ This shows how to use memory with the above. For memory, we need to\nmanage that outside at the memory. For returning the retrieved\ndocuments, we just need to pass them through all the way. from operator import itemgetter from langchain . memory import ConversationBufferMemory memory = ConversationBufferMemory ( return_messages = True , output_key = \"answer\" , input_key = \"question\" ) # First we add a step to load memory # This adds a \"memory\" key to the input object loaded_memory = RunnablePassthrough . assign ( chat_history = RunnableLambda ( memory . load_memory_variables ) | itemgetter ( \"history\" ) , ) # Now we calculate the standalone question standalone_question = { \"standalone_question\" : { \"question\" : lambda x : x [ \"question\" ] , \"chat_history\" : lambda x : get_buffer_string ( x [ \"chat_history\" ] ) , } | CONDENSE_QUESTION_PROMPT | ChatOpenAI ( temperature = 0 ) | StrOutputParser ( ) , } # Now we retrieve the documents retrieved_documents = { \"docs\" : itemgetter ( \"standalone_question\" ) | retriever , \"question\" : lambda x : x [ \"standalone_question\" ] , } # Now we construct the inputs for the final prompt final_inputs = { \"context\" : lambda x : _combine_documents ( x [ \"docs\" ] ) , \"question\" : itemgetter ( \"question\" ) , } # And finally, we do the part that returns the answers answer = { \"answer\" : final_inputs | ANSWER_PROMPT | ChatOpenAI ( ) , \"docs\" : itemgetter ( \"docs\" ) , } # And now we put it all together! final_chain = loaded_memory | standalone_question | retrieved_documents | answer inputs = { \"question\" : \"where did harrison work?\" } result = final_chain . invoke ( inputs ) result {'answer': AIMessage(content='Harrison was employed at Kensho.'), 'docs': [Document(page_content='harrison worked at kensho')]} # Note that the memory does not save automatically # This will be improved in the future # For now you need to save it yourself memory . save_context ( inputs , { \"answer\" : result [ \"answer\" ] . content } ) memory . load_memory_variables ( { } ) {'history': [HumanMessage(content='where did harrison work?'), AIMessage(content='Harrison was employed at Kensho.')]} inputs = { \"question\" : \"but where did he really work?\" } result = final_chain . invoke ( inputs ) result {'answer': AIMessage(content='Harrison actually worked at Kensho.'), 'docs': [Document(page_content='harrison worked at kensho')]} Previous Prompt + LLM Next Multiple chains"
    },
    {
        "title": "RunnableParallel: Manipulating data | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/how_to/map",
        "text_content": "LangChain Expression Language How to RunnableParallel: Manipulating data On this page Manipulating inputs & output RunnableParallel can be useful for manipulating the output of one\nRunnable to match the input format of the next Runnable in a sequence. Here the input to prompt is expected to be a map with keys “context” and\n“question”. The user input is just the question. So we need to get the\ncontext using our retriever and passthrough the user input under the\n“question” key. from langchain . chat_models import ChatOpenAI from langchain . embeddings import OpenAIEmbeddings from langchain . prompts import ChatPromptTemplate from langchain . vectorstores import FAISS from langchain_core . output_parsers import StrOutputParser from langchain_core . runnables import RunnablePassthrough vectorstore = FAISS . from_texts ( [ \"harrison worked at kensho\" ] , embedding = OpenAIEmbeddings ( ) ) retriever = vectorstore . as_retriever ( ) template = \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\" prompt = ChatPromptTemplate . from_template ( template ) model = ChatOpenAI ( ) retrieval_chain = ( { \"context\" : retriever , \"question\" : RunnablePassthrough ( ) } | prompt | model | StrOutputParser ( ) ) retrieval_chain . invoke ( \"where did harrison work?\" ) 'Harrison worked at Kensho.' tip Note that when composing a RunnableParallel with another Runnable we\ndon’t even need to wrap our dictionary in the RunnableParallel class\n— the type conversion is handled for us. In the context of a chain,\nthese are equivalent: {\"context\": retriever, \"question\": RunnablePassthrough()} RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()}) RunnableParallel(context=retriever, question=RunnablePassthrough()) Using itemgetter as shorthand ​ Note that you can use Python’s itemgetter as shorthand to extract data\nfrom the map when combining with RunnableParallel . You can find more\ninformation about itemgetter in the Python\nDocumentation . In the example below, we use itemgetter to extract specific keys from\nthe map: from operator import itemgetter from langchain . chat_models import ChatOpenAI from langchain . embeddings import OpenAIEmbeddings from langchain . prompts import ChatPromptTemplate from langchain . vectorstores import FAISS from langchain_core . output_parsers import StrOutputParser from langchain_core . runnables import RunnablePassthrough vectorstore = FAISS . from_texts ( [ \"harrison worked at kensho\" ] , embedding = OpenAIEmbeddings ( ) ) retriever = vectorstore . as_retriever ( ) template = \"\"\"Answer the question based only on the following context: {context} Question: {question} Answer in the following language: {language} \"\"\" prompt = ChatPromptTemplate . from_template ( template ) chain = ( { \"context\" : itemgetter ( \"question\" ) | retriever , \"question\" : itemgetter ( \"question\" ) , \"language\" : itemgetter ( \"language\" ) , } | prompt | model | StrOutputParser ( ) ) chain . invoke ( { \"question\" : \"where did harrison work\" , \"language\" : \"italian\" } ) 'Harrison ha lavorato a Kensho.' Parallelize steps ​ RunnableParallel (aka. RunnableMap) makes it easy to execute multiple\nRunnables in parallel, and to return the output of these Runnables as a\nmap. from langchain . chat_models import ChatOpenAI from langchain . prompts import ChatPromptTemplate from langchain_core . runnables import RunnableParallel model = ChatOpenAI ( ) joke_chain = ChatPromptTemplate . from_template ( \"tell me a joke about {topic}\" ) | model poem_chain = ( ChatPromptTemplate . from_template ( \"write a 2-line poem about {topic}\" ) | model ) map_chain = RunnableParallel ( joke = joke_chain , poem = poem_chain ) map_chain . invoke ( { \"topic\" : \"bear\" } ) {'joke': AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\"), 'poem': AIMessage(content=\"In the wild's embrace, bear roams free,\\nStrength and grace, a majestic decree.\")} Parallelism ​ RunnableParallel are also useful for running independent processes in\nparallel, since each Runnable in the map is executed in parallel. For\nexample, we can see our earlier joke_chain , poem_chain and map_chain all have about the same runtime, even though map_chain executes both of the other two. % % timeit joke_chain . invoke ( { \"topic\" : \"bear\" } ) 958 ms ± 402 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) % % timeit poem_chain . invoke ( { \"topic\" : \"bear\" } ) 1.22 s ± 508 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) % % timeit map_chain . invoke ( { \"topic\" : \"bear\" } ) 1.15 s ± 119 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) Previous How to Next RunnablePassthrough: Passing data through"
    },
    {
        "title": "RunnablePassthrough: Passing data through | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/how_to/passthrough",
        "text_content": "LangChain Expression Language How to RunnablePassthrough: Passing data through On this page Passing data through RunnablePassthrough allows to pass inputs unchanged or with the addition\nof extra keys. This typically is used in conjuction with\nRunnableParallel to assign data to a new key in the map. RunnablePassthrough() called on it’s own, will simply take the input and\npass it through. RunnablePassthrough called with assign\n( RunnablePassthrough.assign(...) ) will take the input, and will add\nthe extra arguments passed to the assign function. See the example below: from langchain_core . runnables import RunnableParallel , RunnablePassthrough runnable = RunnableParallel ( passed = RunnablePassthrough ( ) , extra = RunnablePassthrough . assign ( mult = lambda x : x [ \"num\" ] * 3 ) , modified = lambda x : x [ \"num\" ] + 1 , ) runnable . invoke ( { \"num\" : 1 } ) {'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2} As seen above, passed key was called with RunnablePassthrough() and\nso it simply passed on {'num': 1} . In the second line, we used RunnablePastshrough.assign with a lambda\nthat multiplies the numerical value by 3. In this cased, extra was set\nwith {'num': 1, 'mult': 3} which is the original value with the mult key added. Finally, we also set a third key in the map with modified which uses a\nlabmda to set a single value adding 1 to the num, which resulted in modified key with the value of 2 . Retrieval Example ​ In the example below, we see a use case where we use RunnablePassthrough\nalong with RunnableMap. from langchain . chat_models import ChatOpenAI from langchain . embeddings import OpenAIEmbeddings from langchain . prompts import ChatPromptTemplate from langchain . vectorstores import FAISS from langchain_core . output_parsers import StrOutputParser from langchain_core . runnables import RunnablePassthrough vectorstore = FAISS . from_texts ( [ \"harrison worked at kensho\" ] , embedding = OpenAIEmbeddings ( ) ) retriever = vectorstore . as_retriever ( ) template = \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\" prompt = ChatPromptTemplate . from_template ( template ) model = ChatOpenAI ( ) retrieval_chain = ( { \"context\" : retriever , \"question\" : RunnablePassthrough ( ) } | prompt | model | StrOutputParser ( ) ) retrieval_chain . invoke ( \"where did harrison work?\" ) 'Harrison worked at Kensho.' Here the input to prompt is expected to be a map with keys “context” and\n“question”. The user input is just the question. So we need to get the\ncontext using our retriever and passthrough the user input under the\n“question” key. In this case, the RunnablePassthrough allows us to pass\non the user’s question to the prompt and model. Previous RunnableParallel: Manipulating data Next RunnableLambda: Run Custom Functions"
    },
    {
        "title": "RunnableLambda: Run Custom Functions | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/how_to/functions",
        "text_content": "LangChain Expression Language How to RunnableLambda: Run Custom Functions On this page Run custom functions You can use arbitrary functions in the pipeline. Note that all inputs to these functions need to be a SINGLE argument. If\nyou have a function that accepts multiple arguments, you should write a\nwrapper that accepts a single input and unpacks it into multiple\nargument. from operator import itemgetter from langchain . chat_models import ChatOpenAI from langchain . prompts import ChatPromptTemplate from langchain_core . runnables import RunnableLambda def length_function ( text ) : return len ( text ) def _multiple_length_function ( text1 , text2 ) : return len ( text1 ) * len ( text2 ) def multiple_length_function ( _dict ) : return _multiple_length_function ( _dict [ \"text1\" ] , _dict [ \"text2\" ] ) prompt = ChatPromptTemplate . from_template ( \"what is {a} + {b}\" ) model = ChatOpenAI ( ) chain1 = prompt | model chain = ( { \"a\" : itemgetter ( \"foo\" ) | RunnableLambda ( length_function ) , \"b\" : { \"text1\" : itemgetter ( \"foo\" ) , \"text2\" : itemgetter ( \"bar\" ) } | RunnableLambda ( multiple_length_function ) , } | prompt | model ) chain . invoke ( { \"foo\" : \"bar\" , \"bar\" : \"gah\" } ) AIMessage(content='3 + 9 equals 12.') Accepting a Runnable Config ​ Runnable lambdas can optionally accept a RunnableConfig ,\nwhich they can use to pass callbacks, tags, and other configuration\ninformation to nested runs. from langchain_core . output_parsers import StrOutputParser from langchain_core . runnables import RunnableConfig import json def parse_or_fix ( text : str , config : RunnableConfig ) : fixing_chain = ( ChatPromptTemplate . from_template ( \"Fix the following text:\\n\\n```text\\n{input}\\n```\\nError: {error}\" \" Don't narrate, just respond with the fixed data.\" ) | ChatOpenAI ( ) | StrOutputParser ( ) ) for _ in range ( 3 ) : try : return json . loads ( text ) except Exception as e : text = fixing_chain . invoke ( { \"input\" : text , \"error\" : e } , config ) return \"Failed to parse\" from langchain . callbacks import get_openai_callback with get_openai_callback ( ) as cb : output = RunnableLambda ( parse_or_fix ) . invoke ( \"{foo: bar}\" , { \"tags\" : [ \"my-tag\" ] , \"callbacks\" : [ cb ] } ) print ( output ) print ( cb ) {'foo': 'bar'} Tokens Used: 65 Prompt Tokens: 56 Completion Tokens: 9 Successful Requests: 1 Total Cost (USD): $0.00010200000000000001 Previous RunnablePassthrough: Passing data through Next RunnableBranch: Dynamically route logic based on input"
    },
    {
        "title": "RunnableBranch: Dynamically route logic based on input | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/how_to/routing",
        "text_content": "LangChain Expression Language How to RunnableBranch: Dynamically route logic based on input On this page Dynamically route logic based on input This notebook covers how to do routing in the LangChain Expression\nLanguage. Routing allows you to create non-deterministic chains where the output\nof a previous step defines the next step. Routing helps provide\nstructure and consistency around interactions with LLMs. There are two ways to perform routing: Using a RunnableBranch . Writing custom factory function that takes the input of a previous\nstep and returns a runnable . Importantly, this should return a runnable and NOT actually execute. We’ll illustrate both methods using a two step sequence where the first\nstep classifies an input question as being about LangChain , Anthropic , or Other , then routes to a corresponding prompt chain. Using a RunnableBranch ​ A RunnableBranch is initialized with a list of (condition, runnable)\npairs and a default runnable. It selects which branch by passing each\ncondition the input it’s invoked with. It selects the first condition to\nevaluate to True, and runs the corresponding runnable to that condition\nwith the input. If no provided conditions match, it runs the default runnable. Here’s an example of what it looks like in action: from langchain . chat_models import ChatAnthropic from langchain . prompts import PromptTemplate from langchain_core . output_parsers import StrOutputParser First, let’s create a chain that will identify incoming questions as\nbeing about LangChain , Anthropic , or Other : chain = ( PromptTemplate . from_template ( \"\"\"Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`. Do not respond with more than one word. <question> {question} </question> Classification:\"\"\" ) | ChatAnthropic ( ) | StrOutputParser ( ) ) chain . invoke ( { \"question\" : \"how do I call Anthropic?\" } ) ' Anthropic' Now, let’s create three sub chains: langchain_chain = ( PromptTemplate . from_template ( \"\"\"You are an expert in langchain. \\ Always answer questions starting with \"As Harrison Chase told me\". \\ Respond to the following question: Question: {question} Answer:\"\"\" ) | ChatAnthropic ( ) ) anthropic_chain = ( PromptTemplate . from_template ( \"\"\"You are an expert in anthropic. \\ Always answer questions starting with \"As Dario Amodei told me\". \\ Respond to the following question: Question: {question} Answer:\"\"\" ) | ChatAnthropic ( ) ) general_chain = ( PromptTemplate . from_template ( \"\"\"Respond to the following question: Question: {question} Answer:\"\"\" ) | ChatAnthropic ( ) ) from langchain_core . runnables import RunnableBranch branch = RunnableBranch ( ( lambda x : \"anthropic\" in x [ \"topic\" ] . lower ( ) , anthropic_chain ) , ( lambda x : \"langchain\" in x [ \"topic\" ] . lower ( ) , langchain_chain ) , general_chain , ) full_chain = { \"topic\" : chain , \"question\" : lambda x : x [ \"question\" ] } | branch full_chain . invoke ( { \"question\" : \"how do I use Anthropic?\" } ) AIMessage(content=\" As Dario Amodei told me, here are some ways to use Anthropic:\\n\\n- Sign up for an account on Anthropic's website to access tools like Claude, Constitutional AI, and Writer. \\n\\n- Use Claude for tasks like email generation, customer service chat, and QA. Claude can understand natural language prompts and provide helpful responses.\\n\\n- Use Constitutional AI if you need an AI assistant that is harmless, honest, and helpful. It is designed to be safe and aligned with human values.\\n\\n- Use Writer to generate natural language content for things like marketing copy, stories, reports, and more. Give it a topic and prompt and it will create high-quality written content.\\n\\n- Check out Anthropic's documentation and blog for tips, tutorials, examples, and announcements about new capabilities as they continue to develop their AI technology.\\n\\n- Follow Anthropic on social media or subscribe to their newsletter to stay up to date on new features and releases.\\n\\n- For most people, the easiest way to leverage Anthropic's technology is through their website - just create an account to get started!\", additional_kwargs={}, example=False) full_chain . invoke ( { \"question\" : \"how do I use LangChain?\" } ) AIMessage(content=' As Harrison Chase told me, here is how you use LangChain:\\n\\nLangChain is an AI assistant that can have conversations, answer questions, and generate text. To use LangChain, you simply type or speak your input and LangChain will respond. \\n\\nYou can ask LangChain questions, have discussions, get summaries or explanations about topics, and request it to generate text on a subject. Some examples of interactions:\\n\\n- Ask general knowledge questions and LangChain will try to answer factually. For example \"What is the capital of France?\"\\n\\n- Have conversations on topics by taking turns speaking. You can prompt the start of a conversation by saying something like \"Let\\'s discuss machine learning\"\\n\\n- Ask for summaries or high-level explanations on subjects. For example \"Can you summarize the main themes in Shakespeare\\'s Hamlet?\" \\n\\n- Give creative writing prompts or requests to have LangChain generate text in different styles. For example \"Write a short children\\'s story about a mouse\" or \"Generate a poem in the style of Robert Frost about nature\"\\n\\n- Correct LangChain if it makes an inaccurate statement and provide the right information. This helps train it.\\n\\nThe key is interacting naturally and giving it clear prompts and requests', additional_kwargs={}, example=False) full_chain . invoke ( { \"question\" : \"whats 2 + 2\" } ) AIMessage(content=' 2 + 2 = 4', additional_kwargs={}, example=False) Using a custom function ​ You can also use a custom function to route between different outputs.\nHere’s an example: def route ( info ) : if \"anthropic\" in info [ \"topic\" ] . lower ( ) : return anthropic_chain elif \"langchain\" in info [ \"topic\" ] . lower ( ) : return langchain_chain else : return general_chain from langchain_core . runnables import RunnableLambda full_chain = { \"topic\" : chain , \"question\" : lambda x : x [ \"question\" ] } | RunnableLambda ( route ) full_chain . invoke ( { \"question\" : \"how do I use Anthropic?\" } ) AIMessage(content=' As Dario Amodei told me, to use Anthropic IPC you first need to import it:\\n\\n```python\\nfrom anthroipc import ic\\n```\\n\\nThen you can create a client and connect to the server:\\n\\n```python \\nclient = ic.connect()\\n```\\n\\nAfter that, you can call methods on the client and get responses:\\n\\n```python\\nresponse = client.ask(\"What is the meaning of life?\")\\nprint(response)\\n```\\n\\nYou can also register callbacks to handle events: \\n\\n```python\\ndef on_poke(event):\\n  print(\"Got poked!\")\\n\\nclient.on(\\'poke\\', on_poke)\\n```\\n\\nAnd that\\'s the basics of using the Anthropic IPC client library for Python! Let me know if you have any other questions!', additional_kwargs={}, example=False) full_chain . invoke ( { \"question\" : \"how do I use LangChain?\" } ) AIMessage(content=' As Harrison Chase told me, to use LangChain you first need to sign up for an API key at platform.langchain.com. Once you have your API key, you can install the Python library and write a simple Python script to call the LangChain API. Here is some sample code to get started:\\n\\n```python\\nimport langchain\\n\\napi_key = \"YOUR_API_KEY\"\\n\\nlangchain.set_key(api_key)\\n\\nresponse = langchain.ask(\"What is the capital of France?\")\\n\\nprint(response.response)\\n```\\n\\nThis will send the question \"What is the capital of France?\" to the LangChain API and print the response. You can customize the request by providing parameters like max_tokens, temperature, etc. The LangChain Python library documentation has more details on the available options. The key things are getting an API key and calling langchain.ask() with your question text. Let me know if you have any other questions!', additional_kwargs={}, example=False) full_chain . invoke ( { \"question\" : \"whats 2 + 2\" } ) AIMessage(content=' 4', additional_kwargs={}, example=False) Previous RunnableLambda: Run Custom Functions Next Bind runtime args"
    },
    {
        "title": "Bind runtime args | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/how_to/binding",
        "text_content": "LangChain Expression Language How to Bind runtime args On this page Bind runtime args Sometimes we want to invoke a Runnable within a Runnable sequence with\nconstant arguments that are not part of the output of the preceding\nRunnable in the sequence, and which are not part of the user input. We\ncan use Runnable.bind() to easily pass these arguments in. Suppose we have a simple prompt + model sequence: from langchain . chat_models import ChatOpenAI from langchain . prompts import ChatPromptTemplate from langchain . schema import StrOutputParser from langchain_core . runnables import RunnablePassthrough prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , \"Write out the following equation using algebraic symbols then solve it. Use the format\\n\\nEQUATION:...\\nSOLUTION:...\\n\\n\" , ) , ( \"human\" , \"{equation_statement}\" ) , ] ) model = ChatOpenAI ( temperature = 0 ) runnable = ( { \"equation_statement\" : RunnablePassthrough ( ) } | prompt | model | StrOutputParser ( ) ) print ( runnable . invoke ( \"x raised to the third plus seven equals 12\" ) ) EQUATION: x^3 + 7 = 12 SOLUTION: Subtracting 7 from both sides of the equation, we get: x^3 = 12 - 7 x^3 = 5 Taking the cube root of both sides, we get: x = ∛5 Therefore, the solution to the equation x^3 + 7 = 12 is x = ∛5. and want to call the model with certain stop words: runnable = ( { \"equation_statement\" : RunnablePassthrough ( ) } | prompt | model . bind ( stop = \"SOLUTION\" ) | StrOutputParser ( ) ) print ( runnable . invoke ( \"x raised to the third plus seven equals 12\" ) ) EQUATION: x^3 + 7 = 12 Attaching OpenAI functions ​ One particularly useful application of binding is to attach OpenAI\nfunctions to a compatible OpenAI model: function = { \"name\" : \"solver\" , \"description\" : \"Formulates and solves an equation\" , \"parameters\" : { \"type\" : \"object\" , \"properties\" : { \"equation\" : { \"type\" : \"string\" , \"description\" : \"The algebraic expression of the equation\" , } , \"solution\" : { \"type\" : \"string\" , \"description\" : \"The solution to the equation\" , } , } , \"required\" : [ \"equation\" , \"solution\" ] , } , } # Need gpt-4 to solve this one correctly prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , \"Write out the following equation using algebraic symbols then solve it.\" , ) , ( \"human\" , \"{equation_statement}\" ) , ] ) model = ChatOpenAI ( model = \"gpt-4\" , temperature = 0 ) . bind ( function_call = { \"name\" : \"solver\" } , functions = [ function ] ) runnable = { \"equation_statement\" : RunnablePassthrough ( ) } | prompt | model runnable . invoke ( \"x raised to the third plus seven equals 12\" ) AIMessage(content='', additional_kwargs={'function_call': {'name': 'solver', 'arguments': '{\\n\"equation\": \"x^3 + 7 = 12\",\\n\"solution\": \"x = ∛5\"\\n}'}}, example=False) Attaching OpenAI tools ​ tools = [ { \"type\" : \"function\" , \"function\" : { \"name\" : \"get_current_weather\" , \"description\" : \"Get the current weather in a given location\" , \"parameters\" : { \"type\" : \"object\" , \"properties\" : { \"location\" : { \"type\" : \"string\" , \"description\" : \"The city and state, e.g. San Francisco, CA\" , } , \"unit\" : { \"type\" : \"string\" , \"enum\" : [ \"celsius\" , \"fahrenheit\" ] } , } , \"required\" : [ \"location\" ] , } , } , } ] model = ChatOpenAI ( model = \"gpt-3.5-turbo-1106\" ) . bind ( tools = tools ) model . invoke ( \"What's the weather in SF, NYC and LA?\" ) AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_zHN0ZHwrxM7nZDdqTp6dkPko', 'function': {'arguments': '{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_aqdMm9HBSlFW9c9rqxTa7eQv', 'function': {'arguments': '{\"location\": \"New York, NY\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_cx8E567zcLzYV2WSWVgO63f1', 'function': {'arguments': '{\"location\": \"Los Angeles, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}]}) Previous RunnableBranch: Dynamically route logic based on input Next Configure chain internals at runtime"
    },
    {
        "title": "Configure chain internals at runtime | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/how_to/configure",
        "text_content": "LangChain Expression Language How to Configure chain internals at runtime On this page Configure chain internals at runtime Oftentimes you may want to experiment with, or even expose to the end\nuser, multiple different ways of doing things. In order to make this\nexperience as easy as possible, we have defined two methods. First, a configurable_fields method. This lets you configure\nparticular fields of a runnable. Second, a configurable_alternatives method. With this method, you can\nlist out alternatives for any particular runnable that can be set during\nruntime. Configuration Fields ​ With LLMs ​ With LLMs we can configure things like temperature from langchain . chat_models import ChatOpenAI from langchain . prompts import PromptTemplate from langchain_core . runnables import ConfigurableField model = ChatOpenAI ( temperature = 0 ) . configurable_fields ( temperature = ConfigurableField ( id = \"llm_temperature\" , name = \"LLM Temperature\" , description = \"The temperature of the LLM\" , ) ) model . invoke ( \"pick a random number\" ) AIMessage(content='7') model . with_config ( configurable = { \"llm_temperature\" : 0.9 } ) . invoke ( \"pick a random number\" ) AIMessage(content='34') We can also do this when its used as part of a chain prompt = PromptTemplate . from_template ( \"Pick a random number above {x}\" ) chain = prompt | model chain . invoke ( { \"x\" : 0 } ) AIMessage(content='57') chain . with_config ( configurable = { \"llm_temperature\" : 0.9 } ) . invoke ( { \"x\" : 0 } ) AIMessage(content='6') With HubRunnables ​ This is useful to allow for switching of prompts from langchain . runnables . hub import HubRunnable prompt = HubRunnable ( \"rlm/rag-prompt\" ) . configurable_fields ( owner_repo_commit = ConfigurableField ( id = \"hub_commit\" , name = \"Hub Commit\" , description = \"The Hub commit to pull from\" , ) ) prompt . invoke ( { \"question\" : \"foo\" , \"context\" : \"bar\" } ) ChatPromptValue(messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: foo \\nContext: bar \\nAnswer:\")]) prompt . with_config ( configurable = { \"hub_commit\" : \"rlm/rag-prompt-llama\" } ) . invoke ( { \"question\" : \"foo\" , \"context\" : \"bar\" } ) ChatPromptValue(messages=[HumanMessage(content=\"[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>> \\nQuestion: foo \\nContext: bar \\nAnswer: [/INST]\")]) Configurable Alternatives ​ With LLMs ​ Let’s take a look at doing this with LLMs from langchain . chat_models import ChatAnthropic , ChatOpenAI from langchain . prompts import PromptTemplate from langchain_core . runnables import ConfigurableField llm = ChatAnthropic ( temperature = 0 ) . configurable_alternatives ( # This gives this field an id # When configuring the end runnable, we can then use this id to configure this field ConfigurableField ( id = \"llm\" ) , # This sets a default_key. # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used default_key = \"anthropic\" , # This adds a new option, with name `openai` that is equal to `ChatOpenAI()` openai = ChatOpenAI ( ) , # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=\"gpt-4\")` gpt4 = ChatOpenAI ( model = \"gpt-4\" ) , # You can add more configuration options here ) prompt = PromptTemplate . from_template ( \"Tell me a joke about {topic}\" ) chain = prompt | llm # By default it will call Anthropic chain . invoke ( { \"topic\" : \"bears\" } ) AIMessage(content=\" Here's a silly joke about bears:\\n\\nWhat do you call a bear with no teeth?\\nA gummy bear!\") # We can use `.with_config(configurable={\"llm\": \"openai\"})` to specify an llm to use chain . with_config ( configurable = { \"llm\" : \"openai\" } ) . invoke ( { \"topic\" : \"bears\" } ) AIMessage(content=\"Sure, here's a bear joke for you:\\n\\nWhy don't bears wear shoes?\\n\\nBecause they already have bear feet!\") # If we use the `default_key` then it uses the default chain . with_config ( configurable = { \"llm\" : \"anthropic\" } ) . invoke ( { \"topic\" : \"bears\" } ) AIMessage(content=\" Here's a silly joke about bears:\\n\\nWhat do you call a bear with no teeth?\\nA gummy bear!\") With Prompts ​ We can do a similar thing, but alternate between prompts llm = ChatAnthropic ( temperature = 0 ) prompt = PromptTemplate . from_template ( \"Tell me a joke about {topic}\" ) . configurable_alternatives ( # This gives this field an id # When configuring the end runnable, we can then use this id to configure this field ConfigurableField ( id = \"prompt\" ) , # This sets a default_key. # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used default_key = \"joke\" , # This adds a new option, with name `poem` poem = PromptTemplate . from_template ( \"Write a short poem about {topic}\" ) , # You can add more configuration options here ) chain = prompt | llm # By default it will write a joke chain . invoke ( { \"topic\" : \"bears\" } ) AIMessage(content=\" Here's a silly joke about bears:\\n\\nWhat do you call a bear with no teeth?\\nA gummy bear!\") # We can configure it write a poem chain . with_config ( configurable = { \"prompt\" : \"poem\" } ) . invoke ( { \"topic\" : \"bears\" } ) AIMessage(content=' Here is a short poem about bears:\\n\\nThe bears awaken from their sleep\\nAnd lumber out into the deep\\nForests filled with trees so tall\\nForaging for food before nightfall \\nTheir furry coats and claws so sharp\\nSniffing for berries and fish to nab\\nLumbering about without a care\\nThe mighty grizzly and black bear\\nProud creatures, wild and free\\nRuling their domain majestically\\nWandering the woods they call their own\\nBefore returning to their dens alone') With Prompts and LLMs ​ We can also have multiple things configurable! Here’s an example doing\nthat with both prompts and LLMs. llm = ChatAnthropic ( temperature = 0 ) . configurable_alternatives ( # This gives this field an id # When configuring the end runnable, we can then use this id to configure this field ConfigurableField ( id = \"llm\" ) , # This sets a default_key. # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used default_key = \"anthropic\" , # This adds a new option, with name `openai` that is equal to `ChatOpenAI()` openai = ChatOpenAI ( ) , # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=\"gpt-4\")` gpt4 = ChatOpenAI ( model = \"gpt-4\" ) , # You can add more configuration options here ) prompt = PromptTemplate . from_template ( \"Tell me a joke about {topic}\" ) . configurable_alternatives ( # This gives this field an id # When configuring the end runnable, we can then use this id to configure this field ConfigurableField ( id = \"prompt\" ) , # This sets a default_key. # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used default_key = \"joke\" , # This adds a new option, with name `poem` poem = PromptTemplate . from_template ( \"Write a short poem about {topic}\" ) , # You can add more configuration options here ) chain = prompt | llm # We can configure it write a poem with OpenAI chain . with_config ( configurable = { \"prompt\" : \"poem\" , \"llm\" : \"openai\" } ) . invoke ( { \"topic\" : \"bears\" } ) AIMessage(content=\"In the forest, where tall trees sway,\\nA creature roams, both fierce and gray.\\nWith mighty paws and piercing eyes,\\nThe bear, a symbol of strength, defies.\\n\\nThrough snow-kissed mountains, it does roam,\\nA guardian of its woodland home.\\nWith fur so thick, a shield of might,\\nIt braves the coldest winter night.\\n\\nA gentle giant, yet wild and free,\\nThe bear commands respect, you see.\\nWith every step, it leaves a trace,\\nOf untamed power and ancient grace.\\n\\nFrom honeyed feast to salmon's leap,\\nIt takes its place, in nature's keep.\\nA symbol of untamed delight,\\nThe bear, a wonder, day and night.\\n\\nSo let us honor this noble beast,\\nIn forests where its soul finds peace.\\nFor in its presence, we come to know,\\nThe untamed spirit that in us also flows.\") # We can always just configure only one if we want chain . with_config ( configurable = { \"llm\" : \"openai\" } ) . invoke ( { \"topic\" : \"bears\" } ) AIMessage(content=\"Sure, here's a bear joke for you:\\n\\nWhy don't bears wear shoes?\\n\\nBecause they have bear feet!\") Saving configurations ​ We can also easily save configured chains as their own objects openai_poem = chain . with_config ( configurable = { \"llm\" : \"openai\" } ) openai_poem . invoke ( { \"topic\" : \"bears\" } ) AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\") Previous Bind runtime args Next Add fallbacks"
    },
    {
        "title": "Add fallbacks | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/how_to/fallbacks",
        "text_content": "LangChain Expression Language How to Add fallbacks On this page Add fallbacks There are many possible points of failure in an LLM application, whether\nthat be issues with LLM API’s, poor model outputs, issues with other\nintegrations, etc. Fallbacks help you gracefully handle and isolate\nthese issues. Crucially, fallbacks can be applied not only on the LLM level but on the\nwhole runnable level. Handling LLM API Errors ​ This is maybe the most common use case for fallbacks. A request to an\nLLM API can fail for a variety of reasons - the API could be down, you\ncould have hit rate limits, any number of things. Therefore, using\nfallbacks can help protect against these types of things. IMPORTANT: By default, a lot of the LLM wrappers catch errors and retry.\nYou will most likely want to turn those off when working with fallbacks.\nOtherwise the first wrapper will keep on retrying and not failing. from langchain . chat_models import ChatAnthropic , ChatOpenAI First, let’s mock out what happens if we hit a RateLimitError from\nOpenAI from unittest . mock import patch import httpx from openai import RateLimitError request = httpx . Request ( \"GET\" , \"/\" ) response = httpx . Response ( 200 , request = request ) error = RateLimitError ( \"rate limit\" , response = response , body = \"\" ) # Note that we set max_retries = 0 to avoid retrying on RateLimits, etc openai_llm = ChatOpenAI ( max_retries = 0 ) anthropic_llm = ChatAnthropic ( ) llm = openai_llm . with_fallbacks ( [ anthropic_llm ] ) # Let's use just the OpenAI LLm first, to show that we run into an error with patch ( \"openai.resources.chat.completions.Completions.create\" , side_effect = error ) : try : print ( openai_llm . invoke ( \"Why did the chicken cross the road?\" ) ) except RateLimitError : print ( \"Hit error\" ) Hit error # Now let's try with fallbacks to Anthropic with patch ( \"openai.resources.chat.completions.Completions.create\" , side_effect = error ) : try : print ( llm . invoke ( \"Why did the chicken cross the road?\" ) ) except RateLimitError : print ( \"Hit error\" ) content=' I don\\'t actually know why the chicken crossed the road, but here are some possible humorous answers:\\n\\n- To get to the other side!\\n\\n- It was too chicken to just stand there. \\n\\n- It wanted a change of scenery.\\n\\n- It wanted to show the possum it could be done.\\n\\n- It was on its way to a poultry farmers\\' convention.\\n\\nThe joke plays on the double meaning of \"the other side\" - literally crossing the road to the other side, or the \"other side\" meaning the afterlife. So it\\'s an anti-joke, with a silly or unexpected pun as the answer.' additional_kwargs={} example=False We can use our “LLM with Fallbacks” as we would a normal LLM. from langchain . prompts import ChatPromptTemplate prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , \"You're a nice assistant who always includes a compliment in your response\" , ) , ( \"human\" , \"Why did the {animal} cross the road\" ) , ] ) chain = prompt | llm with patch ( \"openai.resources.chat.completions.Completions.create\" , side_effect = error ) : try : print ( chain . invoke ( { \"animal\" : \"kangaroo\" } ) ) except RateLimitError : print ( \"Hit error\" ) content=\" I don't actually know why the kangaroo crossed the road, but I'm happy to take a guess! Maybe the kangaroo was trying to get to the other side to find some tasty grass to eat. Or maybe it was trying to get away from a predator or other danger. Kangaroos do need to cross roads and other open areas sometimes as part of their normal activities. Whatever the reason, I'm sure the kangaroo looked both ways before hopping across!\" additional_kwargs={} example=False Specifying errors to handle ​ We can also specify the errors to handle if we want to be more specific\nabout when the fallback is invoked: llm = openai_llm . with_fallbacks ( [ anthropic_llm ] , exceptions_to_handle = ( KeyboardInterrupt , ) ) chain = prompt | llm with patch ( \"openai.resources.chat.completions.Completions.create\" , side_effect = error ) : try : print ( chain . invoke ( { \"animal\" : \"kangaroo\" } ) ) except RateLimitError : print ( \"Hit error\" ) Hit error Fallbacks for Sequences ​ We can also create fallbacks for sequences, that are sequences\nthemselves. Here we do that with two different models: ChatOpenAI and\nthen normal OpenAI (which does not use a chat model). Because OpenAI is\nNOT a chat model, you likely want a different prompt. # First let's create a chain with a ChatModel # We add in a string output parser here so the outputs between the two are the same type from langchain_core . output_parsers import StrOutputParser chat_prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , \"You're a nice assistant who always includes a compliment in your response\" , ) , ( \"human\" , \"Why did the {animal} cross the road\" ) , ] ) # Here we're going to use a bad model name to easily create a chain that will error chat_model = ChatOpenAI ( model_name = \"gpt-fake\" ) bad_chain = chat_prompt | chat_model | StrOutputParser ( ) # Now lets create a chain with the normal OpenAI model from langchain . llms import OpenAI from langchain . prompts import PromptTemplate prompt_template = \"\"\"Instructions: You should always include a compliment in your response. Question: Why did the {animal} cross the road?\"\"\" prompt = PromptTemplate . from_template ( prompt_template ) llm = OpenAI ( ) good_chain = prompt | llm # We can now create a final chain which combines the two chain = bad_chain . with_fallbacks ( [ good_chain ] ) chain . invoke ( { \"animal\" : \"turtle\" } ) '\\n\\nAnswer: The turtle crossed the road to get to the other side, and I have to say he had some impressive determination.' Previous Configure chain internals at runtime Next Stream custom generator functions"
    },
    {
        "title": "Stream custom generator functions | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/how_to/generators",
        "text_content": "LangChain Expression Language How to Stream custom generator functions On this page Stream custom generator functions You can use generator functions (ie. functions that use the yield keyword, and behave like iterators) in a LCEL pipeline. The signature of these generators should be Iterator[Input] -> Iterator[Output] . Or for async generators: AsyncIterator[Input] -> AsyncIterator[Output] . These are useful for: - implementing a custom output parser - modifying\nthe output of a previous step, while preserving streaming capabilities Let’s implement a custom output parser for comma-separated lists. Sync version ​ from typing import Iterator , List from langchain . chat_models import ChatOpenAI from langchain . prompts . chat import ChatPromptTemplate from langchain_core . output_parsers import StrOutputParser prompt = ChatPromptTemplate . from_template ( \"Write a comma-separated list of 5 animals similar to: {animal}\" ) model = ChatOpenAI ( temperature = 0.0 ) str_chain = prompt | model | StrOutputParser ( ) for chunk in str_chain . stream ( { \"animal\" : \"bear\" } ) : print ( chunk , end = \"\" , flush = True ) lion, tiger, wolf, gorilla, panda str_chain . invoke ( { \"animal\" : \"bear\" } ) 'lion, tiger, wolf, gorilla, panda' # This is a custom parser that splits an iterator of llm tokens # into a list of strings separated by commas def split_into_list ( input : Iterator [ str ] ) - > Iterator [ List [ str ] ] : # hold partial input until we get a comma buffer = \"\" for chunk in input : # add current chunk to buffer buffer += chunk # while there are commas in the buffer while \",\" in buffer : # split buffer on comma comma_index = buffer . index ( \",\" ) # yield everything before the comma yield [ buffer [ : comma_index ] . strip ( ) ] # save the rest for the next iteration buffer = buffer [ comma_index + 1 : ] # yield the last chunk yield [ buffer . strip ( ) ] list_chain = str_chain | split_into_list for chunk in list_chain . stream ( { \"animal\" : \"bear\" } ) : print ( chunk , flush = True ) ['lion'] ['tiger'] ['wolf'] ['gorilla'] ['panda'] list_chain . invoke ( { \"animal\" : \"bear\" } ) ['lion', 'tiger', 'wolf', 'gorilla', 'panda'] Async version ​ from typing import AsyncIterator async def asplit_into_list ( input : AsyncIterator [ str ] , ) - > AsyncIterator [ List [ str ] ] : # async def buffer = \"\" async for ( chunk ) in input : # `input` is a `async_generator` object, so use `async for` buffer += chunk while \",\" in buffer : comma_index = buffer . index ( \",\" ) yield [ buffer [ : comma_index ] . strip ( ) ] buffer = buffer [ comma_index + 1 : ] yield [ buffer . strip ( ) ] list_chain = str_chain | asplit_into_list async for chunk in list_chain . astream ( { \"animal\" : \"bear\" } ) : print ( chunk , flush = True ) ['lion'] ['tiger'] ['wolf'] ['gorilla'] ['panda'] await list_chain . ainvoke ( { \"animal\" : \"bear\" } ) ['lion', 'tiger', 'wolf', 'gorilla', 'panda'] Previous Add fallbacks Next Add message history (memory)"
    },
    {
        "title": "Add message history (memory) | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/how_to/message_history",
        "text_content": "LangChain Expression Language How to Add message history (memory) On this page Add message history (memory) The RunnableWithMessageHistory let’s us add message history to certain\ntypes of chains. Specifically, it can be used for any Runnable that takes as input one of a sequence of BaseMessage a dict with a key that takes a sequence of BaseMessage a dict with a key that takes the latest message(s) as a string or\nsequence of BaseMessage , and a separate key that takes historical\nmessages And returns as output one of a string that can be treated as the contents of an AIMessage a sequence of BaseMessage a dict with a key that contains a sequence of BaseMessage Let’s take a look at some examples to see how it works. Setup ​ We’ll use Redis to store our chat message histories and Anthropic’s\nclaude-2 model so we’ll need to install the following dependencies: !pip install - U langchain redis anthropic Set your Anthropic API key : import getpass import os os . environ [ \"ANTHROPIC_API_KEY\" ] = getpass . getpass ( ) Start a local Redis Stack server if we don’t have an existing Redis\ndeployment to connect to: docker run -d -p 6379 :6379 -p 8001 :8001 redis/redis-stack:latest REDIS_URL = \"redis://localhost:6379/0\" LangSmith ​ LangSmith is especially useful for something like message history\ninjection, where it can be hard to otherwise understand what the inputs\nare to various parts of the chain. Note that LangSmith is not needed, but it is helpful. If you do want to\nuse LangSmith, after you sign up at the link above, make sure to\nuncoment the below and set your environment variables to start logging\ntraces: # os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" # os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass() Example: Dict input, message output ​ Let’s create a simple chain that takes a dict as input and returns a\nBaseMessage. In this case the \"question\" key in the input represents our input\nmessage, and the \"history\" key is where our historical messages will\nbe injected. from typing import Optional from langchain . chat_models import ChatAnthropic from langchain . memory . chat_message_histories import RedisChatMessageHistory from langchain . prompts import ChatPromptTemplate , MessagesPlaceholder from langchain_core . chat_history import BaseChatMessageHistory from langchain_core . runnables . history import RunnableWithMessageHistory prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , \"You're an assistant who's good at {ability}\" ) , MessagesPlaceholder ( variable_name = \"history\" ) , ( \"human\" , \"{question}\" ) , ] ) chain = prompt | ChatAnthropic ( model = \"claude-2\" ) Adding message history ​ To add message history to our original chain we wrap it in the RunnableWithMessageHistory class. Crucially, we also need to define a method that takes a session_id\nstring and based on it returns a BaseChatMessageHistory . Given the\nsame input, this method should return an equivalent output. In this case we’ll also want to specify input_messages_key (the key to\nbe treated as the latest input message) and history_messages_key (the\nkey to add historical messages to). chain_with_history = RunnableWithMessageHistory ( chain , lambda session_id : RedisChatMessageHistory ( session_id , url = REDIS_URL ) , input_messages_key = \"question\" , history_messages_key = \"history\" , ) Invoking with config ​ Whenever we call our chain with message history, we need to include a\nconfig that contains the session_id config = { \"configurable\" : { \"session_id\" : \"<SESSION_ID>\" } } Given the same configuration, our chain should be pulling from the same\nchat message history. chain_with_history . invoke ( { \"ability\" : \"math\" , \"question\" : \"What does cosine mean?\" } , config = { \"configurable\" : { \"session_id\" : \"foobar\" } } , ) AIMessage(content=' Cosine is one of the basic trigonometric functions in mathematics. It is defined as the ratio of the adjacent side to the hypotenuse in a right triangle.\\n\\nSome key properties and facts about cosine:\\n\\n- It is denoted by cos(θ), where θ is the angle in a right triangle. \\n\\n- The cosine of an acute angle is always positive. For angles greater than 90 degrees, cosine can be negative.\\n\\n- Cosine is one of the three main trig functions along with sine and tangent.\\n\\n- The cosine of 0 degrees is 1. As the angle increases towards 90 degrees, the cosine value decreases towards 0.\\n\\n- The range of values for cosine is -1 to 1.\\n\\n- The cosine function maps angles in a circle to the x-coordinate on the unit circle.\\n\\n- Cosine is used to find adjacent side lengths in right triangles, and has many other applications in mathematics, physics, engineering and more.\\n\\n- Key cosine identities include: cos(A+B) = cosAcosB − sinAsinB and cos(2A) = cos^2(A) − sin^2(A)\\n\\nSo in summary, cosine is a fundamental trig') chain_with_history . invoke ( { \"ability\" : \"math\" , \"question\" : \"What's its inverse\" } , config = { \"configurable\" : { \"session_id\" : \"foobar\" } } , ) AIMessage(content=' The inverse of the cosine function is called the arccosine or inverse cosine, often denoted as cos-1(x) or arccos(x).\\n\\nThe key properties and facts about arccosine:\\n\\n- It is defined as the angle θ between 0 and π radians whose cosine is x. So arccos(x) = θ such that cos(θ) = x.\\n\\n- The range of arccosine is 0 to π radians (0 to 180 degrees).\\n\\n- The domain of arccosine is -1 to 1. \\n\\n- arccos(cos(θ)) = θ for values of θ from 0 to π radians.\\n\\n- arccos(x) is the angle in a right triangle whose adjacent side is x and hypotenuse is 1.\\n\\n- arccos(0) = 90 degrees. As x increases from 0 to 1, arccos(x) decreases from 90 to 0 degrees.\\n\\n- arccos(1) = 0 degrees. arccos(-1) = 180 degrees.\\n\\n- The graph of y = arccos(x) is part of the unit circle, restricted to x') Langsmith\ntrace Looking at the Langsmith trace for the second call, we can see that when\nconstructing the prompt, a “history” variable has been injected which is\na list of two messages (our first input and first output). Example: messages input, dict output ​ from langchain_core . messages import HumanMessage from langchain_core . runnables import RunnableParallel chain = RunnableParallel ( { \"output_message\" : ChatAnthropic ( model = \"claude-2\" ) } ) chain_with_history = RunnableWithMessageHistory ( chain , lambda session_id : RedisChatMessageHistory ( session_id , url = REDIS_URL ) , output_messages_key = \"output_message\" , ) chain_with_history . invoke ( [ HumanMessage ( content = \"What did Simone de Beauvoir believe about free will\" ) ] , config = { \"configurable\" : { \"session_id\" : \"baz\" } } , ) {'output_message': AIMessage(content=' Here is a summary of Simone de Beauvoir\\'s views on free will:\\n\\n- De Beauvoir was an existentialist philosopher and believed strongly in the concept of free will. She rejected the idea that human nature or instincts determine behavior.\\n\\n- Instead, de Beauvoir argued that human beings define their own essence or nature through their actions and choices. As she famously wrote, \"One is not born, but rather becomes, a woman.\"\\n\\n- De Beauvoir believed that while individuals are situated in certain cultural contexts and social conditions, they still have agency and the ability to transcend these situations. Freedom comes from choosing one\\'s attitude toward these constraints.\\n\\n- She emphasized the radical freedom and responsibility of the individual. We are \"condemned to be free\" because we cannot escape making choices and taking responsibility for our choices. \\n\\n- De Beauvoir felt that many people evade their freedom and responsibility by adopting rigid mindsets, ideologies, or conforming uncritically to social roles.\\n\\n- She advocated for the recognition of ambiguity in the human condition and warned against the quest for absolute rules that deny freedom and responsibility. Authentic living involves embracing ambiguity.\\n\\nIn summary, de Beauvoir promoted an existential ethics')} chain_with_history . invoke ( [ HumanMessage ( content = \"How did this compare to Sartre\" ) ] , config = { \"configurable\" : { \"session_id\" : \"baz\" } } , ) {'output_message': AIMessage(content=\" There are many similarities between Simone de Beauvoir's views on free will and those of Jean-Paul Sartre, though some key differences emerge as well:\\n\\nSimilarities with Sartre:\\n\\n- Both were existentialist thinkers who rejected determinism and emphasized human freedom and responsibility.\\n\\n- They agreed that existence precedes essence - there is no predefined human nature that determines who we are.\\n\\n- Individuals must define themselves through their choices and actions. This leads to anxiety but also freedom.\\n\\n- The human condition is characterized by ambiguity and uncertainty, rather than fixed meanings/values.\\n\\n- Both felt that most people evade their freedom through self-deception, conformity, or adopting collective identities/values uncritically.\\n\\nDifferences from Sartre: \\n\\n- Sartre placed more emphasis on the burden and anguish of radical freedom. De Beauvoir focused more on its positive potential.\\n\\n- De Beauvoir critiqued Sartre's premise that human relations are necessarily conflictual. She saw more potential for mutual recognition.\\n\\n- Sartre saw the Other's gaze as a threat to freedom. De Beauvoir put more stress on how the Other's gaze can confirm\")} LangSmith\ntrace More examples ​ We could also do any of the below: from operator import itemgetter # messages in, messages out RunnableWithMessageHistory ( ChatAnthropic ( model = \"claude-2\" ) , lambda session_id : RedisChatMessageHistory ( session_id , url = REDIS_URL ) , ) # dict with single key for all messages in, messages out RunnableWithMessageHistory ( itemgetter ( \"input_messages\" ) | ChatAnthropic ( model = \"claude-2\" ) , lambda session_id : RedisChatMessageHistory ( session_id , url = REDIS_URL ) , input_messages_key = \"input_messages\" , ) Previous Stream custom generator functions Next Cookbook"
    },
    {
        "title": "Prompt + LLM | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/cookbook/prompt_llm_parser",
        "text_content": "LangChain Expression Language Cookbook Prompt + LLM On this page Prompt + LLM The most common and valuable composition is taking: PromptTemplate / ChatPromptTemplate - > LLM / ChatModel - > OutputParser Almost any other chains you build will use this building block. PromptTemplate + LLM ​ The simplest composition is just combining a prompt and model to create\na chain that takes user input, adds it to a prompt, passes it to a\nmodel, and returns the raw model output. Note, you can mix and match PromptTemplate/ChatPromptTemplates and\nLLMs/ChatModels as you like here. from langchain . chat_models import ChatOpenAI from langchain . prompts import ChatPromptTemplate prompt = ChatPromptTemplate . from_template ( \"tell me a joke about {foo}\" ) model = ChatOpenAI ( ) chain = prompt | model chain . invoke ( { \"foo\" : \"bears\" } ) AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\", additional_kwargs={}, example=False) Often times we want to attach kwargs that’ll be passed to each model\ncall. Here are a few examples of that: Attaching Stop Sequences ​ chain = prompt | model . bind ( stop = [ \"\\n\" ] ) chain . invoke ( { \"foo\" : \"bears\" } ) AIMessage(content='Why did the bear never wear shoes?', additional_kwargs={}, example=False) Attaching Function Call information ​ functions = [ { \"name\" : \"joke\" , \"description\" : \"A joke\" , \"parameters\" : { \"type\" : \"object\" , \"properties\" : { \"setup\" : { \"type\" : \"string\" , \"description\" : \"The setup for the joke\" } , \"punchline\" : { \"type\" : \"string\" , \"description\" : \"The punchline for the joke\" , } , } , \"required\" : [ \"setup\" , \"punchline\" ] , } , } ] chain = prompt | model . bind ( function_call = { \"name\" : \"joke\" } , functions = functions ) chain . invoke ( { \"foo\" : \"bears\" } , config = { } ) AIMessage(content='', additional_kwargs={'function_call': {'name': 'joke', 'arguments': '{\\n  \"setup\": \"Why don\\'t bears wear shoes?\",\\n  \"punchline\": \"Because they have bear feet!\"\\n}'}}, example=False) PromptTemplate + LLM + OutputParser ​ We can also add in an output parser to easily transform the raw\nLLM/ChatModel output into a more workable format from langchain_core . output_parsers import StrOutputParser chain = prompt | model | StrOutputParser ( ) Notice that this now returns a string - a much more workable format for\ndownstream tasks chain . invoke ( { \"foo\" : \"bears\" } ) \"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\" Functions Output Parser ​ When you specify the function to return, you may just want to parse that\ndirectly from langchain . output_parsers . openai_functions import JsonOutputFunctionsParser chain = ( prompt | model . bind ( function_call = { \"name\" : \"joke\" } , functions = functions ) | JsonOutputFunctionsParser ( ) ) chain . invoke ( { \"foo\" : \"bears\" } ) {'setup': \"Why don't bears like fast food?\", 'punchline': \"Because they can't catch it!\"} from langchain . output_parsers . openai_functions import JsonKeyOutputFunctionsParser chain = ( prompt | model . bind ( function_call = { \"name\" : \"joke\" } , functions = functions ) | JsonKeyOutputFunctionsParser ( key_name = \"setup\" ) ) chain . invoke ( { \"foo\" : \"bears\" } ) \"Why don't bears wear shoes?\" Simplifying input ​ To make invocation even simpler, we can add a RunnableParallel to take\ncare of creating the prompt input dict for us: from langchain_core . runnables import RunnableParallel , RunnablePassthrough map_ = RunnableParallel ( foo = RunnablePassthrough ( ) ) chain = ( map_ | prompt | model . bind ( function_call = { \"name\" : \"joke\" } , functions = functions ) | JsonKeyOutputFunctionsParser ( key_name = \"setup\" ) ) chain . invoke ( \"bears\" ) \"Why don't bears wear shoes?\" Since we’re composing our map with another Runnable, we can even use\nsome syntactic sugar and just use a dict: chain = ( { \"foo\" : RunnablePassthrough ( ) } | prompt | model . bind ( function_call = { \"name\" : \"joke\" } , functions = functions ) | JsonKeyOutputFunctionsParser ( key_name = \"setup\" ) ) chain . invoke ( \"bears\" ) \"Why don't bears like fast food?\" Previous Cookbook Next RAG"
    },
    {
        "title": "Multiple chains | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/cookbook/multiple_chains",
        "text_content": "LangChain Expression Language Cookbook Multiple chains On this page Multiple chains Runnables can easily be used to string together multiple Chains from operator import itemgetter from langchain . chat_models import ChatOpenAI from langchain . prompts import ChatPromptTemplate from langchain . schema import StrOutputParser prompt1 = ChatPromptTemplate . from_template ( \"what is the city {person} is from?\" ) prompt2 = ChatPromptTemplate . from_template ( \"what country is the city {city} in? respond in {language}\" ) model = ChatOpenAI ( ) chain1 = prompt1 | model | StrOutputParser ( ) chain2 = ( { \"city\" : chain1 , \"language\" : itemgetter ( \"language\" ) } | prompt2 | model | StrOutputParser ( ) ) chain2 . invoke ( { \"person\" : \"obama\" , \"language\" : \"spanish\" } ) 'El país donde se encuentra la ciudad de Honolulu, donde nació Barack Obama, el 44º Presidente de los Estados Unidos, es Estados Unidos. Honolulu se encuentra en la isla de Oahu, en el estado de Hawái.' from langchain_core . runnables import RunnablePassthrough prompt1 = ChatPromptTemplate . from_template ( \"generate a {attribute} color. Return the name of the color and nothing else:\" ) prompt2 = ChatPromptTemplate . from_template ( \"what is a fruit of color: {color}. Return the name of the fruit and nothing else:\" ) prompt3 = ChatPromptTemplate . from_template ( \"what is a country with a flag that has the color: {color}. Return the name of the country and nothing else:\" ) prompt4 = ChatPromptTemplate . from_template ( \"What is the color of {fruit} and the flag of {country}?\" ) model_parser = model | StrOutputParser ( ) color_generator = ( { \"attribute\" : RunnablePassthrough ( ) } | prompt1 | { \"color\" : model_parser } ) color_to_fruit = prompt2 | model_parser color_to_country = prompt3 | model_parser question_generator = ( color_generator | { \"fruit\" : color_to_fruit , \"country\" : color_to_country } | prompt4 ) question_generator . invoke ( \"warm\" ) ChatPromptValue(messages=[HumanMessage(content='What is the color of strawberry and the flag of China?', additional_kwargs={}, example=False)]) prompt = question_generator . invoke ( \"warm\" ) model . invoke ( prompt ) AIMessage(content='The color of an apple is typically red or green. The flag of China is predominantly red with a large yellow star in the upper left corner and four smaller yellow stars surrounding it.', additional_kwargs={}, example=False) Branching and Merging ​ You may want the output of one component to be processed by 2 or more\nother components. RunnableParallels let you split or fork the chain so multiple components can process the\ninput in parallel. Later, other components can join or merge the results\nto synthesize a final response. This type of chain creates a computation\ngraph that looks like the following: Input / \\ /   \\ Branch1 Branch2 \\   / \\ / Combine planner = ( ChatPromptTemplate . from_template ( \"Generate an argument about: {input}\" ) | ChatOpenAI ( ) | StrOutputParser ( ) | { \"base_response\" : RunnablePassthrough ( ) } ) arguments_for = ( ChatPromptTemplate . from_template ( \"List the pros or positive aspects of {base_response}\" ) | ChatOpenAI ( ) | StrOutputParser ( ) ) arguments_against = ( ChatPromptTemplate . from_template ( \"List the cons or negative aspects of {base_response}\" ) | ChatOpenAI ( ) | StrOutputParser ( ) ) final_responder = ( ChatPromptTemplate . from_messages ( [ ( \"ai\" , \"{original_response}\" ) , ( \"human\" , \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\" ) , ( \"system\" , \"Generate a final response given the critique\" ) , ] ) | ChatOpenAI ( ) | StrOutputParser ( ) ) chain = ( planner | { \"results_1\" : arguments_for , \"results_2\" : arguments_against , \"original_response\" : itemgetter ( \"base_response\" ) , } | final_responder ) chain . invoke ( { \"input\" : \"scrum\" } ) 'While Scrum has its potential cons and challenges, many organizations have successfully embraced and implemented this project management framework to great effect. The cons mentioned above can be mitigated or overcome with proper training, support, and a commitment to continuous improvement. It is also important to note that not all cons may be applicable to every organization or project.\\n\\nFor example, while Scrum may be complex initially, with proper training and guidance, teams can quickly grasp the concepts and practices. The lack of predictability can be mitigated by implementing techniques such as velocity tracking and release planning. The limited documentation can be addressed by maintaining a balance between lightweight documentation and clear communication among team members. The dependency on team collaboration can be improved through effective communication channels and regular team-building activities.\\n\\nScrum can be scaled and adapted to larger projects by using frameworks like Scrum of Scrums or LeSS (Large Scale Scrum). Concerns about speed versus quality can be addressed by incorporating quality assurance practices, such as continuous integration and automated testing, into the Scrum process. Scope creep can be managed by having a well-defined and prioritized product backlog, and a strong product owner can be developed through training and mentorship.\\n\\nResistance to change can be overcome by providing proper education and communication to stakeholders and involving them in the decision-making process. Ultimately, the cons of Scrum can be seen as opportunities for growth and improvement, and with the right mindset and support, they can be effectively managed.\\n\\nIn conclusion, while Scrum may have its challenges and potential cons, the benefits and advantages it offers in terms of collaboration, flexibility, adaptability, transparency, and customer satisfaction make it a widely adopted and successful project management framework. With proper implementation and continuous improvement, organizations can leverage Scrum to drive innovation, efficiency, and project success.' Previous RAG Next Querying a SQL DB"
    },
    {
        "title": "Querying a SQL DB | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/cookbook/sql_db",
        "text_content": "LangChain Expression Language Cookbook Querying a SQL DB Querying a SQL DB We can replicate our SQLDatabaseChain with Runnables. from langchain . prompts import ChatPromptTemplate template = \"\"\"Based on the table schema below, write a SQL query that would answer the user's question: {schema} Question: {question} SQL Query:\"\"\" prompt = ChatPromptTemplate . from_template ( template ) from langchain . utilities import SQLDatabase We’ll need the Chinook sample DB for this example. There’s many places\nto download it from,\ne.g. https://database.guide/2-sample-databases-sqlite/ db = SQLDatabase . from_uri ( \"sqlite:///./Chinook.db\" ) def get_schema ( _ ) : return db . get_table_info ( ) def run_query ( query ) : return db . run ( query ) from langchain . chat_models import ChatOpenAI from langchain_core . output_parsers import StrOutputParser from langchain_core . runnables import RunnablePassthrough model = ChatOpenAI ( ) sql_response = ( RunnablePassthrough . assign ( schema = get_schema ) | prompt | model . bind ( stop = [ \"\\nSQLResult:\" ] ) | StrOutputParser ( ) ) sql_response . invoke ( { \"question\" : \"How many employees are there?\" } ) 'SELECT COUNT(*) FROM Employee' template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response: {schema} Question: {question} SQL Query: {query} SQL Response: {response}\"\"\" prompt_response = ChatPromptTemplate . from_template ( template ) full_chain = ( RunnablePassthrough . assign ( query = sql_response ) | RunnablePassthrough . assign ( schema = get_schema , response = lambda x : db . run ( x [ \"query\" ] ) , ) | prompt_response | model ) full_chain . invoke ( { \"question\" : \"How many employees are there?\" } ) AIMessage(content='There are 8 employees.', additional_kwargs={}, example=False) Previous Multiple chains Next Agents"
    },
    {
        "title": "Agents | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/cookbook/agent",
        "text_content": "LangChain Expression Language Cookbook Agents Agents You can pass a Runnable into an agent. from langchain . agents import AgentExecutor , XMLAgent , tool from langchain . chat_models import ChatAnthropic model = ChatAnthropic ( model = \"claude-2\" ) @tool def search ( query : str ) - > str : \"\"\"Search things about current events.\"\"\" return \"32 degrees\" tool_list = [ search ] # Get prompt to use prompt = XMLAgent . get_default_prompt ( ) # Logic for going from intermediate steps to a string to pass into model # This is pretty tied to the prompt def convert_intermediate_steps ( intermediate_steps ) : log = \"\" for action , observation in intermediate_steps : log += ( f\"<tool> { action . tool } </tool><tool_input> { action . tool_input } \" f\"</tool_input><observation> { observation } </observation>\" ) return log # Logic for converting tools to string to go in prompt def convert_tools ( tools ) : return \"\\n\" . join ( [ f\" { tool . name } : { tool . description } \" for tool in tools ] ) Building an agent from a runnable usually involves a few things: Data processing for the intermediate steps. These need to\nrepresented in a way that the language model can recognize them.\nThis should be pretty tightly coupled to the instructions in the\nprompt The prompt itself The model, complete with stop tokens if needed The output parser - should be in sync with how the prompt specifies\nthings to be formatted. agent = ( { \"question\" : lambda x : x [ \"question\" ] , \"intermediate_steps\" : lambda x : convert_intermediate_steps ( x [ \"intermediate_steps\" ] ) , } | prompt . partial ( tools = convert_tools ( tool_list ) ) | model . bind ( stop = [ \"</tool_input>\" , \"</final_answer>\" ] ) | XMLAgent . get_default_output_parser ( ) ) agent_executor = AgentExecutor ( agent = agent , tools = tool_list , verbose = True ) agent_executor . invoke ( { \"question\" : \"whats the weather in New york?\" } ) > Entering new AgentExecutor chain... <tool>search</tool> <tool_input>weather in new york32 degrees <final_answer>The weather in New York is 32 degrees > Finished chain. {'question': 'whats the weather in New york?', 'output': 'The weather in New York is 32 degrees'} Previous Querying a SQL DB Next Code writing"
    },
    {
        "title": "Code writing | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/cookbook/code_writing",
        "text_content": "LangChain Expression Language Cookbook Code writing Code writing Example of how to use LCEL to write Python code. from langchain . chat_models import ChatOpenAI from langchain . prompts import ( ChatPromptTemplate , ) from langchain_core . output_parsers import StrOutputParser from langchain_experimental . utilities import PythonREPL template = \"\"\"Write some python code to solve the user's problem. Return only python code in Markdown format, e.g.: ```python .... ```\"\"\" prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , template ) , ( \"human\" , \"{input}\" ) ] ) model = ChatOpenAI ( ) def _sanitize_output ( text : str ) : _ , after = text . split ( \"```python\" ) return after . split ( \"```\" ) [ 0 ] chain = prompt | model | StrOutputParser ( ) | _sanitize_output | PythonREPL ( ) . run chain . invoke ( { \"input\" : \"whats 2 plus 2\" } ) Python REPL can execute arbitrary code. Use with caution. '4\\n' Previous Agents Next Routing by semantic similarity"
    },
    {
        "title": "Routing by semantic similarity | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/cookbook/embedding_router",
        "text_content": "LangChain Expression Language Cookbook Routing by semantic similarity Routing by semantic similarity With LCEL you can easily add custom routing\nlogic to your chain to dynamically determine the chain logic based on user\ninput. All you need to do is define a function that given an input\nreturns a Runnable . One especially useful technique is to use embeddings to route a query to\nthe most relevant prompt. Here’s a very simple example. from langchain . chat_models import ChatOpenAI from langchain . embeddings import OpenAIEmbeddings from langchain . prompts import PromptTemplate from langchain . utils . math import cosine_similarity from langchain_core . output_parsers import StrOutputParser from langchain_core . runnables import RunnableLambda , RunnablePassthrough physics_template = \"\"\"You are a very smart physics professor. \\ You are great at answering questions about physics in a concise and easy to understand manner. \\ When you don't know the answer to a question you admit that you don't know. Here is a question: {query}\"\"\" math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\ You are so good because you are able to break down hard problems into their component parts, \\ answer the component parts, and then put them together to answer the broader question. Here is a question: {query}\"\"\" embeddings = OpenAIEmbeddings ( ) prompt_templates = [ physics_template , math_template ] prompt_embeddings = embeddings . embed_documents ( prompt_templates ) def prompt_router ( input ) : query_embedding = embeddings . embed_query ( input [ \"query\" ] ) similarity = cosine_similarity ( [ query_embedding ] , prompt_embeddings ) [ 0 ] most_similar = prompt_templates [ similarity . argmax ( ) ] print ( \"Using MATH\" if most_similar == math_template else \"Using PHYSICS\" ) return PromptTemplate . from_template ( most_similar ) chain = ( { \"query\" : RunnablePassthrough ( ) } | RunnableLambda ( prompt_router ) | ChatOpenAI ( ) | StrOutputParser ( ) ) print ( chain . invoke ( \"What's a black hole\" ) ) Using PHYSICS A black hole is a region in space where gravity is extremely strong, so strong that nothing, not even light, can escape its gravitational pull. It is formed when a massive star collapses under its own gravity during a supernova explosion. The collapse causes an incredibly dense mass to be concentrated in a small volume, creating a gravitational field that is so intense that it warps space and time. Black holes have a boundary called the event horizon, which marks the point of no return for anything that gets too close. Beyond the event horizon, the gravitational pull is so strong that even light cannot escape, hence the name \"black hole.\" While we have a good understanding of black holes, there is still much to learn, especially about what happens inside them. print ( chain . invoke ( \"What's a path integral\" ) ) Using MATH Thank you for your kind words! I will do my best to break down the concept of a path integral for you. In mathematics and physics, a path integral is a mathematical tool used to calculate the probability amplitude or wave function of a particle or system of particles. It was introduced by Richard Feynman and is an integral over all possible paths that a particle can take to go from an initial state to a final state. To understand the concept better, let's consider an example. Suppose we have a particle moving from point A to point B in space. Classically, we would describe this particle's motion using a definite trajectory, but in quantum mechanics, particles can simultaneously take multiple paths from A to B. The path integral formalism considers all possible paths that the particle could take and assigns a probability amplitude to each path. These probability amplitudes are then added up, taking into account the interference effects between different paths. To calculate a path integral, we need to define an action, which is a mathematical function that describes the behavior of the system. The action is usually expressed in terms of the particle's position, velocity, and time. Once we have the action, we can write down the path integral as an integral over all possible paths. Each path is weighted by a factor determined by the action and the principle of least action, which states that a particle takes a path that minimizes the action. Mathematically, the path integral is expressed as: ∫ e^(iS/ħ) D[x(t)] Here, S is the action, ħ is the reduced Planck's constant, and D[x(t)] represents the integration over all possible paths x(t) of the particle. By evaluating this integral, we can obtain the probability amplitude for the particle to go from the initial state to the final state. The absolute square of this amplitude gives us the probability of finding the particle in a particular state. Path integrals have proven to be a powerful tool in various areas of physics, including quantum mechanics, quantum field theory, and statistical mechanics. They allow us to study complex systems and calculate probabilities that are difficult to obtain using other methods. I hope this explanation helps you understand the concept of a path integral. If you have any further questions, feel free to ask! Previous Code writing Next Adding memory"
    },
    {
        "title": "Adding memory | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/cookbook/memory",
        "text_content": "LangChain Expression Language Cookbook Adding memory Adding memory This shows how to add memory to an arbitrary chain. Right now, you can\nuse the memory classes but need to hook it up manually from operator import itemgetter from langchain . chat_models import ChatOpenAI from langchain . memory import ConversationBufferMemory from langchain . prompts import ChatPromptTemplate , MessagesPlaceholder from langchain_core . runnables import RunnableLambda , RunnablePassthrough model = ChatOpenAI ( ) prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , \"You are a helpful chatbot\" ) , MessagesPlaceholder ( variable_name = \"history\" ) , ( \"human\" , \"{input}\" ) , ] ) memory = ConversationBufferMemory ( return_messages = True ) memory . load_memory_variables ( { } ) {'history': []} chain = ( RunnablePassthrough . assign ( history = RunnableLambda ( memory . load_memory_variables ) | itemgetter ( \"history\" ) ) | prompt | model ) inputs = { \"input\" : \"hi im bob\" } response = chain . invoke ( inputs ) response AIMessage(content='Hello Bob! How can I assist you today?', additional_kwargs={}, example=False) memory . save_context ( inputs , { \"output\" : response . content } ) memory . load_memory_variables ( { } ) {'history': [HumanMessage(content='hi im bob', additional_kwargs={}, example=False), AIMessage(content='Hello Bob! How can I assist you today?', additional_kwargs={}, example=False)]} inputs = { \"input\" : \"whats my name\" } response = chain . invoke ( inputs ) response AIMessage(content='Your name is Bob.', additional_kwargs={}, example=False) Previous Routing by semantic similarity Next Adding moderation"
    },
    {
        "title": "Adding moderation | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/cookbook/moderation",
        "text_content": "LangChain Expression Language Cookbook Adding moderation Adding moderation This shows how to add in moderation (or other safeguards) around your\nLLM application. from langchain . chains import OpenAIModerationChain from langchain . llms import OpenAI from langchain . prompts import ChatPromptTemplate moderate = OpenAIModerationChain ( ) model = OpenAI ( ) prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , \"repeat after me: {input}\" ) ] ) chain = prompt | model chain . invoke ( { \"input\" : \"you are stupid\" } ) '\\n\\nYou are stupid.' moderated_chain = chain | moderate moderated_chain . invoke ( { \"input\" : \"you are stupid\" } ) {'input': '\\n\\nYou are stupid', 'output': \"Text was found that violates OpenAI's content policy.\"} Previous Adding memory Next Managing prompt size"
    },
    {
        "title": "Managing prompt size | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/cookbook/prompt_size",
        "text_content": "LangChain Expression Language Cookbook Managing prompt size Managing prompt size Agents dynamically call tools. The results of those tool calls are added\nback to the prompt, so that the agent can plan the next action.\nDepending on what tools are being used and how they’re being called, the\nagent prompt can easily grow larger than the model context window. With LCEL, it’s easy to add custom functionality for managing the size\nof prompts within your chain or agent. Let’s look at simple agent\nexample that can search Wikipedia for information. # !pip install langchain wikipedia from operator import itemgetter from langchain . agents import AgentExecutor , load_tools from langchain . agents . format_scratchpad import format_to_openai_function_messages from langchain . agents . output_parsers import OpenAIFunctionsAgentOutputParser from langchain . chat_models import ChatOpenAI from langchain . prompts import ChatPromptTemplate , MessagesPlaceholder from langchain . prompts . chat import ChatPromptValue from langchain . tools import WikipediaQueryRun from langchain . tools . render import format_tool_to_openai_function from langchain . utilities import WikipediaAPIWrapper wiki = WikipediaQueryRun ( api_wrapper = WikipediaAPIWrapper ( top_k_results = 5 , doc_content_chars_max = 10_000 ) ) tools = [ wiki ] prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , \"You are a helpful assistant\" ) , ( \"user\" , \"{input}\" ) , MessagesPlaceholder ( variable_name = \"agent_scratchpad\" ) , ] ) llm = ChatOpenAI ( model = \"gpt-3.5-turbo\" ) Let’s try a many-step question without any prompt size handling: agent = ( { \"input\" : itemgetter ( \"input\" ) , \"agent_scratchpad\" : lambda x : format_to_openai_function_messages ( x [ \"intermediate_steps\" ] ) , } | prompt | llm . bind ( functions = [ format_tool_to_openai_function ( t ) for t in tools ] ) | OpenAIFunctionsAgentOutputParser ( ) ) agent_executor = AgentExecutor ( agent = agent , tools = tools , verbose = True ) agent_executor . invoke ( { \"input\" : \"Who is the current US president? What's their home state? What's their home state's bird? What's that bird's scientific name?\" } ) > Entering new AgentExecutor chain... Invoking: `Wikipedia` with `List of presidents of the United States` Page: List of presidents of the United States Summary: The president of the United States is the head of state and head of government of the United States, indirectly elected to a four-year term via the Electoral College. The officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. Since the office was established in 1789, 45 men have served in 46 presidencies. The first president, George Washington, won a unanimous vote of the Electoral College. Grover Cleveland served two non-consecutive terms and is therefore counted as the 22nd and 24th president of the United States, giving rise to the discrepancy between the number of presidencies and the number of persons who have served as president. The incumbent president is Joe Biden.The presidency of William Henry Harrison, who died 31 days after taking office in 1841, was the shortest in American history. Franklin D. Roosevelt served the longest, over twelve years, before dying early in his fourth term in 1945. He is the only U.S. president to have served more than two terms. Since the ratification of the Twenty-second Amendment to the United States Constitution in 1951, no person may be elected president more than twice, and no one who has served more than two years of a term to which someone else was elected may be elected more than once.Four presidents died in office of natural causes (William Henry Harrison, Zachary Taylor, Warren G. Harding, and Franklin D. Roosevelt), four were assassinated (Abraham Lincoln, James A. Garfield, William McKinley, and John F. Kennedy), and one resigned (Richard Nixon, facing impeachment and removal from office). John Tyler was the first vice president to assume the presidency during a presidential term, and set the precedent that a vice president who does so becomes the fully functioning president with his presidency.Throughout most of its history, American politics has been dominated by political parties. The Constitution is silent on the issue of political parties, and at the time it came into force in 1789, no organized parties existed. Soon after the 1st Congress convened, political factions began rallying around dominant Washington administration officials, such as Alexander Hamilton and Thomas Jefferson. Concerned about the capacity of political parties to destroy the fragile unity holding the nation together, Washington remained unaffiliated with any political faction or party throughout his eight-year presidency. He was, and remains, the only U.S. president never affiliated with a political party. Page: List of presidents of the United States by age Summary: In this list of presidents of the United States by age, the first table charts the age of each president of the United States at the time of presidential inauguration (first inauguration if elected to multiple and consecutive terms), upon leaving office, and at the time of death. Where the president is still living, their lifespan and post-presidency timespan are calculated up to November 14, 2023. Page: List of vice presidents of the United States Summary: There have been 49 vice presidents of the United States since the office was created in 1789. Originally, the vice president was the person who received the second-most votes for president in the Electoral College. But after the election of 1800 produced a tie between Thomas Jefferson and Aaron Burr, requiring the House of Representatives to choose between them, lawmakers acted to prevent such a situation from recurring. The Twelfth Amendment was added to the Constitution in 1804, creating the current system where electors cast a separate ballot for the vice presidency.The vice president is the first person in the presidential line of succession—that is, they assume the presidency if the president dies, resigns, or is impeached and removed from office. Nine vice presidents have ascended to the presidency in this way: eight (John Tyler, Millard Fillmore, Andrew Johnson, Chester A. Arthur, Theodore Roosevelt, Calvin Coolidge, Harry S. Truman, and Lyndon B. Johnson) through the president's death and one (Gerald Ford) through the president's resignation. The vice president also serves as the president of the Senate and may choose to cast a tie-breaking vote on decisions made by the Senate. Vice presidents have exercised this latter power to varying extents over the years.Before adoption of the Twenty-fifth Amendment in 1967, an intra-term vacancy in the office of the vice president could not be filled until the next post-election inauguration. Several such vacancies occurred: seven vice presidents died, one resigned and eight succeeded to the presidency. This amendment allowed for a vacancy to be filled through appointment by the president and confirmation by both chambers of the Congress. Since its ratification, the vice presidency has been vacant twice (both in the context of scandals surrounding the Nixon administration) and was filled both times through this process, namely in 1973 following Spiro Agnew's resignation, and again in 1974 after Gerald Ford succeeded to the presidency. The amendment also established a procedure whereby a vice president may, if the president is unable to discharge the powers and duties of the office, temporarily assume the powers and duties of the office as acting president. Three vice presidents have briefly acted as president under the 25th Amendment: George H. W. Bush on July 13, 1985; Dick Cheney on June 29, 2002, and on July 21, 2007; and Kamala Harris on November 19, 2021. The persons who have served as vice president were born in or primarily affiliated with 27 states plus the District of Columbia. New York has produced the most of any state as eight have been born there and three others considered it their home state. Most vice presidents have been in their 50s or 60s and had political experience before assuming the office. Two vice presidents—George Clinton and John C. Calhoun—served under more than one president. Ill with tuberculosis and recovering in Cuba on Inauguration Day in 1853, William R. King, by an Act of Congress, was allowed to take the oath outside the United States. He is the only vice president to take his oath of office in a foreign country. Page: List of presidents of the United States by net worth Summary: The list of presidents of the United States by net worth at peak varies greatly. Debt and depreciation often means that presidents' net worth is less than $0 at the time of death. Most presidents before 1845 were extremely wealthy, especially Andrew Jackson and George Washington. Presidents since 1929, when Herbert Hoover took office, have generally been wealthier than presidents of the late nineteenth and early twentieth centuries; with the exception of Harry S. Truman, all presidents since this time have been millionaires. These presidents have often received income from autobiographies and other writing. Except for Franklin D. Roosevelt and John F. Kennedy (both of whom died while in office), all presidents beginning with Calvin Coolidge have written autobiographies. In addition, many presidents—including Bill Clinton—have earned considerable income from public speaking after leaving office.The richest president in history may be Donald Trump. However, his net worth is not precisely known because the Trump Organization is privately held.Truman was among the poorest U.S. presidents, with a net worth considerably less than $1 million. His financial situation contributed to the doubling of the presidential salary to $100,000 in 1949. In addition, the presidential pension was created in 1958 when Truman was again experiencing financial difficulties. Harry and Bess Truman received the first Medicare cards in 1966 via the Social Security Act of 1965. Page: List of presidents of the United States by home state Summary: These lists give the states of primary affiliation and of birth for each president of the United States. Invoking: `Wikipedia` with `Joe Biden` Page: Joe Biden Summary: Joseph Robinette Biden Jr. (  BY-dən; born November 20, 1942) is an American politician who is the 46th and current president of the United States. Ideologically a moderate member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama and represented Delaware in the United States Senate from 1973 to 2009. Born in Scranton, Pennsylvania, Biden moved with his family to Delaware in 1953. He studied at the University of Delaware before earning his law degree from Syracuse University. He was elected to the New Castle County Council in 1970 and to the U.S. Senate in 1972. As a senator, Biden drafted and led the effort to pass the Violent Crime Control and Law Enforcement Act and the Violence Against Women Act. He also oversaw six U.S. Supreme Court confirmation hearings, including the contentious hearings for Robert Bork and Clarence Thomas. Biden ran unsuccessfully for the Democratic presidential nomination in 1988 and 2008. In 2008, Obama chose Biden as his running mate, and Biden was a close counselor to Obama during his two terms as vice president. In the 2020 presidential election, Biden and his running mate, Kamala Harris, defeated incumbents Donald Trump and Mike Pence. Biden is the second Catholic president in U.S. history (after John F. Kennedy), and his politics have been widely described as profoundly influenced by Catholic social teaching. Taking office at age 78, Biden is the oldest president in U.S. history, the first to have a female vice president, and the first from Delaware. In 2021, he signed a bipartisan infrastructure bill, as well as a $1.9 trillion economic stimulus package in response to the COVID-19 pandemic and its related recession. Biden proposed the Build Back Better Act, which failed in Congress, but aspects of which were incorporated into the Inflation Reduction Act that was signed into law in 2022. Biden also signed the bipartisan CHIPS and Science Act, which focused on manufacturing, appointed Ketanji Brown Jackson to the Supreme Court and worked with congressional Republicans to prevent a first-ever national default by negotiating a deal to raise the debt ceiling. In foreign policy, Biden restored America's membership in the Paris Agreement. He oversaw the complete withdrawal of U.S. troops from Afghanistan that ended the war in Afghanistan, during which the Afghan government collapsed and the Taliban seized control. Biden has responded to the Russian invasion of Ukraine by imposing sanctions on Russia and authorizing civilian and military aid to Ukraine. During the 2023 Israel–Hamas war, Biden announced American military support for Israel, and condemned the actions of Hamas and other Palestinian militants as terrorism. In April 2023, he announced his candidacy for the Democratic Party nomination in the 2024 presidential election. Page: Presidency of Joe Biden Summary: Joe Biden's tenure as the 46th president of the United States began with his inauguration on January 20, 2021. Biden, a Democrat from Delaware who previously served as vice president under Barack Obama, took office following his victory in the 2020 presidential election over Republican incumbent president Donald Trump. Upon his inauguration, he became the oldest president in American history, breaking the record set by his predecessor Trump. Biden entered office amid the COVID-19 pandemic, an economic crisis, and increased political polarization.On the first day of his presidency, Biden made an effort to revert President Trump's energy policy by restoring U.S. participation in the Paris Agreement and revoking the permit for the Keystone XL pipeline. He also halted funding for Trump's border wall, an expansion of the Mexican border wall. On his second day, he issued a series of executive orders to reduce the impact of COVID-19, including invoking the Defense Production Act of 1950, and set an early goal of achieving one hundred million COVID-19 vaccinations in the United States in his first 100 days.Biden signed into law the American Rescue Plan Act of 2021; a $1.9 trillion stimulus bill that temporarily established expanded unemployment insurance and sent $1,400 stimulus checks to most Americans in response to continued economic pressure from COVID-19. He signed the bipartisan Infrastructure Investment and Jobs Act; a ten-year plan brokered by Biden alongside Democrats and Republicans in Congress, to invest in American roads, bridges, public transit, ports and broadband access. Biden signed the Juneteenth National Independence Day Act, making Juneteenth a federal holiday in the United States. He appointed Ketanji Brown Jackson to the U.S. Supreme Court—the first Black woman to serve on the court. After The Supreme Court overturned Roe v. Wade, Biden took executive actions, such as the signing of Executive Order 14076, to preserve and protect women's health rights nationwide, against abortion bans in Republican led states. Biden proposed a significant expansion of the U.S. social safety net through the Build Back Better Act, but those efforts, along with voting rights legislation, failed in Congress. However, in August 2022, Biden signed the Inflation Reduction Act of 2022, a domestic appropriations bill that included some of the provisions of the Build Back Better Act after the entire bill failed to pass. It included significant federal investment in climate and domestic clean energy production, tax credits for solar panels, electric cars and other home energy programs as well as a three-year extension of Affordable Care Act subsidies. Biden signed the CHIPS and Science Act, bolstering the semiconductor and manufacturing industry, the Honoring our PACT Act, expanding healthcare for US veterans, and the Electoral Count Reform and Presidential Transition Improvement Act. In late 2022, Biden signed the Respect for Marriage Act, which repealed the Defense of Marriage Act and codified same-sex and interracial marriage in the United States. In response to the debt-ceiling crisis of 2023, Biden negotiated and signed the Fiscal Responsibility Act of 2023, which restrains federal spending for fiscal years 2024 and 2025, implements minor changes to SNAP and TANF, includes energy permitting reform, claws back some IRS funding and unspent money for COVID-19, and suspends the debt ceiling to January 1, 2025. Biden established the American Climate Corps and created the first ever White House Office of Gun Violence Prevention. On September 26, 2023, Joe Biden visited a United Auto Workers picket line during the 2023 United Auto Workers strike, making him the first US president to visit one. The foreign policy goal of the Biden administration is to restore the US to a \"position of trusted leadership\" among global democracies in order to address the challenges posed by Russia and China. In foreign policy, Biden completed the withdrawal of U.S. military forces from Afghanistan, declaring an end to nation-building efforts and shifting U.S. foreign policy toward strategic competition with China and, to a lesser extent, Russia. However, during the withdrawal, the Afghan government collapsed and the Taliban seized control, leading to Biden receiving bipartisan criticism. He responded to the Russian invasion of Ukraine by imposing sanctions on Russia as well as providing Ukraine with over $100 billion in combined military, economic, and humanitarian aid. Biden also approved a raid which led to the death of Abu Ibrahim al-Hashimi al-Qurashi, the leader of the Islamic State, and approved a drone strike which killed Ayman Al Zawahiri, leader of Al-Qaeda. Biden signed AUKUS, an international security alliance, together with Australia and the United Kingdom. Biden called for the expansion of NATO with the addition of Finland and Sweden, and rallied NATO allies in support of Ukraine. During the 2023 Israel–Hamas war, Biden condemned Hamas and other Palestinian militants as terrorism and announced American military support for Israel; Biden also showed his support and sympathy towards Palestinians affected by the war and has sent humanitarian aid. Biden began his term with over 50% approval ratings; however, these fell significantly after the withdrawal from Afghanistan and remained low as the country experienced high inflation and rising gas prices. His age and mental fitness have also been a subject of discussion. Page: Family of Joe Biden Summary: Joe Biden, the 46th and current president of the United States, has family members who are prominent in law, education, activism and politics. Biden's immediate family became the first family of the United States on his inauguration on January 20, 2021. His immediate family circle was also the second family of the United States from 2009 to 2017, when Biden was vice president. Biden's family is mostly descended from the British Isles, with most of their ancestors coming from Ireland and England, and a smaller number descending from the French.Of Joe Biden's sixteen great-great-grandparents, ten were born in Ireland. He is descended from the Blewitts of County Mayo and the Finnegans of County Louth. One of Biden's great-great-great-grandfathers was born in Sussex, England, and emigrated to Maryland in the United States by 1820. Page: Cabinet of Joe Biden Summary: Joe Biden assumed office as President of the United States on January 20, 2021. The president has the authority to nominate members of his Cabinet to the United States Senate for confirmation under the Appointments Clause of the United States Constitution. Before confirmation and during congressional hearings, a high-level career member of an executive department heads this pre-confirmed cabinet on an acting basis. The Cabinet's creation was part of the transition of power following the 2020 United States presidential election. In addition to the 15 heads of executive departments, there are 10 Cabinet-level officials. Biden altered his cabinet struct Invoking: `Wikipedia` with `Delaware` Page: Delaware Summary: Delaware (  DEL-ə-wair) is a state in the Mid-Atlantic region of the United States. It borders Maryland to its south and west, Pennsylvania to its north, New Jersey to its northeast, and the Atlantic Ocean to its east. The state's name derives from the adjacent Delaware Bay, which in turn was named after Thomas West, 3rd Baron De La Warr, an English nobleman and the Colony of Virginia's first colonial-era governor.Delaware occupies the northeastern portion of the Delmarva Peninsula, and some islands and territory within the Delaware River. It is the 2nd smallest and 6th least populous state, but also the 6th most densely populated. Delaware's most populous city is Wilmington, and the state's capital is Dover, the 2nd most populous city in Delaware. The state is divided into three counties, the fewest number of counties of any of the 50 U.S. states; from north to south, the three counties are: New Castle County, Kent County, and Sussex County. The southern two counties, Kent and Sussex counties, historically have been predominantly agrarian economies/ New Castle is more urbanized and is considered part of the Delaware Valley metropolitan statistical area that surrounds and includes Philadelphia, the nation's 6th most populous city. Delaware is considered part of the Southern United States by the U.S. Census Bureau, but the state's geography, culture, and history are a hybrid of the Mid-Atlantic and Northeastern regions of the country.Before Delaware coastline was explored and developed by Europeans in the 16th century, the state was inhabited by several Native Americans tribes, including the Lenape in the north and Nanticoke in the south. The state was first colonized by Dutch traders at Zwaanendael, near present-day Lewes, Delaware, in 1631. Delaware was one of the Thirteen Colonies that participated in the American Revolution and American Revolutionary War, in which the American Continental Army, led by George Washington, defeated the British, ended British colonization and establishing the United States as a sovereign and independent nation. On December 7, 1787, Delaware was the first state to ratify the Constitution of the United States, earning the state the nickname \"The First State\".Since the turn of the 20th century, Delaware has become an onshore corporate haven whose corporate laws are deemed appealed to corporations; over half of all New York Stock Exchange-listed corporations and over three-fifths of the Fortune 500 is legally incorporated in the state. Page: Delaware City, Delaware Summary: Delaware City is a city in New Castle County, Delaware, United States. The population was 1,885 as of 2020. It is a small port town on the eastern terminus of the Chesapeake and Delaware Canal and is the location of the Forts Ferry Crossing to Fort Delaware on Pea Patch Island. Page: Delaware River Summary: The Delaware River is a major river in the Mid-Atlantic region of the United States and is the longest free-flowing (undammed) river in the Eastern United States. From the meeting of its branches in Hancock, New York, the river flows for 282 miles (454 km) along the borders of New York, Pennsylvania, New Jersey, and Delaware, before emptying into Delaware Bay. The river has been recognized by the National Wildlife Federation as one of the country's Great Waters and has been called the \"Lifeblood of the Northeast\" by American Rivers. Its watershed drains an area of 13,539 square miles (35,070 km2) and provides drinking water for 17 million people, including half of New York City via the Delaware Aqueduct. The Delaware River has two branches that rise in the Catskill Mountains of New York: the West Branch at Mount Jefferson in Jefferson, Schoharie County, and the East Branch at Grand Gorge, Delaware County. The branches merge to form the main Delaware River at Hancock, New York. Flowing south, the river remains relatively undeveloped, with 152 miles (245 km) protected as the Upper, Middle, and Lower Delaware National Scenic Rivers. At Trenton, New Jersey, the Delaware becomes tidal, navigable, and significantly more industrial. This section forms the backbone of the Delaware Valley metropolitan area, serving the port cities of Philadelphia, Camden, New Jersey, and Wilmington, Delaware. The river flows into Delaware Bay at Liston Point, 48 miles (77 km) upstream of the bay's outlet to the Atlantic Ocean between Cape May and Cape Henlopen. Before the arrival of European settlers, the river was the homeland of the Lenape native people. They called the river Lenapewihittuk, or Lenape River, and Kithanne, meaning the largest river in this part of the country.In 1609, the river was visited by a Dutch East India Company expedition led by Henry Hudson. Hudson, an English navigator, was hired to find a western route to Cathay (China), but his encounters set the stage for Dutch colonization of North America in the 17th century. Early Dutch and Swedish settlements were established along the lower section of the river and Delaware Bay. Both colonial powers called the river the South River (Zuidrivier), compared to the Hudson River, which was known as the North River. After the English expelled the Dutch and took control of the New Netherland colony in 1664, the river was renamed Delaware after Sir Thomas West, 3rd Baron De La Warr, an English nobleman and the Virginia colony's first royal governor who defended the colony during the First Anglo-Powhatan War. Page: Lenape Summary: The Lenape (English: , , ; Lenape languages: [lenaːpe]), also called the Lenni Lenape and Delaware people, are an indigenous people of the Northeastern Woodlands, who live in the United States and Canada.The Lenape's historical territory included present-day northeastern Delaware, all of New Jersey, the eastern Pennsylvania regions of the Lehigh Valley and Northeastern Pennsylvania, and New York Bay, western Long Island, and the lower Hudson Valley in New York state. Today they are based in Oklahoma, Wisconsin, and Ontario. During the last decades of the 18th century, European settlers and the effects of the American Revolutionary War displaced most Lenape from their homelands and pushed them north and west. In the 1860s, under the Indian removal policy, the U.S. federal government relocated most Lenape remaining in the Eastern United States to the Indian Territory and surrounding regions. Lenape people currently belong to the Delaware Nation and Delaware Tribe of Indians in Oklahoma, the Stockbridge–Munsee Community in Wisconsin, and the Munsee-Delaware Nation, Moravian of the Thames First Nation, and Delaware of Six Nations in Ontario. Page: University of Delaware Summary: The University of Delaware (colloquially known as UD or Delaware) is a privately governed, state-assisted land-grant research university located in Newark, Delaware. UD is the largest university in Delaware. It offers three associate's programs, 148 bachelor's programs, 121 master's programs (with 13 joint degrees), and 55 doctoral programs across its eight colleges. The main campus is in Newark, with satellite campuses in Dover, Wilmington, Lewes, and Georgetown. It is considered a large institution with approximately 18,200 undergraduate and 4,200 graduate students. It is a privately governed university which receives public funding for being a land-grant, sea-grant, and space-grant state-supported research institution.UD is classified among \"R1: Doctoral Universities – Very high research activity\". According to the National Science Foundation, UD spent $186 million on research and development in 2018, ranking it 119th in the nation.  It is recognized with the Community Engagement Classification by the Carnegie Foundation for the Advancement of Teaching.UD students, alumni, and sports teams are known as the \"Fightin' Blue Hens\", more commonly shortened to \"Blue Hens\", and the school colors are Delaware blue and gold. UD sponsors 21 men's and women's NCAA Division-I sports teams and have competed in the Colonial Athletic Association (CAA) since 2001. BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 5478 tokens (5410 in the messages, 68 in the functions). Please reduce the length of the messages or functions.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}} LangSmith\ntrace Unfortunately we run out of space in our model’s context window before\nwe the agent can get to the final answer. Now let’s add some prompt\nhandling logic. To keep things simple, if our messages have too many\ntokens we’ll start dropping the earliest AI, Function message pairs\n(this is the model tool invocation message and the subsequent tool\noutput message) in the chat history. def condense_prompt ( prompt : ChatPromptValue ) - > ChatPromptValue : messages = prompt . to_messages ( ) num_tokens = llm . get_num_tokens_from_messages ( messages ) ai_function_messages = messages [ 2 : ] while num_tokens > 4_000 : ai_function_messages = ai_function_messages [ 2 : ] num_tokens = llm . get_num_tokens_from_messages ( messages [ : 2 ] + ai_function_messages ) messages = messages [ : 2 ] + ai_function_messages return ChatPromptValue ( messages = messages ) agent = ( { \"input\" : itemgetter ( \"input\" ) , \"agent_scratchpad\" : lambda x : format_to_openai_function_messages ( x [ \"intermediate_steps\" ] ) , } | prompt | condense_prompt | llm . bind ( functions = [ format_tool_to_openai_function ( t ) for t in tools ] ) | OpenAIFunctionsAgentOutputParser ( ) ) agent_executor = AgentExecutor ( agent = agent , tools = tools , verbose = True ) agent_executor . invoke ( { \"input\" : \"Who is the current US president? What's their home state? What's their home state's bird? What's that bird's scientific name?\" } ) > Entering new AgentExecutor chain... Invoking: `Wikipedia` with `List of presidents of the United States` Page: List of presidents of the United States Summary: The president of the United States is the head of state and head of government of the United States, indirectly elected to a four-year term via the Electoral College. The officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. Since the office was established in 1789, 45 men have served in 46 presidencies. The first president, George Washington, won a unanimous vote of the Electoral College. Grover Cleveland served two non-consecutive terms and is therefore counted as the 22nd and 24th president of the United States, giving rise to the discrepancy between the number of presidencies and the number of persons who have served as president. The incumbent president is Joe Biden.The presidency of William Henry Harrison, who died 31 days after taking office in 1841, was the shortest in American history. Franklin D. Roosevelt served the longest, over twelve years, before dying early in his fourth term in 1945. He is the only U.S. president to have served more than two terms. Since the ratification of the Twenty-second Amendment to the United States Constitution in 1951, no person may be elected president more than twice, and no one who has served more than two years of a term to which someone else was elected may be elected more than once.Four presidents died in office of natural causes (William Henry Harrison, Zachary Taylor, Warren G. Harding, and Franklin D. Roosevelt), four were assassinated (Abraham Lincoln, James A. Garfield, William McKinley, and John F. Kennedy), and one resigned (Richard Nixon, facing impeachment and removal from office). John Tyler was the first vice president to assume the presidency during a presidential term, and set the precedent that a vice president who does so becomes the fully functioning president with his presidency.Throughout most of its history, American politics has been dominated by political parties. The Constitution is silent on the issue of political parties, and at the time it came into force in 1789, no organized parties existed. Soon after the 1st Congress convened, political factions began rallying around dominant Washington administration officials, such as Alexander Hamilton and Thomas Jefferson. Concerned about the capacity of political parties to destroy the fragile unity holding the nation together, Washington remained unaffiliated with any political faction or party throughout his eight-year presidency. He was, and remains, the only U.S. president never affiliated with a political party. Page: List of presidents of the United States by age Summary: In this list of presidents of the United States by age, the first table charts the age of each president of the United States at the time of presidential inauguration (first inauguration if elected to multiple and consecutive terms), upon leaving office, and at the time of death. Where the president is still living, their lifespan and post-presidency timespan are calculated up to November 14, 2023. Page: List of vice presidents of the United States Summary: There have been 49 vice presidents of the United States since the office was created in 1789. Originally, the vice president was the person who received the second-most votes for president in the Electoral College. But after the election of 1800 produced a tie between Thomas Jefferson and Aaron Burr, requiring the House of Representatives to choose between them, lawmakers acted to prevent such a situation from recurring. The Twelfth Amendment was added to the Constitution in 1804, creating the current system where electors cast a separate ballot for the vice presidency.The vice president is the first person in the presidential line of succession—that is, they assume the presidency if the president dies, resigns, or is impeached and removed from office. Nine vice presidents have ascended to the presidency in this way: eight (John Tyler, Millard Fillmore, Andrew Johnson, Chester A. Arthur, Theodore Roosevelt, Calvin Coolidge, Harry S. Truman, and Lyndon B. Johnson) through the president's death and one (Gerald Ford) through the president's resignation. The vice president also serves as the president of the Senate and may choose to cast a tie-breaking vote on decisions made by the Senate. Vice presidents have exercised this latter power to varying extents over the years.Before adoption of the Twenty-fifth Amendment in 1967, an intra-term vacancy in the office of the vice president could not be filled until the next post-election inauguration. Several such vacancies occurred: seven vice presidents died, one resigned and eight succeeded to the presidency. This amendment allowed for a vacancy to be filled through appointment by the president and confirmation by both chambers of the Congress. Since its ratification, the vice presidency has been vacant twice (both in the context of scandals surrounding the Nixon administration) and was filled both times through this process, namely in 1973 following Spiro Agnew's resignation, and again in 1974 after Gerald Ford succeeded to the presidency. The amendment also established a procedure whereby a vice president may, if the president is unable to discharge the powers and duties of the office, temporarily assume the powers and duties of the office as acting president. Three vice presidents have briefly acted as president under the 25th Amendment: George H. W. Bush on July 13, 1985; Dick Cheney on June 29, 2002, and on July 21, 2007; and Kamala Harris on November 19, 2021. The persons who have served as vice president were born in or primarily affiliated with 27 states plus the District of Columbia. New York has produced the most of any state as eight have been born there and three others considered it their home state. Most vice presidents have been in their 50s or 60s and had political experience before assuming the office. Two vice presidents—George Clinton and John C. Calhoun—served under more than one president. Ill with tuberculosis and recovering in Cuba on Inauguration Day in 1853, William R. King, by an Act of Congress, was allowed to take the oath outside the United States. He is the only vice president to take his oath of office in a foreign country. Page: List of presidents of the United States by net worth Summary: The list of presidents of the United States by net worth at peak varies greatly. Debt and depreciation often means that presidents' net worth is less than $0 at the time of death. Most presidents before 1845 were extremely wealthy, especially Andrew Jackson and George Washington. Presidents since 1929, when Herbert Hoover took office, have generally been wealthier than presidents of the late nineteenth and early twentieth centuries; with the exception of Harry S. Truman, all presidents since this time have been millionaires. These presidents have often received income from autobiographies and other writing. Except for Franklin D. Roosevelt and John F. Kennedy (both of whom died while in office), all presidents beginning with Calvin Coolidge have written autobiographies. In addition, many presidents—including Bill Clinton—have earned considerable income from public speaking after leaving office.The richest president in history may be Donald Trump. However, his net worth is not precisely known because the Trump Organization is privately held.Truman was among the poorest U.S. presidents, with a net worth considerably less than $1 million. His financial situation contributed to the doubling of the presidential salary to $100,000 in 1949. In addition, the presidential pension was created in 1958 when Truman was again experiencing financial difficulties. Harry and Bess Truman received the first Medicare cards in 1966 via the Social Security Act of 1965. Page: List of presidents of the United States by home state Summary: These lists give the states of primary affiliation and of birth for each president of the United States. Invoking: `Wikipedia` with `Joe Biden` Page: Joe Biden Summary: Joseph Robinette Biden Jr. (  BY-dən; born November 20, 1942) is an American politician who is the 46th and current president of the United States. Ideologically a moderate member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama and represented Delaware in the United States Senate from 1973 to 2009. Born in Scranton, Pennsylvania, Biden moved with his family to Delaware in 1953. He studied at the University of Delaware before earning his law degree from Syracuse University. He was elected to the New Castle County Council in 1970 and to the U.S. Senate in 1972. As a senator, Biden drafted and led the effort to pass the Violent Crime Control and Law Enforcement Act and the Violence Against Women Act. He also oversaw six U.S. Supreme Court confirmation hearings, including the contentious hearings for Robert Bork and Clarence Thomas. Biden ran unsuccessfully for the Democratic presidential nomination in 1988 and 2008. In 2008, Obama chose Biden as his running mate, and Biden was a close counselor to Obama during his two terms as vice president. In the 2020 presidential election, Biden and his running mate, Kamala Harris, defeated incumbents Donald Trump and Mike Pence. Biden is the second Catholic president in U.S. history (after John F. Kennedy), and his politics have been widely described as profoundly influenced by Catholic social teaching. Taking office at age 78, Biden is the oldest president in U.S. history, the first to have a female vice president, and the first from Delaware. In 2021, he signed a bipartisan infrastructure bill, as well as a $1.9 trillion economic stimulus package in response to the COVID-19 pandemic and its related recession. Biden proposed the Build Back Better Act, which failed in Congress, but aspects of which were incorporated into the Inflation Reduction Act that was signed into law in 2022. Biden also signed the bipartisan CHIPS and Science Act, which focused on manufacturing, appointed Ketanji Brown Jackson to the Supreme Court and worked with congressional Republicans to prevent a first-ever national default by negotiating a deal to raise the debt ceiling. In foreign policy, Biden restored America's membership in the Paris Agreement. He oversaw the complete withdrawal of U.S. troops from Afghanistan that ended the war in Afghanistan, during which the Afghan government collapsed and the Taliban seized control. Biden has responded to the Russian invasion of Ukraine by imposing sanctions on Russia and authorizing civilian and military aid to Ukraine. During the 2023 Israel–Hamas war, Biden announced American military support for Israel, and condemned the actions of Hamas and other Palestinian militants as terrorism. In April 2023, he announced his candidacy for the Democratic Party nomination in the 2024 presidential election. Page: Presidency of Joe Biden Summary: Joe Biden's tenure as the 46th president of the United States began with his inauguration on January 20, 2021. Biden, a Democrat from Delaware who previously served as vice president under Barack Obama, took office following his victory in the 2020 presidential election over Republican incumbent president Donald Trump. Upon his inauguration, he became the oldest president in American history, breaking the record set by his predecessor Trump. Biden entered office amid the COVID-19 pandemic, an economic crisis, and increased political polarization.On the first day of his presidency, Biden made an effort to revert President Trump's energy policy by restoring U.S. participation in the Paris Agreement and revoking the permit for the Keystone XL pipeline. He also halted funding for Trump's border wall, an expansion of the Mexican border wall. On his second day, he issued a series of executive orders to reduce the impact of COVID-19, including invoking the Defense Production Act of 1950, and set an early goal of achieving one hundred million COVID-19 vaccinations in the United States in his first 100 days.Biden signed into law the American Rescue Plan Act of 2021; a $1.9 trillion stimulus bill that temporarily established expanded unemployment insurance and sent $1,400 stimulus checks to most Americans in response to continued economic pressure from COVID-19. He signed the bipartisan Infrastructure Investment and Jobs Act; a ten-year plan brokered by Biden alongside Democrats and Republicans in Congress, to invest in American roads, bridges, public transit, ports and broadband access. Biden signed the Juneteenth National Independence Day Act, making Juneteenth a federal holiday in the United States. He appointed Ketanji Brown Jackson to the U.S. Supreme Court—the first Black woman to serve on the court. After The Supreme Court overturned Roe v. Wade, Biden took executive actions, such as the signing of Executive Order 14076, to preserve and protect women's health rights nationwide, against abortion bans in Republican led states. Biden proposed a significant expansion of the U.S. social safety net through the Build Back Better Act, but those efforts, along with voting rights legislation, failed in Congress. However, in August 2022, Biden signed the Inflation Reduction Act of 2022, a domestic appropriations bill that included some of the provisions of the Build Back Better Act after the entire bill failed to pass. It included significant federal investment in climate and domestic clean energy production, tax credits for solar panels, electric cars and other home energy programs as well as a three-year extension of Affordable Care Act subsidies. Biden signed the CHIPS and Science Act, bolstering the semiconductor and manufacturing industry, the Honoring our PACT Act, expanding healthcare for US veterans, and the Electoral Count Reform and Presidential Transition Improvement Act. In late 2022, Biden signed the Respect for Marriage Act, which repealed the Defense of Marriage Act and codified same-sex and interracial marriage in the United States. In response to the debt-ceiling crisis of 2023, Biden negotiated and signed the Fiscal Responsibility Act of 2023, which restrains federal spending for fiscal years 2024 and 2025, implements minor changes to SNAP and TANF, includes energy permitting reform, claws back some IRS funding and unspent money for COVID-19, and suspends the debt ceiling to January 1, 2025. Biden established the American Climate Corps and created the first ever White House Office of Gun Violence Prevention. On September 26, 2023, Joe Biden visited a United Auto Workers picket line during the 2023 United Auto Workers strike, making him the first US president to visit one. The foreign policy goal of the Biden administration is to restore the US to a \"position of trusted leadership\" among global democracies in order to address the challenges posed by Russia and China. In foreign policy, Biden completed the withdrawal of U.S. military forces from Afghanistan, declaring an end to nation-building efforts and shifting U.S. foreign policy toward strategic competition with China and, to a lesser extent, Russia. However, during the withdrawal, the Afghan government collapsed and the Taliban seized control, leading to Biden receiving bipartisan criticism. He responded to the Russian invasion of Ukraine by imposing sanctions on Russia as well as providing Ukraine with over $100 billion in combined military, economic, and humanitarian aid. Biden also approved a raid which led to the death of Abu Ibrahim al-Hashimi al-Qurashi, the leader of the Islamic State, and approved a drone strike which killed Ayman Al Zawahiri, leader of Al-Qaeda. Biden signed AUKUS, an international security alliance, together with Australia and the United Kingdom. Biden called for the expansion of NATO with the addition of Finland and Sweden, and rallied NATO allies in support of Ukraine. During the 2023 Israel–Hamas war, Biden condemned Hamas and other Palestinian militants as terrorism and announced American military support for Israel; Biden also showed his support and sympathy towards Palestinians affected by the war and has sent humanitarian aid. Biden began his term with over 50% approval ratings; however, these fell significantly after the withdrawal from Afghanistan and remained low as the country experienced high inflation and rising gas prices. His age and mental fitness have also been a subject of discussion. Page: Family of Joe Biden Summary: Joe Biden, the 46th and current president of the United States, has family members who are prominent in law, education, activism and politics. Biden's immediate family became the first family of the United States on his inauguration on January 20, 2021. His immediate family circle was also the second family of the United States from 2009 to 2017, when Biden was vice president. Biden's family is mostly descended from the British Isles, with most of their ancestors coming from Ireland and England, and a smaller number descending from the French.Of Joe Biden's sixteen great-great-grandparents, ten were born in Ireland. He is descended from the Blewitts of County Mayo and the Finnegans of County Louth. One of Biden's great-great-great-grandfathers was born in Sussex, England, and emigrated to Maryland in the United States by 1820. Page: Cabinet of Joe Biden Summary: Joe Biden assumed office as President of the United States on January 20, 2021. The president has the authority to nominate members of his Cabinet to the United States Senate for confirmation under the Appointments Clause of the United States Constitution. Before confirmation and during congressional hearings, a high-level career member of an executive department heads this pre-confirmed cabinet on an acting basis. The Cabinet's creation was part of the transition of power following the 2020 United States presidential election. In addition to the 15 heads of executive departments, there are 10 Cabinet-level officials. Biden altered his cabinet struct Invoking: `Wikipedia` with `Delaware` Page: Delaware Summary: Delaware (  DEL-ə-wair) is a state in the Mid-Atlantic region of the United States. It borders Maryland to its south and west, Pennsylvania to its north, New Jersey to its northeast, and the Atlantic Ocean to its east. The state's name derives from the adjacent Delaware Bay, which in turn was named after Thomas West, 3rd Baron De La Warr, an English nobleman and the Colony of Virginia's first colonial-era governor.Delaware occupies the northeastern portion of the Delmarva Peninsula, and some islands and territory within the Delaware River. It is the 2nd smallest and 6th least populous state, but also the 6th most densely populated. Delaware's most populous city is Wilmington, and the state's capital is Dover, the 2nd most populous city in Delaware. The state is divided into three counties, the fewest number of counties of any of the 50 U.S. states; from north to south, the three counties are: New Castle County, Kent County, and Sussex County. The southern two counties, Kent and Sussex counties, historically have been predominantly agrarian economies/ New Castle is more urbanized and is considered part of the Delaware Valley metropolitan statistical area that surrounds and includes Philadelphia, the nation's 6th most populous city. Delaware is considered part of the Southern United States by the U.S. Census Bureau, but the state's geography, culture, and history are a hybrid of the Mid-Atlantic and Northeastern regions of the country.Before Delaware coastline was explored and developed by Europeans in the 16th century, the state was inhabited by several Native Americans tribes, including the Lenape in the north and Nanticoke in the south. The state was first colonized by Dutch traders at Zwaanendael, near present-day Lewes, Delaware, in 1631. Delaware was one of the Thirteen Colonies that participated in the American Revolution and American Revolutionary War, in which the American Continental Army, led by George Washington, defeated the British, ended British colonization and establishing the United States as a sovereign and independent nation. On December 7, 1787, Delaware was the first state to ratify the Constitution of the United States, earning the state the nickname \"The First State\".Since the turn of the 20th century, Delaware has become an onshore corporate haven whose corporate laws are deemed appealed to corporations; over half of all New York Stock Exchange-listed corporations and over three-fifths of the Fortune 500 is legally incorporated in the state. Page: Delaware City, Delaware Summary: Delaware City is a city in New Castle County, Delaware, United States. The population was 1,885 as of 2020. It is a small port town on the eastern terminus of the Chesapeake and Delaware Canal and is the location of the Forts Ferry Crossing to Fort Delaware on Pea Patch Island. Page: Delaware River Summary: The Delaware River is a major river in the Mid-Atlantic region of the United States and is the longest free-flowing (undammed) river in the Eastern United States. From the meeting of its branches in Hancock, New York, the river flows for 282 miles (454 km) along the borders of New York, Pennsylvania, New Jersey, and Delaware, before emptying into Delaware Bay. The river has been recognized by the National Wildlife Federation as one of the country's Great Waters and has been called the \"Lifeblood of the Northeast\" by American Rivers. Its watershed drains an area of 13,539 square miles (35,070 km2) and provides drinking water for 17 million people, including half of New York City via the Delaware Aqueduct. The Delaware River has two branches that rise in the Catskill Mountains of New York: the West Branch at Mount Jefferson in Jefferson, Schoharie County, and the East Branch at Grand Gorge, Delaware County. The branches merge to form the main Delaware River at Hancock, New York. Flowing south, the river remains relatively undeveloped, with 152 miles (245 km) protected as the Upper, Middle, and Lower Delaware National Scenic Rivers. At Trenton, New Jersey, the Delaware becomes tidal, navigable, and significantly more industrial. This section forms the backbone of the Delaware Valley metropolitan area, serving the port cities of Philadelphia, Camden, New Jersey, and Wilmington, Delaware. The river flows into Delaware Bay at Liston Point, 48 miles (77 km) upstream of the bay's outlet to the Atlantic Ocean between Cape May and Cape Henlopen. Before the arrival of European settlers, the river was the homeland of the Lenape native people. They called the river Lenapewihittuk, or Lenape River, and Kithanne, meaning the largest river in this part of the country.In 1609, the river was visited by a Dutch East India Company expedition led by Henry Hudson. Hudson, an English navigator, was hired to find a western route to Cathay (China), but his encounters set the stage for Dutch colonization of North America in the 17th century. Early Dutch and Swedish settlements were established along the lower section of the river and Delaware Bay. Both colonial powers called the river the South River (Zuidrivier), compared to the Hudson River, which was known as the North River. After the English expelled the Dutch and took control of the New Netherland colony in 1664, the river was renamed Delaware after Sir Thomas West, 3rd Baron De La Warr, an English nobleman and the Virginia colony's first royal governor who defended the colony during the First Anglo-Powhatan War. Page: Lenape Summary: The Lenape (English: , , ; Lenape languages: [lenaːpe]), also called the Lenni Lenape and Delaware people, are an indigenous people of the Northeastern Woodlands, who live in the United States and Canada.The Lenape's historical territory included present-day northeastern Delaware, all of New Jersey, the eastern Pennsylvania regions of the Lehigh Valley and Northeastern Pennsylvania, and New York Bay, western Long Island, and the lower Hudson Valley in New York state. Today they are based in Oklahoma, Wisconsin, and Ontario. During the last decades of the 18th century, European settlers and the effects of the American Revolutionary War displaced most Lenape from their homelands and pushed them north and west. In the 1860s, under the Indian removal policy, the U.S. federal government relocated most Lenape remaining in the Eastern United States to the Indian Territory and surrounding regions. Lenape people currently belong to the Delaware Nation and Delaware Tribe of Indians in Oklahoma, the Stockbridge–Munsee Community in Wisconsin, and the Munsee-Delaware Nation, Moravian of the Thames First Nation, and Delaware of Six Nations in Ontario. Page: University of Delaware Summary: The University of Delaware (colloquially known as UD or Delaware) is a privately governed, state-assisted land-grant research university located in Newark, Delaware. UD is the largest university in Delaware. It offers three associate's programs, 148 bachelor's programs, 121 master's programs (with 13 joint degrees), and 55 doctoral programs across its eight colleges. The main campus is in Newark, with satellite campuses in Dover, Wilmington, Lewes, and Georgetown. It is considered a large institution with approximately 18,200 undergraduate and 4,200 graduate students. It is a privately governed university which receives public funding for being a land-grant, sea-grant, and space-grant state-supported research institution.UD is classified among \"R1: Doctoral Universities – Very high research activity\". According to the National Science Foundation, UD spent $186 million on research and development in 2018, ranking it 119th in the nation.  It is recognized with the Community Engagement Classification by the Carnegie Foundation for the Advancement of Teaching.UD students, alumni, and sports teams are known as the \"Fightin' Blue Hens\", more commonly shortened to \"Blue Hens\", and the school colors are Delaware blue and gold. UD sponsors 21 men's and women's NCAA Division-I sports teams and have competed in the Colonial Athletic Association (CAA) since 2001. Invoking: `Wikipedia` with `Delaware Blue Hen` Page: Delaware Blue Hen Summary: The Delaware Blue Hen or Blue Hen of Delaware is a blue strain of American gamecock. Under the name Blue Hen Chicken it is the official bird of the State of Delaware. It is the emblem or mascot of several institutions in the state, among them the sports teams of the University of Delaware. Page: Delaware Fightin' Blue Hens football Summary: The Delaware Fightin' Blue Hens football team represents the University of Delaware (UD) in National Collegiate Athletic Association (NCAA) Division I Football Championship Subdivision (FCS) college football as a member of CAA Football, the technically separate football arm of UD's full-time home of the Coastal Athletic Association. The team is currently led by head coach Ryan Carty  and plays on Tubby Raymond Field at 22,000-seat Delaware Stadium located in Newark, Delaware. The Fightin' Blue Hens have won six national titles in their 117-year history – 1946 (AP College Division), 1963 (UPI College Division), 1971 (AP/UPI College Division), 1972 (AP/UPI College Division), 1979 (Division II), and 2003 (Division I-AA). They returned to the FCS National Championship game in 2007 and 2010. The program has produced NFL quarterbacks Rich Gannon, Joe Flacco, Jeff Komlo, Pat Devlin and Scott Brunner. The Blue Hens are recognized as a perennial power in FCS football and Delaware was the only FCS program to average more than 20,000 fans per regular-season home game for each season from 1999 to 2010. Page: Delaware Fightin' Blue Hens Summary: The Delaware Fightin' Blue Hens are the athletic teams of the University of Delaware of Newark, Delaware, in the United States. The Blue Hens compete in the Football Championship Subdivision (FCS) of Division I of the National Collegiate Athletic Association (NCAA) as members of the Coastal Athletic Association. Page: Delaware Fightin' Blue Hens men's basketball Summary: The Delaware Fightin' Blue Hens men's basketball team is the basketball team that represents University of Delaware in Newark, Delaware. The school's team currently competes in the National Collegiate Athletic Association (NCAA) at the Division I level as a member of the Colonial Athletic Association since 2001. Home games are played at the Acierno Arena at the Bob Carpenter Center. The Blue Hens are coached by Martin Ingelsby who has been the head coach since 2016. Page: University of Delaware Summary: The University of Delaware (colloquially known as UD or Delaware) is a privately governed, state-assisted land-grant research university located in Newark, Delaware. UD is the largest university in Delaware. It offers three associate's programs, 148 bachelor's programs, 121 master's programs (with 13 joint degrees), and 55 doctoral programs across its eight colleges. The main campus is in Newark, with satellite campuses in Dover, Wilmington, Lewes, and Georgetown. It is considered a large institution with approximately 18,200 undergraduate and 4,200 graduate students. It is a privately governed university which receives public funding for being a land-grant, sea-grant, and space-grant state-supported research institution.UD is classified among \"R1: Doctoral Universities – Very high research activity\". According to the National Science Foundation, UD spent $186 million on research and development in 2018, ranking it 119th in the nation.  It is recognized with the Community Engagement Classification by the Carnegie Foundation for the Advancement of Teaching.UD students, alumni, and sports teams are known as the \"Fightin' Blue Hens\", more commonly shortened to \"Blue Hens\", and the school colors are Delaware blue and gold. UD sponsors 21 men's and women's NCAA Division-I sports teams and have competed in the Colonial Athletic Association (CAA) since 2001.The current US president is Joe Biden. His home state is Delaware. The state bird of Delaware is the Delaware Blue Hen. Its scientific name is Gallus gallus domesticus. > Finished chain. {'input': \"Who is the current US president? What's their home state? What's their home state's bird? What's that bird's scientific name?\", 'output': 'The current US president is Joe Biden. His home state is Delaware. The state bird of Delaware is the Delaware Blue Hen. Its scientific name is Gallus gallus domesticus.'} LangSmith\ntrace Previous Adding moderation Next Using tools"
    },
    {
        "title": "Using tools | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/expression_language/cookbook/tools",
        "text_content": "LangChain Expression Language Cookbook Using tools Using tools You can use any Tools with Runnables easily. !pip install duckduckgo - search from langchain . chat_models import ChatOpenAI from langchain . prompts import ChatPromptTemplate from langchain . tools import DuckDuckGoSearchRun from langchain_core . output_parsers import StrOutputParser search = DuckDuckGoSearchRun ( ) template = \"\"\"turn the following user input into a search query for a search engine: {input}\"\"\" prompt = ChatPromptTemplate . from_template ( template ) model = ChatOpenAI ( ) chain = prompt | model | StrOutputParser ( ) | search chain . invoke ( { \"input\" : \"I'd like to figure out what games are tonight\" } ) 'What sports games are on TV today & tonight? Watch and stream live sports on TV today, tonight, tomorrow. Today\\'s 2023 sports TV schedule includes football, basketball, baseball, hockey, motorsports, soccer and more. Watch on TV or stream online on ESPN, FOX, FS1, CBS, NBC, ABC, Peacock, Paramount+, fuboTV, local channels and many other networks. MLB Games Tonight: How to Watch on TV, Streaming & Odds - Thursday, September 7. Seattle Mariners\\' Julio Rodriguez greets teammates in the dugout after scoring against the Oakland Athletics in a ... Circle - Country Music and Lifestyle. Live coverage of all the MLB action today is available to you, with the information provided below. The Brewers will look to pick up a road win at PNC Park against the Pirates on Wednesday at 12:35 PM ET. Check out the latest odds and with BetMGM Sportsbook. Use bonus code \"GNPLAY\" for special offers! MLB Games Tonight: How to Watch on TV, Streaming & Odds - Tuesday, September 5. Houston Astros\\' Kyle Tucker runs after hitting a double during the fourth inning of a baseball game against the Los Angeles Angels, Sunday, Aug. 13, 2023, in Houston. (AP Photo/Eric Christian Smith) (APMedia) The Houston Astros versus the Texas Rangers is one of ... The second half of tonight\\'s college football schedule still has some good games remaining to watch on your television.. We\\'ve already seen an exciting one when Colorado upset TCU. And we saw some ...' Previous Managing prompt size Next LangChain Expression Language (LCEL)"
    },
    {
        "title": "Memory | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/memory/",
        "text_content": "Modules More Memory On this page Memory Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation.\nAt bare minimum, a conversational system should be able to access some window of past messages directly.\nA more complex system will need to have a world model that it is constantly updating, which allows it to do things like maintain information about entities and their relationships. We call this ability to store information about past interactions \"memory\".\nLangChain provides a lot of utilities for adding memory to a system.\nThese utilities can be used by themselves or incorporated seamlessly into a chain. A memory system needs to support two basic actions: reading and writing.\nRecall that every chain defines some core execution logic that expects certain inputs.\nSome of these inputs come directly from the user, but some of these inputs can come from memory.\nA chain will interact with its memory system twice in a given run. AFTER receiving the initial user inputs but BEFORE executing the core logic, a chain will READ from its memory system and augment the user inputs. AFTER executing the core logic but BEFORE returning the answer, a chain will WRITE the inputs and outputs of the current run to memory, so that they can be referred to in future runs. Building memory into a system ​ The two core design decisions in any memory system are: How state is stored How state is queried Storing: List of chat messages ​ Underlying any memory is a history of all chat interactions.\nEven if these are not all used directly, they need to be stored in some form.\nOne of the key parts of the LangChain memory module is a series of integrations for storing these chat messages,\nfrom in-memory lists to persistent databases. Chat message storage : How to work with Chat Messages, and the various integrations offered. Querying: Data structures and algorithms on top of chat messages ​ Keeping a list of chat messages is fairly straight-forward.\nWhat is less straight-forward are the data structures and algorithms built on top of chat messages that serve a view of those messages that is most useful. A very simply memory system might just return the most recent messages each run. A slightly more complex memory system might return a succinct summary of the past K messages.\nAn even more sophisticated system might extract entities from stored messages and only return information about entities referenced in the current run. Each application can have different requirements for how memory is queried. The memory module should make it easy to both get started with simple memory systems and write your own custom systems if needed. Memory types : The various data structures and algorithms that make up the memory types LangChain supports Get started ​ Let's take a look at what Memory actually looks like in LangChain.\nHere we'll cover the basics of interacting with an arbitrary memory class. Let's take a look at how to use ConversationBufferMemory in chains. ConversationBufferMemory is an extremely simple form of memory that just keeps a list of chat messages in a buffer\nand passes those into the prompt template. from langchain . memory import ConversationBufferMemory memory = ConversationBufferMemory ( ) memory . chat_memory . add_user_message ( \"hi!\" ) memory . chat_memory . add_ai_message ( \"what's up?\" ) When using memory in a chain, there are a few key concepts to understand.\nNote that here we cover general concepts that are useful for most types of memory.\nEach individual memory type may very well have its own parameters and concepts that are necessary to understand. What variables get returned from memory ​ Before going into the chain, various variables are read from memory.\nThese have specific names which need to align with the variables the chain expects.\nYou can see what these variables are by calling memory.load_memory_variables({}) .\nNote that the empty dictionary that we pass in is just a placeholder for real variables.\nIf the memory type you are using is dependent upon the input variables, you may need to pass some in. memory . load_memory_variables ( { } ) {'history': \"Human: hi!\\nAI: what's up?\"} In this case, you can see that load_memory_variables returns a single key, history .\nThis means that your chain (and likely your prompt) should expect an input named history .\nYou can usually control this variable through parameters on the memory class.\nFor example, if you want the memory variables to be returned in the key chat_history you can do: memory = ConversationBufferMemory ( memory_key = \"chat_history\" ) memory . chat_memory . add_user_message ( \"hi!\" ) memory . chat_memory . add_ai_message ( \"what's up?\" ) {'chat_history': \"Human: hi!\\nAI: what's up?\"} The parameter name to control these keys may vary per memory type, but it's important to understand that (1) this is controllable, and (2) how to control it. Whether memory is a string or a list of messages ​ One of the most common types of memory involves returning a list of chat messages.\nThese can either be returned as a single string, all concatenated together (useful when they will be passed into LLMs)\nor a list of ChatMessages (useful when passed into ChatModels). By default, they are returned as a single string.\nIn order to return as a list of messages, you can set return_messages=True memory = ConversationBufferMemory ( return_messages = True ) memory . chat_memory . add_user_message ( \"hi!\" ) memory . chat_memory . add_ai_message ( \"what's up?\" ) {'history': [HumanMessage(content='hi!', additional_kwargs={}, example=False), AIMessage(content='what's up?', additional_kwargs={}, example=False)]} What keys are saved to memory ​ Often times chains take in or return multiple input/output keys.\nIn these cases, how can we know which keys we want to save to the chat message history?\nThis is generally controllable by input_key and output_key parameters on the memory types.\nThese default to None - and if there is only one input/output key it is known to just use that.\nHowever, if there are multiple input/output keys then you MUST specify the name of which one to use. End to end example ​ Finally, let's take a look at using this in a chain.\nWe'll use an LLMChain , and show working with both an LLM and a ChatModel. Using an LLM ​ from langchain . llms import OpenAI from langchain . prompts import PromptTemplate from langchain . chains import LLMChain from langchain . memory import ConversationBufferMemory llm = OpenAI ( temperature = 0 ) # Notice that \"chat_history\" is present in the prompt template template = \"\"\"You are a nice chatbot having a conversation with a human. Previous conversation: {chat_history} New human question: {question} Response:\"\"\" prompt = PromptTemplate . from_template ( template ) # Notice that we need to align the `memory_key` memory = ConversationBufferMemory ( memory_key = \"chat_history\" ) conversation = LLMChain ( llm = llm , prompt = prompt , verbose = True , memory = memory ) # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation ( { \"question\" : \"hi\" } ) Using a ChatModel ​ from langchain . chat_models import ChatOpenAI from langchain . prompts import ( ChatPromptTemplate , MessagesPlaceholder , SystemMessagePromptTemplate , HumanMessagePromptTemplate , ) from langchain . chains import LLMChain from langchain . memory import ConversationBufferMemory llm = ChatOpenAI ( ) prompt = ChatPromptTemplate ( messages = [ SystemMessagePromptTemplate . from_template ( \"You are a nice chatbot having a conversation with a human.\" ) , # The `variable_name` here is what must align with memory MessagesPlaceholder ( variable_name = \"chat_history\" ) , HumanMessagePromptTemplate . from_template ( \"{question}\" ) ] ) # Notice that we `return_messages=True` to fit into the MessagesPlaceholder # Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name. memory = ConversationBufferMemory ( memory_key = \"chat_history\" , return_messages = True ) conversation = LLMChain ( llm = llm , prompt = prompt , verbose = True , memory = memory ) # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation ( { \"question\" : \"hi\" } ) Next steps ​ And that's it for getting started!\nPlease see the other sections for walkthroughs of more advanced topics,\nlike custom memory, multiple memories, and more. Previous Chains Next Chat Messages"
    },
    {
        "title": "Prompts | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/model_io/prompts/",
        "text_content": "Modules Model I/O Prompts Prompts A prompt for a language model is a set of instructions or input provided by a user to\nguide the model's response, helping it understand the context and generate relevant\nand coherent language-based output, such as answering questions, completing sentences,\nor engaging in a conversation. LangChain provides several classes and functions to help construct and work with prompts. Prompt templates : Parametrized model inputs Example selectors : Dynamically select examples to include in prompts Previous Model I/O Next Prompt templates"
    },
    {
        "title": "Chat models | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/model_io/chat/",
        "text_content": "Modules Model I/O Chat models On this page Chat models Head to Integrations for documentation on\nbuilt-in integrations with chat model providers. Chat models are a variation on language models. While chat models use\nlanguage models under the hood, the interface they use is a bit\ndifferent. Rather than using a “text in, text out” API, they use an\ninterface where “chat messages” are the inputs and outputs. Setup ​ For this example we’ll need to install the OpenAI Python package: pip install openai Accessing the API requires an API key, which you can get by creating an\naccount and heading here . Once we have a key\nwe’ll want to set it as an environment variable by running: export OPENAI_API_KEY = \"...\" If you’d prefer not to set an environment variable you can pass the key\nin directly via the openai_api_key named parameter when initiating the\nOpenAI LLM class: from langchain . chat_models import ChatOpenAI chat = ChatOpenAI ( openai_api_key = \"...\" ) Otherwise you can initialize without any params: from langchain . chat_models import ChatOpenAI chat = ChatOpenAI ( ) Messages ​ The chat model interface is based around messages rather than raw text.\nThe types of messages currently supported in LangChain are AIMessage , HumanMessage , SystemMessage , FunctionMessage and ChatMessage – ChatMessage takes in an arbitrary role parameter. Most of the time,\nyou’ll just be dealing with HumanMessage , AIMessage , and SystemMessage LCEL ​ Chat models implement the Runnable\ninterface , the basic building\nblock of the LangChain Expression Language\n(LCEL) . This means they support invoke , ainvoke , stream , astream , batch , abatch , astream_log calls. Chat models accept List[BaseMessage] as inputs, or objects which can\nbe coerced to messages, including str (converted to HumanMessage )\nand PromptValue . from langchain_core . messages import HumanMessage , SystemMessage messages = [ SystemMessage ( content = \"You're a helpful assistant\" ) , HumanMessage ( content = \"What is the purpose of model regularization?\" ) , ] chat . invoke ( messages ) AIMessage(content=\"The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and starts to fit the noise in the training data, leading to poor generalization on unseen data. Regularization techniques introduce additional constraints or penalties to the model's objective function, discouraging it from becoming overly complex and promoting simpler and more generalizable models. Regularization helps to strike a balance between fitting the training data well and avoiding overfitting, leading to better performance on new, unseen data.\") for chunk in chat . stream ( messages ) : print ( chunk . content , end = \"\" , flush = True ) The purpose of model regularization is to prevent overfitting and improve the generalization of a machine learning model. Overfitting occurs when a model is too complex and learns the noise or random variations in the training data, which leads to poor performance on new, unseen data. Regularization techniques introduce additional constraints or penalties to the model's learning process, discouraging it from fitting the noise and reducing the complexity of the model. This helps to improve the model's ability to generalize well and make accurate predictions on unseen data. chat . batch ( [ messages ] ) [AIMessage(content=\"The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and starts to learn the noise or random fluctuations in the training data, rather than the underlying patterns or relationships. Regularization techniques add a penalty term to the model's objective function, which discourages the model from becoming too complex and helps it generalize better to new, unseen data. This improves the model's ability to make accurate predictions on new data by reducing the variance and increasing the model's overall performance.\")] await chat . ainvoke ( messages ) AIMessage(content='The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and starts to memorize the training data instead of learning general patterns and relationships. This leads to poor performance on new, unseen data.\\n\\nRegularization techniques introduce additional constraints or penalties to the model during training, discouraging it from becoming overly complex. This helps to strike a balance between fitting the training data well and generalizing to new data. Regularization techniques can include adding a penalty term to the loss function, such as L1 or L2 regularization, or using techniques like dropout or early stopping. By regularizing the model, it encourages it to learn the most relevant features and reduce the impact of noise or outliers in the data.') async for chunk in chat . astream ( messages ) : print ( chunk . content , end = \"\" , flush = True ) The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and starts to memorize the training data instead of learning the underlying patterns. Regularization techniques help in reducing the complexity of the model by adding a penalty to the loss function. This penalty encourages the model to have smaller weights or fewer features, making it more generalized and less prone to overfitting. The goal is to find the right balance between fitting the training data well and being able to generalize well to unseen data. async for chunk in chat . astream_log ( messages ) : print ( chunk ) RunLogPatch({'op': 'replace', 'path': '', 'value': {'final_output': None, 'id': '754c4143-2348-46c4-ad2b-3095913084c6', 'logs': {}, 'streamed_output': []}}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='The')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' purpose')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' of')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' model')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' regularization')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' is')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' to')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' prevent')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' a')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' machine')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' learning')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' model')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' from')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' over')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='fit')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='ting')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' the')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' training')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' data')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' and')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' improve')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' its')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' general')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='ization')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' ability')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='.')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' Over')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='fit')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='ting')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' occurs')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' when')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' a')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' model')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' becomes')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' too')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' complex')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' and')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' learns')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' to')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' fit')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' the')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' noise')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' or')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' random')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' fluctuations')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' in')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' the')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' training')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' data')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=',')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' instead')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' of')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' capturing')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' the')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' underlying')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' patterns')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' and')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' relationships')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='.')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' Regular')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='ization')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' techniques')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' introduce')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' a')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' penalty')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' term')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' to')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' the')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' model')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=\"'s\")}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' objective')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' function')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=',')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' which')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' discour')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='ages')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' the')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' model')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' from')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' becoming')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' too')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' complex')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='.')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' This')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' helps')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' to')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' control')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' the')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' model')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=\"'s\")}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' complexity')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' and')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' reduces')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' the')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' risk')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' of')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' over')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='fit')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='ting')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=',')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' leading')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' to')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' better')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' performance')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' on')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' unseen')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' data')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='.')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='')}) RunLogPatch({'op': 'replace', 'path': '/final_output', 'value': {'generations': [[{'generation_info': {'finish_reason': 'stop'}, 'message': AIMessageChunk(content=\"The purpose of model regularization is to prevent a machine learning model from overfitting the training data and improve its generalization ability. Overfitting occurs when a model becomes too complex and learns to fit the noise or random fluctuations in the training data, instead of capturing the underlying patterns and relationships. Regularization techniques introduce a penalty term to the model's objective function, which discourages the model from becoming too complex. This helps to control the model's complexity and reduces the risk of overfitting, leading to better performance on unseen data.\"), 'text': 'The purpose of model regularization is ' 'to prevent a machine learning model ' 'from overfitting the training data and ' 'improve its generalization ability. ' 'Overfitting occurs when a model becomes ' 'too complex and learns to fit the noise ' 'or random fluctuations in the training ' 'data, instead of capturing the ' 'underlying patterns and relationships. ' 'Regularization techniques introduce a ' \"penalty term to the model's objective \" 'function, which discourages the model ' 'from becoming too complex. This helps ' \"to control the model's complexity and \" 'reduces the risk of overfitting, ' 'leading to better performance on unseen ' 'data.'}]], 'llm_output': None, 'run': None}}) LangSmith ​ All ChatModel s come with built-in LangSmith tracing. Just set the\nfollowing environment variables: export LANGCHAIN_TRACING_V2 = \"true\" export LANGCHAIN_API_KEY = < your-api-key > and any ChatModel invocation (whether it’s nested in a chain or not)\nwill automatically be traced. A trace will include inputs, outputs,\nlatency, token usage, invocation params, environment params, and more.\nSee an example here: https://smith.langchain.com/public/a54192ae-dd5c-4f7a-88d1-daa1eaba1af7/r . In LangSmith you can then provide feedback for any trace, compile\nannotated datasets for evals, debug performance in the playground, and\nmore. [ Legacy ] __call__ {#legacy- call } ​ Messages in - > message out ​ For convenience you can also treat chat models as callables. You can get\nchat completions by passing one or more messages to the chat model. The\nresponse will be a message. from langchain . schema import HumanMessage , SystemMessage chat ( [ HumanMessage ( content = \"Translate this sentence from English to French: I love programming.\" ) ] ) AIMessage(content=\"J'adore la programmation.\") OpenAI’s chat model supports multiple messages as input. See here for more information. Here is an example of sending a system and user\nmessage to the chat model: messages = [ SystemMessage ( content = \"You are a helpful assistant that translates English to French.\" ) , HumanMessage ( content = \"I love programming.\" ) , ] chat ( messages ) AIMessage(content=\"J'adore la programmation.\") [ Legacy ] generate ​ Batch calls, richer outputs ​ You can go one step further and generate completions for multiple sets\nof messages using generate . This returns an LLMResult with an\nadditional message parameter. This will include additional information\nabout each generation beyond the returned message (e.g. the finish\nreason) and additional information about the full API call (e.g. total\ntokens used). batch_messages = [ [ SystemMessage ( content = \"You are a helpful assistant that translates English to French.\" ) , HumanMessage ( content = \"I love programming.\" ) , ] , [ SystemMessage ( content = \"You are a helpful assistant that translates English to French.\" ) , HumanMessage ( content = \"I love artificial intelligence.\" ) , ] , ] result = chat . generate ( batch_messages ) result LLMResult(generations=[[ChatGeneration(text=\"J'adore programmer.\", generation_info={'finish_reason': 'stop'}, message=AIMessage(content=\"J'adore programmer.\"))], [ChatGeneration(text=\"J'adore l'intelligence artificielle.\", generation_info={'finish_reason': 'stop'}, message=AIMessage(content=\"J'adore l'intelligence artificielle.\"))]], llm_output={'token_usage': {'prompt_tokens': 53, 'completion_tokens': 18, 'total_tokens': 71}, 'model_name': 'gpt-3.5-turbo'}, run=[RunInfo(run_id=UUID('077917a9-026c-47c4-b308-77b37c3a3bfa')), RunInfo(run_id=UUID('0a70a0bf-c599-4f51-932a-c7d42202c984'))]) You can recover things like token usage from this LLMResult: result . llm_output {'token_usage': {'prompt_tokens': 53, 'completion_tokens': 18, 'total_tokens': 71}, 'model_name': 'gpt-3.5-turbo'} Previous Select by similarity Next Caching"
    },
    {
        "title": "LLMs | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/model_io/llms/",
        "text_content": "Modules Model I/O LLMs On this page LLMs Head to Integrations for documentation on\nbuilt-in integrations with LLM providers. Large Language Models (LLMs) are a core component of LangChain.\nLangChain does not serve its own LLMs, but rather provides a standard\ninterface for interacting with many different LLMs. There are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) -\nthe LLM class is designed to provide a standard interface for all of\nthem. In this walkthrough we’ll work with an OpenAI LLM wrapper, although the\nfunctionalities highlighted are generic for all LLM types. Setup ​ For this example we’ll need to install the OpenAI Python package: pip install openai Accessing the API requires an API key, which you can get by creating an\naccount and heading here . Once we have a key\nwe’ll want to set it as an environment variable by running: export OPENAI_API_KEY = \"...\" If you’d prefer not to set an environment variable you can pass the key\nin directly via the openai_api_key named parameter when initiating the\nOpenAI LLM class: from langchain . llms import OpenAI llm = OpenAI ( openai_api_key = \"...\" ) otherwise you can initialize without any params: from langchain . llms import OpenAI llm = OpenAI ( ) LCEL ​ LLMs implement the Runnable\ninterface , the basic building\nblock of the LangChain Expression Language\n(LCEL) . This means they support invoke , ainvoke , stream , astream , batch , abatch , astream_log calls. LLMs accept strings as inputs, or objects which can be coerced to\nstring prompts, including List[BaseMessage] and PromptValue . llm . invoke ( \"What are some theories about the relationship between unemployment and inflation?\" ) '\\n\\n1. The Phillips Curve Theory: This suggests that there is an inverse relationship between unemployment and inflation, meaning that when unemployment is low, inflation will be higher, and when unemployment is high, inflation will be lower.\\n\\n2. The Monetarist Theory: This theory suggests that the relationship between unemployment and inflation is weak, and that changes in the money supply are more important in determining inflation.\\n\\n3. The Resource Utilization Theory: This suggests that when unemployment is low, firms are able to raise wages and prices in order to take advantage of the increased demand for their products and services. This leads to higher inflation.' for chunk in llm . stream ( \"What are some theories about the relationship between unemployment and inflation?\" ) : print ( chunk , end = \"\" , flush = True ) 1. The Phillips Curve Theory: This theory states that there is an inverse relationship between unemployment and inflation. As unemployment decreases, inflation increases and vice versa. 2. The Cost-Push Inflation Theory: This theory suggests that an increase in unemployment leads to a decrease in aggregate demand, which causes prices to go up due to a decrease in supply. 3. The Wage-Push Inflation Theory: This theory states that when unemployment is low, wages tend to increase due to competition for labor, which causes prices to rise. 4. The Monetarist Theory: This theory states that there is no direct relationship between unemployment and inflation, but rather, an increase in the money supply leads to inflation, which can be caused by an increase in unemployment. llm . batch ( [ \"What are some theories about the relationship between unemployment and inflation?\" ] ) ['\\n\\n1. The Phillips Curve Theory: This theory suggests that there is an inverse relationship between unemployment and inflation, meaning that when unemployment decreases, inflation rises, and when unemployment increases, inflation decreases. This theory is based on the idea that when the economy is doing well, there is more demand for goods and services, causing prices to increase.\\n\\n2. The Cost-Push Theory: This theory suggests that when the cost of production increases, it leads to higher prices and lower output. This can lead to higher unemployment and eventually higher inflation.\\n\\n3. The Demand-Pull Theory: This theory suggests that when demand for goods and services increases, it leads to higher prices and eventually higher inflation. This can lead to higher unemployment as businesses cannot keep up with the higher demand.\\n\\n4. The Structural Unemployment Theory: This theory suggests that when there is a mismatch between the skills of the unemployed and the skills required in the job market, it leads to higher unemployment and eventually higher inflation.'] await llm . ainvoke ( \"What are some theories about the relationship between unemployment and inflation?\" ) '\\n\\n1. Phillips Curve Theory: This theory states that there is an inverse relationship between inflation and unemployment. As unemployment decreases, inflation increases, and vice versa.\\n\\n2. Cost-Push Theory: This theory suggests that inflation is caused by rising costs, which can be caused by an increase in unemployment. As unemployment rises, businesses are unable to keep up with demand and have to raise prices to compensate.\\n\\n3. Demand-Pull Theory: This theory suggests that inflation occurs when demand exceeds supply. As unemployment increases, demand for goods and services decreases, leading to a decrease in inflation.\\n\\n4. Monetary Theory: This theory suggests that the money supply and inflation are related to unemployment. When the money supply increases, prices increase, leading to an increase in inflation. If unemployment is high, then the money supply increases, leading to an increase in inflation.' async for chunk in llm . astream ( \"What are some theories about the relationship between unemployment and inflation?\" ) : print ( chunk , end = \"\" , flush = True ) 1. Phillips Curve Theory: This theory suggests that there is an inverse relationship between unemployment and inflation, meaning that when unemployment is low, inflation rises and vice versa. 2. Cost-Push Theory: This theory suggests that inflation is caused by rising costs of production, such as wages, raw materials, and energy. It states that when costs increase, firms must pass these costs onto the consumer, thus raising the price of goods and services and leading to inflation. 3. Demand-Pull Theory: This theory suggests that inflation is caused by an increase in demand for goods and services, leading to a rise in prices. It suggests that when unemployment is low, people have more money to spend and this increased demand pushes up prices. 4. Monetarist Theory: This theory states that inflation is caused by an increase in the money supply. It suggests that when the money supply increases, people have more money to spend, leading to higher prices. await llm . abatch ( [ \"What are some theories about the relationship between unemployment and inflation?\" ] ) ['\\n\\n1. The Phillips Curve Theory: This theory states that there is an inverse relationship between unemployment and inflation. When unemployment is low, wages increase, leading to higher prices and overall inflation.\\n\\n2. The Cost-Push Theory: This theory states that inflation is caused by increases in the costs of production, such as wages, goods, and services. When the cost of production increases, the prices of goods and services must also increase, leading to inflation.\\n\\n3. The Demand Pull Theory: This theory states that inflation is caused by an increase in aggregate demand for goods and services. When the demand is high, prices must increase in order to meet the demand. This leads to inflation.\\n\\n4. The Structural Unemployment Theory: This theory states that when unemployment is high, there is an excess supply of labor. This excess supply of labor can result in lower wages, which can cause inflation as people are willing to accept lower wages for the same amount of work.'] async for chunk in llm . astream_log ( \"What are some theories about the relationship between unemployment and inflation?\" ) : print ( chunk ) RunLogPatch({'op': 'replace', 'path': '', 'value': {'final_output': None, 'id': 'baf410ad-618e-44db-93c8-809da4e3ed44', 'logs': {}, 'streamed_output': []}}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '1'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' The'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Phillips'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Curve'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ':'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' This'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' theory'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' suggests'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' that'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' there'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' is'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' an'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' inverse'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' relationship'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' between'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' unemployment and'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' inflation'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' When'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' unemployment'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' is'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' low'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' inflation'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' tends'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' be'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' high'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' when'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' unemployment'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' is'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' high'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' inflation'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' tends'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' be'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' low'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' '}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '2'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' The'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' NA'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'IR'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'U'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Theory'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ':'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' This'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' theory'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' suggests'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' that there is'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' a'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' natural'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' rate'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' of'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' unemployment'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' also'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' known'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' as'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Non'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Ac'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'celer'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ating'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' In'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'flation'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Rate'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' of'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Unemployment'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' ('}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'NA'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'IR'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'U'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ').'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' According'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' this'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' theory'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' when'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' unemployment'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' is'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' below'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' NA'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'IR'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'U'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' then'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' inflation'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' will'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' increase'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' when'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' unemployment'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' is'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' above'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' NA'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'IR'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'U'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' then'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' inflation'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' will'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' decrease'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '3'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' The'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Cost'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Push'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' In'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'flation'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Theory'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ':'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' This'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' theory'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' suggests'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' that high unemployment'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' leads'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' higher'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' wages'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' which'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' in'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' turn'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' leads'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' higher'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' prices'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and higher inflation'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''}) RunLogPatch({'op': 'replace', 'path': '/final_output', 'value': {'generations': [[{'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'text': '\\n' '\\n' '1. The Phillips Curve: This theory ' 'suggests that there is an inverse ' 'relationship between unemployment and ' 'inflation. When unemployment is low, ' 'inflation tends to be high, and when ' 'unemployment is high, inflation tends ' 'to be low. \\n' '\\n' '2. The NAIRU Theory: This theory ' 'suggests that there is a natural rate ' 'of unemployment, also known as the ' 'Non-Accelerating Inflation Rate of ' 'Unemployment (NAIRU). According to this ' 'theory, when unemployment is below the ' 'NAIRU, then inflation will increase, ' 'and when unemployment is above the ' 'NAIRU, then inflation will decrease.\\n' '\\n' '3. The Cost-Push Inflation Theory: This ' 'theory suggests that high unemployment ' 'leads to higher wages, which in turn ' 'leads to higher prices and higher ' 'inflation.'}]], 'llm_output': None, 'run': None}}) LangSmith ​ All LLM s come with built-in LangSmith tracing. Just set the following\nenvironment variables: export LANGCHAIN_TRACING_V2 = \"true\" export LANGCHAIN_API_KEY = < your-api-key > and any LLM invocation (whether it’s nested in a chain or not) will\nautomatically be traced. A trace will include inputs, outputs, latency,\ntoken usage, invocation params, environment params, and more. See an\nexample here: https://smith.langchain.com/public/7924621a-ff58-4b1c-a2a2-035a354ef434/r . In LangSmith you can then provide feedback for any trace, compile\nannotated datasets for evals, debug performance in the playground, and\nmore. [ Legacy ] __call__ : string in - > string out {#legacy- call -string-in---string-out} ​ The simplest way to use an LLM is a callable: pass in a string, get a\nstring completion. llm ( \"Tell me a joke\" ) '\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!' [ Legacy ] generate : batch calls, richer outputs ​ generate lets you call the model with a list of strings, getting back\na more complete response than just the text. This complete response can\ninclude things like multiple top responses and other LLM\nprovider-specific information: llm_result = llm . generate ( [ \"Tell me a joke\" , \"Tell me a poem\" ] * 15 ) len ( llm_result . generations ) 30 llm_result . generations [ 0 ] [Generation(text=\"\\n\\nQ: Why don't scientists trust atoms?\\nA: Because they make up everything!\", generation_info={'finish_reason': 'stop', 'logprobs': None})] llm_result . generations [ - 1 ] [Generation(text='\\n\\nRoses are red,\\nViolets are blue,\\nSugar is sweet,\\nAnd so are you!', generation_info={'finish_reason': 'stop', 'logprobs': None})] You can also access provider specific information that is returned. This\ninformation is not standardized across providers. llm_result . llm_output {'token_usage': {'completion_tokens': 900, 'total_tokens': 1020, 'prompt_tokens': 120}, 'model_name': 'text-davinci-003'} Previous Tracking token usage Next Async API"
    },
    {
        "title": "Output parsers | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/model_io/output_parsers/",
        "text_content": "Modules Model I/O Output parsers On this page Output parsers Language models output text. But many times you may want to get more\nstructured information than just text back. This is where output parsers\ncome in. Output parsers are classes that help structure language model responses.\nThere are two main methods an output parser must implement: “Get format instructions”: A method which returns a string\ncontaining instructions for how the output of a language model\nshould be formatted. “Parse”: A method which takes in a string (assumed to be the\nresponse from a language model) and parses it into some structure. And then one optional one: “Parse with prompt”: A method which takes in a string (assumed to be\nthe response from a language model) and a prompt (assumed to be the\nprompt that generated such a response) and parses it into some\nstructure. The prompt is largely provided in the event the\nOutputParser wants to retry or fix the output in some way, and needs\ninformation from the prompt to do so. Get started ​ Below we go over the main type of output parser, the PydanticOutputParser . from langchain . llms import OpenAI from langchain . output_parsers import PydanticOutputParser from langchain . prompts import PromptTemplate from langchain_core . pydantic_v1 import BaseModel , Field , validator model = OpenAI ( model_name = \"gpt-3.5-turbo-instruct\" , temperature = 0.0 ) # Define your desired data structure. class Joke ( BaseModel ) : setup : str = Field ( description = \"question to set up a joke\" ) punchline : str = Field ( description = \"answer to resolve the joke\" ) # You can add custom validation logic easily with Pydantic. @validator ( \"setup\" ) def question_ends_with_question_mark ( cls , field ) : if field [ - 1 ] != \"?\" : raise ValueError ( \"Badly formed question!\" ) return field # Set up a parser + inject instructions into the prompt template. parser = PydanticOutputParser ( pydantic_object = Joke ) prompt = PromptTemplate ( template = \"Answer the user query.\\n{format_instructions}\\n{query}\\n\" , input_variables = [ \"query\" ] , partial_variables = { \"format_instructions\" : parser . get_format_instructions ( ) } , ) # And a query intended to prompt a language model to populate the data structure. prompt_and_model = prompt | model output = prompt_and_model . invoke ( { \"query\" : \"Tell me a joke.\" } ) parser . invoke ( output ) Joke(setup='Why did the chicken cross the road?', punchline='To get to the other side!') LCEL ​ Output parsers implement the Runnable\ninterface , the basic building\nblock of the LangChain Expression Language\n(LCEL) . This means they support invoke , ainvoke , stream , astream , batch , abatch , astream_log calls. Output parsers accept a string or BaseMessage as input and can return\nan arbitrary type. parser . invoke ( output ) Joke(setup='Why did the chicken cross the road?', punchline='To get to the other side!') Instead of manually invoking the parser, we also could’ve just added it\nto our Runnable sequence: chain = prompt | model | parser chain . invoke ( { \"query\" : \"Tell me a joke.\" } ) Joke(setup='Why did the chicken cross the road?', punchline='To get to the other side!') While all parsers support the streaming interface, only certain parsers\ncan stream through partially parsed objects, since this is highly\ndependent on the output type. Parsers which cannot construct partial\nobjects will simply yield the fully parsed output. The SimpleJsonOutputParser for example can stream through partial\noutputs: from langchain . output_parsers . json import SimpleJsonOutputParser json_prompt = PromptTemplate . from_template ( \"Return a JSON object with an `answer` key that answers the following question: {question}\" ) json_parser = SimpleJsonOutputParser ( ) json_chain = json_prompt | model | json_parser list ( json_chain . stream ( { \"question\" : \"Who invented the microscope?\" } ) ) [{}, {'answer': ''}, {'answer': 'Ant'}, {'answer': 'Anton'}, {'answer': 'Antonie'}, {'answer': 'Antonie van'}, {'answer': 'Antonie van Lee'}, {'answer': 'Antonie van Leeu'}, {'answer': 'Antonie van Leeuwen'}, {'answer': 'Antonie van Leeuwenho'}, {'answer': 'Antonie van Leeuwenhoek'}] While the PydanticOutputParser cannot: list ( chain . stream ( { \"query\" : \"Tell me a joke.\" } ) ) [Joke(setup='Why did the chicken cross the road?', punchline='To get to the other side!')] Previous Tracking token usage Next List parser"
    },
    {
        "title": "Document loaders | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/data_connection/document_loaders/",
        "text_content": "Modules Retrieval Document loaders On this page Document loaders info Head to Integrations for documentation on built-in document loader integrations with 3rd-party tools. Use document loaders to load data from a source as Document 's. A Document is a piece of text\nand associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text\ncontents of any web page, or even for loading a transcript of a YouTube video. Document loaders provide a \"load\" method for loading data as documents from a configured source. They optionally\nimplement a \"lazy load\" as well for lazily loading data into memory. Get started ​ The simplest loader reads in a file as text and places it all into one document. from langchain . document_loaders import TextLoader loader = TextLoader ( \"./index.md\" ) loader . load ( ) [ Document(page_content='---\\nsidebar_position: 0\\n---\\n# Document loaders\\n\\nUse document loaders to load data from a source as `Document`\\'s. A `Document` is a piece of text\\nand associated metadata. For example, there are document loaders for loading a simple `.txt` file, for loading the text\\ncontents of any web page, or even for loading a transcript of a YouTube video.\\n\\nEvery document loader exposes two methods:\\n1. \"Load\": load documents from the configured source\\n2. \"Load and split\": load documents from the configured source and split them using the passed in text splitter\\n\\nThey optionally implement:\\n\\n3. \"Lazy load\": load documents into memory lazily\\n', metadata={'source': '../docs/docs/modules/data_connection/document_loaders/index.md'}) ] Previous Retrieval Next CSV"
    },
    {
        "title": "Document transformers | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/data_connection/document_transformers/",
        "text_content": "Modules Retrieval Document transformers On this page Document transformers info Head to Integrations for documentation on built-in document transformer integrations with 3rd-party tools. Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example\nis you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain\nhas a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents. Text splitters ​ When you want to deal with long pieces of text, it is necessary to split up that text into chunks.\nAs simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text.\nThis notebook showcases several ways to do that. At a high level, text splitters work as following: Split the text up into small, semantically meaningful chunks (often sentences). Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function). Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks). That means there are two different axes along which you can customize your text splitter: How the text is split How the chunk size is measured Get started with text splitters ​ The default recommended text splitter is the RecursiveCharacterTextSplitter. This text splitter takes a list of characters. It tries to create chunks based on splitting on the first character, but if any chunks are too large it then moves onto the next character, and so forth. By default the characters it tries to split on are [\"\\n\\n\", \"\\n\", \" \", \"\"] In addition to controlling which characters you can split on, you can also control a few other things: length_function : how the length of chunks is calculated. Defaults to just counting number of characters, but it's pretty common to pass a token counter here. chunk_size : the maximum size of your chunks (as measured by the length function). chunk_overlap : the maximum overlap between chunks. It can be nice to have some overlap to maintain some continuity between chunks (e.g. do a sliding window). add_start_index : whether to include the starting position of each chunk within the original document in the metadata. # This is a long document we can split up. with open ( '../../state_of_the_union.txt' ) as f : state_of_the_union = f . read ( ) from langchain . text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter ( # Set a really small chunk size, just to show. chunk_size = 100 , chunk_overlap = 20 , length_function = len , add_start_index = True , ) texts = text_splitter . create_documents ( [ state_of_the_union ] ) print ( texts [ 0 ] ) print ( texts [ 1 ] ) page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and' metadata={'start_index': 0} page_content='of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.' metadata={'start_index': 82} Evaluate text splitters ​ You can evaluate text splitters with the Chunkviz utility created by Greg Kamradt . Chunkviz is a great tool for visualizing how your text splitter is working. It will show you how your text is\nbeing split up and help in tuning up the splitting parameters. Other transformations: ​ Filter redundant docs, translate docs, extract metadata, and more ​ We can do perform a number of transformations on docs which are not simply splitting the text. With the EmbeddingsRedundantFilter we can identify similar documents and filter out redundancies. With integrations like doctran we can do things like translate documents from one language\nto another, extract desired properties and add them to metadata, and convert conversational dialogue into a Q/A format\nset of documents. Previous PDF Next HTMLHeaderTextSplitter"
    },
    {
        "title": "Text embedding models | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/data_connection/text_embedding/",
        "text_content": "Modules Retrieval Text embedding models On this page Text embedding models info Head to Integrations for documentation on built-in integrations with text embedding model providers. The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them. Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space. The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself). Get started ​ Setup ​ To start we'll need to install the OpenAI Python package: pip install openai Accessing the API requires an API key, which you can get by creating an account and heading here . Once we have a key we'll want to set it as an environment variable by running: export OPENAI_API_KEY = \"...\" If you'd prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class: from langchain . embeddings import OpenAIEmbeddings embeddings_model = OpenAIEmbeddings ( openai_api_key = \"...\" ) Otherwise you can initialize without any params: from langchain . embeddings import OpenAIEmbeddings embeddings_model = OpenAIEmbeddings ( ) embed_documents ​ Embed list of texts ​ embeddings = embeddings_model . embed_documents ( [ \"Hi there!\" , \"Oh, hello!\" , \"What's your name?\" , \"My friends call me World\" , \"Hello World!\" ] ) len ( embeddings ) , len ( embeddings [ 0 ] ) (5, 1536) embed_query ​ Embed single query ​ Embed a single piece of text for the purpose of comparing to other embedded pieces of texts. embedded_query = embeddings_model . embed_query ( \"What was the name mentioned in the conversation?\" ) embedded_query [ : 5 ] [0.0053587136790156364, -0.0004999046213924885, 0.038883671164512634, -0.003001077566295862, -0.00900818221271038] Previous Retrieval Next CacheBackedEmbeddings"
    },
    {
        "title": "Vector stores | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/data_connection/vectorstores/",
        "text_content": "Modules Retrieval Vector stores On this page Vector stores info Head to Integrations for documentation on built-in integrations with 3rd-party vector stores. One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding\nvectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are\n'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search\nfor you. Get started ​ This walkthrough showcases basic functionality related to vector stores. A key part of working with vector stores is creating the vector to put in them, which is usually created via embeddings. Therefore, it is recommended that you familiarize yourself with the text embedding model interfaces before diving into this. There are many great vector store options, here are a few that are free, open-source, and run entirely on your local machine. Review all integrations for many great hosted offerings. Chroma FAISS Lance This walkthrough uses the chroma vector database, which runs on your local machine as a library. pip install chromadb We want to use OpenAIEmbeddings so we have to get the OpenAI API Key. import os import getpass os . environ [ 'OPENAI_API_KEY' ] = getpass . getpass ( 'OpenAI API Key:' ) from langchain . document_loaders import TextLoader from langchain . embeddings . openai import OpenAIEmbeddings from langchain . text_splitter import CharacterTextSplitter from langchain . vectorstores import Chroma # Load the document, split it into chunks, embed each chunk and load it into the vector store. raw_documents = TextLoader ( '../../../state_of_the_union.txt' ) . load ( ) text_splitter = CharacterTextSplitter ( chunk_size = 1000 , chunk_overlap = 0 ) documents = text_splitter . split_documents ( raw_documents ) db = Chroma . from_documents ( documents , OpenAIEmbeddings ( ) ) This walkthrough uses the FAISS vector database, which makes use of the Facebook AI Similarity Search (FAISS) library. pip install faiss-cpu We want to use OpenAIEmbeddings so we have to get the OpenAI API Key. import os import getpass os . environ [ 'OPENAI_API_KEY' ] = getpass . getpass ( 'OpenAI API Key:' ) from langchain . document_loaders import TextLoader from langchain . embeddings . openai import OpenAIEmbeddings from langchain . text_splitter import CharacterTextSplitter from langchain . vectorstores import FAISS # Load the document, split it into chunks, embed each chunk and load it into the vector store. raw_documents = TextLoader ( '../../../state_of_the_union.txt' ) . load ( ) text_splitter = CharacterTextSplitter ( chunk_size = 1000 , chunk_overlap = 0 ) documents = text_splitter . split_documents ( raw_documents ) db = FAISS . from_documents ( documents , OpenAIEmbeddings ( ) ) This notebook shows how to use functionality related to the LanceDB vector database based on the Lance data format. pip install lancedb We want to use OpenAIEmbeddings so we have to get the OpenAI API Key. import os import getpass os . environ [ 'OPENAI_API_KEY' ] = getpass . getpass ( 'OpenAI API Key:' ) from langchain . document_loaders import TextLoader from langchain . embeddings . openai import OpenAIEmbeddings from langchain . text_splitter import CharacterTextSplitter from langchain . vectorstores import LanceDB import lancedb db = lancedb . connect ( \"/tmp/lancedb\" ) table = db . create_table ( \"my_table\" , data = [ { \"vector\" : embeddings . embed_query ( \"Hello World\" ) , \"text\" : \"Hello World\" , \"id\" : \"1\" , } ] , mode = \"overwrite\" , ) # Load the document, split it into chunks, embed each chunk and load it into the vector store. raw_documents = TextLoader ( '../../../state_of_the_union.txt' ) . load ( ) text_splitter = CharacterTextSplitter ( chunk_size = 1000 , chunk_overlap = 0 ) documents = text_splitter . split_documents ( raw_documents ) db = LanceDB . from_documents ( documents , OpenAIEmbeddings ( ) , connection = table ) Similarity search ​ query = \"What did the president say about Ketanji Brown Jackson\" docs = db . similarity_search ( query ) print ( docs [ 0 ] . page_content ) Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence. Similarity search by vector ​ It is also possible to do a search for documents similar to a given embedding vector using similarity_search_by_vector which accepts an embedding vector as a parameter instead of a string. embedding_vector = OpenAIEmbeddings ( ) . embed_query ( query ) docs = db . similarity_search_by_vector ( embedding_vector ) print ( docs [ 0 ] . page_content ) The query is the same, and so the result is also the same. Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence. Asynchronous operations ​ Vector stores are usually run as a separate service that requires some IO operations, and therefore they might be called asynchronously. That gives performance benefits as you don't waste time waiting for responses from external services. That might also be important if you work with an asynchronous framework, such as FastAPI . LangChain supports async operation on vector stores. All the methods might be called using their async counterparts, with the prefix a , meaning async . Qdrant is a vector store, which supports all the async operations, thus it will be used in this walkthrough. pip install qdrant-client from langchain . vectorstores import Qdrant Create a vector store asynchronously ​ db = await Qdrant . afrom_documents ( documents , embeddings , \"http://localhost:6333\" ) Similarity search ​ query = \"What did the president say about Ketanji Brown Jackson\" docs = await db . asimilarity_search ( query ) print ( docs [ 0 ] . page_content ) Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence. Similarity search by vector ​ embedding_vector = embeddings . embed_query ( query ) docs = await db . asimilarity_search_by_vector ( embedding_vector ) Maximum marginal relevance search (MMR) ​ Maximal marginal relevance optimizes for similarity to query and diversity among selected documents. It is also supported in async API. query = \"What did the president say about Ketanji Brown Jackson\" found_docs = await qdrant . amax_marginal_relevance_search ( query , k = 2 , fetch_k = 10 ) for i , doc in enumerate ( found_docs ) : print ( f\" { i + 1 } .\" , doc . page_content , \"\\n\" ) 1. Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence. 2. We can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. Officer Mora was 27 years old. Officer Rivera was 22. Both Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. I’ve worked on these issues a long time. I know what works: Investing in crime prevention and community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety. Previous CacheBackedEmbeddings Next Retrievers"
    },
    {
        "title": "Retrievers | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/data_connection/retrievers/",
        "text_content": "Modules Retrieval Retrievers On this page Retrievers Head to Integrations for documentation\non built-in retriever integrations with 3rd-party tools. A retriever is an interface that returns documents given an unstructured\nquery. It is more general than a vector store. A retriever does not need\nto be able to store documents, only to return (or retrieve) them. Vector\nstores can be used as the backbone of a retriever, but there are other\ntypes of retrievers as well. Retrievers implement the Runnable\ninterface , the basic building\nblock of the LangChain Expression Language\n(LCEL) . This means they support invoke , ainvoke , stream , astream , batch , abatch , astream_log calls. Retrievers accept a string query as input and return a list of Document ’s as output. Get started ​ In this example we’ll use a Chroma vector store-backed retriever. To\nget setup we’ll need to run: pip install chromadb And download the state_of_the_union.txt file here . from langchain . embeddings import OpenAIEmbeddings from langchain . text_splitter import CharacterTextSplitter from langchain . vectorstores import Chroma full_text = open ( \"state_of_the_union.txt\" , \"r\" ) . read ( ) text_splitter = CharacterTextSplitter ( chunk_size = 1000 , chunk_overlap = 100 ) texts = text_splitter . split_text ( full_text ) embeddings = OpenAIEmbeddings ( ) db = Chroma . from_texts ( texts , embeddings ) retriever = db . as_retriever ( ) retrieved_docs = retriever . invoke ( \"What did the president say about Ketanji Brown Jackson?\" ) print ( retrieved_docs [ 0 ] . page_content ) One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence. A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. We can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling. We’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers. LCEL ​ Since retrievers are Runnable ’s, we can easily compose them with other Runnable objects: from langchain . chat_models import ChatOpenAI from langchain . prompts import ChatPromptTemplate from langchain . schema import StrOutputParser from langchain_core . runnables import RunnablePassthrough template = \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\" prompt = ChatPromptTemplate . from_template ( template ) model = ChatOpenAI ( ) def format_docs ( docs ) : return \"\\n\\n\" . join ( [ d . page_content for d in docs ] ) chain = ( { \"context\" : retriever | format_docs , \"question\" : RunnablePassthrough ( ) } | prompt | model | StrOutputParser ( ) ) chain . invoke ( \"What did the president say about technology?\" ) 'The president said that technology plays a crucial role in the future and that passing the Bipartisan Innovation Act will make record investments in emerging technologies and American manufacturing. The president also mentioned Intel\\'s plans to build a semiconductor \"mega site\" and increase their investment from $20 billion to $100 billion, which would be one of the biggest investments in manufacturing in American history.' Previous Vector stores Next MultiQueryRetriever"
    },
    {
        "title": "Indexing | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/data_connection/indexing",
        "text_content": "Modules Retrieval Indexing On this page Indexing Here, we will look at a basic indexing workflow using the LangChain\nindexing API. The indexing API lets you load and keep in sync documents from any\nsource into a vector store. Specifically, it helps: Avoid writing duplicated content into the vector store Avoid re-writing unchanged content Avoid re-computing embeddings over unchanged content All of which should save you time and money, as well as improve your\nvector search results. Crucially, the indexing API will work even with documents that have gone\nthrough several transformation steps (e.g., via text chunking) with\nrespect to the original source documents. How it works ​ LangChain indexing makes use of a record manager ( RecordManager ) that\nkeeps track of document writes into the vector store. When indexing content, hashes are computed for each document, and the\nfollowing information is stored in the record manager: the document hash (hash of both page content and metadata) write time the source id – each document should include information in its\nmetadata to allow us to determine the ultimate source of this\ndocument Deletion modes ​ When indexing documents into a vector store, it’s possible that some\nexisting documents in the vector store should be deleted. In certain\nsituations you may want to remove any existing documents that are\nderived from the same sources as the new documents being indexed. In\nothers you may want to delete all existing documents wholesale. The\nindexing API deletion modes let you pick the behavior you want: Cleanup Mode De-Duplicates Content Parallelizable Cleans Up Deleted Source Docs Cleans Up Mutations of Source Docs and/or Derived Docs Clean Up Timing None ✅ ✅ ❌ ❌ - Incremental ✅ ✅ ❌ ✅ Continuously Full ✅ ❌ ✅ ✅ At end of indexing None does not do any automatic clean up, allowing the user to manually\ndo clean up of old content. incremental and full offer the following automated clean up: If the content of the source document or derived documents has changed , both incremental or full modes will clean up\n(delete) previous versions of the content. If the source document has been deleted (meaning it is not\nincluded in the documents currently being indexed), the full cleanup mode will delete it from the vector store correctly, but the incremental mode will not. When content is mutated (e.g., the source PDF file was revised) there\nwill be a period of time during indexing when both the new and old\nversions may be returned to the user. This happens after the new content\nwas written, but before the old version was deleted. incremental indexing minimizes this period of time as it is able\nto do clean up continuously, as it writes. full mode does the clean up after all batches have been written. Requirements ​ Do not use with a store that has been pre-populated with content\nindependently of the indexing API, as the record manager will not\nknow that records have been inserted previously. Only works with LangChain vectorstore ’s that support: document addition by id ( add_documents method with ids argument) delete by id ( delete method with ids argument) Compatible Vectorstores: AnalyticDB , AstraDB , AwaDB , Bagel , Cassandra , Chroma , DashVector , DatabricksVectorSearch , DeepLake , Dingo , ElasticVectorSearch , ElasticsearchStore , FAISS , MyScale , PGVector , Pinecone , Qdrant , Redis , ScaNN , SupabaseVectorStore , TimescaleVector , Vald , Vearch , VespaStore , Weaviate , ZepVectorStore . Caution ​ The record manager relies on a time-based mechanism to determine what\ncontent can be cleaned up (when using full or incremental cleanup\nmodes). If two tasks run back-to-back, and the first task finishes before the\nclock time changes, then the second task may not be able to clean up\ncontent. This is unlikely to be an issue in actual settings for the following\nreasons: The RecordManager uses higher resolution timestamps. The data would need to change between the first and the second tasks\nruns, which becomes unlikely if the time interval between the tasks\nis small. Indexing tasks typically take more than a few ms. Quickstart ​ from langchain . embeddings import OpenAIEmbeddings from langchain . indexes import SQLRecordManager , index from langchain . schema import Document from langchain . vectorstores import ElasticsearchStore Initialize a vector store and set up the embeddings: collection_name = \"test_index\" embedding = OpenAIEmbeddings ( ) vectorstore = ElasticsearchStore ( es_url = \"http://localhost:9200\" , index_name = \"test_index\" , embedding = embedding ) Initialize a record manager with an appropriate namespace. Suggestion: Use a namespace that takes into account both the vector\nstore and the collection name in the vector store; e.g.,\n‘redis/my_docs’, ‘chromadb/my_docs’ or ‘postgres/my_docs’. namespace = f\"elasticsearch/ { collection_name } \" record_manager = SQLRecordManager ( namespace , db_url = \"sqlite:///record_manager_cache.sql\" ) Create a schema before using the record manager. record_manager . create_schema ( ) Let’s index some test documents: doc1 = Document ( page_content = \"kitty\" , metadata = { \"source\" : \"kitty.txt\" } ) doc2 = Document ( page_content = \"doggy\" , metadata = { \"source\" : \"doggy.txt\" } ) Indexing into an empty vector store: def _clear ( ) : \"\"\"Hacky helper method to clear content. See the `full` mode section to to understand why it works.\"\"\" index ( [ ] , record_manager , vectorstore , cleanup = \"full\" , source_id_key = \"source\" ) None deletion mode ​ This mode does not do automatic clean up of old versions of content;\nhowever, it still takes care of content de-duplication. _clear ( ) index ( [ doc1 , doc1 , doc1 , doc1 , doc1 ] , record_manager , vectorstore , cleanup = None , source_id_key = \"source\" , ) {'num_added': 1, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0} _clear ( ) index ( [ doc1 , doc2 ] , record_manager , vectorstore , cleanup = None , source_id_key = \"source\" ) {'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0} Second time around all content will be skipped: index ( [ doc1 , doc2 ] , record_manager , vectorstore , cleanup = None , source_id_key = \"source\" ) {'num_added': 0, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0} \"incremental\" deletion mode ​ _clear ( ) index ( [ doc1 , doc2 ] , record_manager , vectorstore , cleanup = \"incremental\" , source_id_key = \"source\" , ) {'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0} Indexing again should result in both documents getting skipped –\nalso skipping the embedding operation! index ( [ doc1 , doc2 ] , record_manager , vectorstore , cleanup = \"incremental\" , source_id_key = \"source\" , ) {'num_added': 0, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0} If we provide no documents with incremental indexing mode, nothing will\nchange. index ( [ ] , record_manager , vectorstore , cleanup = \"incremental\" , source_id_key = \"source\" ) {'num_added': 0, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0} If we mutate a document, the new version will be written and all old\nversions sharing the same source will be deleted. changed_doc_2 = Document ( page_content = \"puppy\" , metadata = { \"source\" : \"doggy.txt\" } ) index ( [ changed_doc_2 ] , record_manager , vectorstore , cleanup = \"incremental\" , source_id_key = \"source\" , ) {'num_added': 1, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 1} \"full\" deletion mode ​ In full mode the user should pass the full universe of content that\nshould be indexed into the indexing function. Any documents that are not passed into the indexing function and are\npresent in the vectorstore will be deleted! This behavior is useful to handle deletions of source documents. _clear ( ) all_docs = [ doc1 , doc2 ] index ( all_docs , record_manager , vectorstore , cleanup = \"full\" , source_id_key = \"source\" ) {'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0} Say someone deleted the first doc: del all_docs [ 0 ] all_docs [Document(page_content='doggy', metadata={'source': 'doggy.txt'})] Using full mode will clean up the deleted content as well. index ( all_docs , record_manager , vectorstore , cleanup = \"full\" , source_id_key = \"source\" ) {'num_added': 0, 'num_updated': 0, 'num_skipped': 1, 'num_deleted': 1} Source ​ The metadata attribute contains a field called source . This source\nshould be pointing at the ultimate provenance associated with the\ngiven document. For example, if these documents are representing chunks of some parent\ndocument, the source for both documents should be the same and\nreference the parent document. In general, source should always be specified. Only use a None , if\nyou never intend to use incremental mode, and for some reason\ncan’t specify the source field correctly. from langchain . text_splitter import CharacterTextSplitter doc1 = Document ( page_content = \"kitty kitty kitty kitty kitty\" , metadata = { \"source\" : \"kitty.txt\" } ) doc2 = Document ( page_content = \"doggy doggy the doggy\" , metadata = { \"source\" : \"doggy.txt\" } ) new_docs = CharacterTextSplitter ( separator = \"t\" , keep_separator = True , chunk_size = 12 , chunk_overlap = 2 ) . split_documents ( [ doc1 , doc2 ] ) new_docs [Document(page_content='kitty kit', metadata={'source': 'kitty.txt'}), Document(page_content='tty kitty ki', metadata={'source': 'kitty.txt'}), Document(page_content='tty kitty', metadata={'source': 'kitty.txt'}), Document(page_content='doggy doggy', metadata={'source': 'doggy.txt'}), Document(page_content='the doggy', metadata={'source': 'doggy.txt'})] _clear ( ) index ( new_docs , record_manager , vectorstore , cleanup = \"incremental\" , source_id_key = \"source\" , ) {'num_added': 5, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0} changed_doggy_docs = [ Document ( page_content = \"woof woof\" , metadata = { \"source\" : \"doggy.txt\" } ) , Document ( page_content = \"woof woof woof\" , metadata = { \"source\" : \"doggy.txt\" } ) , ] This should delete the old versions of documents associated with doggy.txt source and replace them with the new versions. index ( changed_doggy_docs , record_manager , vectorstore , cleanup = \"incremental\" , source_id_key = \"source\" , ) {'num_added': 0, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 2} vectorstore . similarity_search ( \"dog\" , k = 30 ) [Document(page_content='tty kitty', metadata={'source': 'kitty.txt'}), Document(page_content='tty kitty ki', metadata={'source': 'kitty.txt'}), Document(page_content='kitty kit', metadata={'source': 'kitty.txt'})] Using with loaders ​ Indexing can accept either an iterable of documents or else any loader. Attention: The loader must set source keys correctly. from langchain . document_loaders . base import BaseLoader class MyCustomLoader ( BaseLoader ) : def lazy_load ( self ) : text_splitter = CharacterTextSplitter ( separator = \"t\" , keep_separator = True , chunk_size = 12 , chunk_overlap = 2 ) docs = [ Document ( page_content = \"woof woof\" , metadata = { \"source\" : \"doggy.txt\" } ) , Document ( page_content = \"woof woof woof\" , metadata = { \"source\" : \"doggy.txt\" } ) , ] yield from text_splitter . split_documents ( docs ) def load ( self ) : return list ( self . lazy_load ( ) ) _clear ( ) loader = MyCustomLoader ( ) loader . load ( ) [Document(page_content='woof woof', metadata={'source': 'doggy.txt'}), Document(page_content='woof woof woof', metadata={'source': 'doggy.txt'})] index ( loader , record_manager , vectorstore , cleanup = \"full\" , source_id_key = \"source\" ) {'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0} vectorstore . similarity_search ( \"dog\" , k = 30 ) [Document(page_content='woof woof', metadata={'source': 'doggy.txt'}), Document(page_content='woof woof woof', metadata={'source': 'doggy.txt'})] Previous WebResearchRetriever Next Agents"
    },
    {
        "title": "Parent Document Retriever | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever",
        "text_content": "Modules Retrieval Retrievers Parent Document Retriever On this page Parent Document Retriever When splitting documents for retrieval, there are often conflicting\ndesires: You may want to have small documents, so that their embeddings can\nmost accurately reflect their meaning. If too long, then the\nembeddings can lose meaning. You want to have long enough documents that the context of each\nchunk is retained. The ParentDocumentRetriever strikes that balance by splitting and\nstoring small chunks of data. During retrieval, it first fetches the\nsmall chunks but then looks up the parent ids for those chunks and\nreturns those larger documents. Note that “parent document” refers to the document that a small chunk\noriginated from. This can either be the whole raw document OR a larger\nchunk. from langchain . retrievers import ParentDocumentRetriever from langchain . document_loaders import TextLoader from langchain . embeddings import OpenAIEmbeddings from langchain . storage import InMemoryStore from langchain . text_splitter import RecursiveCharacterTextSplitter from langchain . vectorstores import Chroma loaders = [ TextLoader ( \"../../paul_graham_essay.txt\" ) , TextLoader ( \"../../state_of_the_union.txt\" ) , ] docs = [ ] for loader in loaders : docs . extend ( loader . load ( ) ) Retrieving full documents ​ In this mode, we want to retrieve the full documents. Therefore, we only\nspecify a child splitter. # This text splitter is used to create the child documents child_splitter = RecursiveCharacterTextSplitter ( chunk_size = 400 ) # The vectorstore to use to index the child chunks vectorstore = Chroma ( collection_name = \"full_documents\" , embedding_function = OpenAIEmbeddings ( ) ) # The storage layer for the parent documents store = InMemoryStore ( ) retriever = ParentDocumentRetriever ( vectorstore = vectorstore , docstore = store , child_splitter = child_splitter , ) retriever . add_documents ( docs , ids = None ) This should yield two keys, because we added two documents. list ( store . yield_keys ( ) ) ['f73cb162-5eb2-4118-abcf-d87aa6a1b564', '8a2478e0-ac7d-4abf-811a-33a8ace3e3b8'] Let’s now call the vector store search functionality - we should see\nthat it returns small chunks (since we’re storing the small chunks). sub_docs = vectorstore . similarity_search ( \"justice breyer\" ) print ( sub_docs [ 0 ] . page_content ) Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. Let’s now retrieve from the overall retriever. This should return large\ndocuments - since it returns the documents where the smaller chunks are\nlocated. retrieved_docs = retriever . get_relevant_documents ( \"justice breyer\" ) len ( retrieved_docs [ 0 ] . page_content ) 38540 Retrieving larger chunks ​ Sometimes, the full documents can be too big to want to retrieve them as\nis. In that case, what we really want to do is to first split the raw\ndocuments into larger chunks, and then split it into smaller chunks. We\nthen index the smaller chunks, but on retrieval we retrieve the larger\nchunks (but still not the full documents). # This text splitter is used to create the parent documents parent_splitter = RecursiveCharacterTextSplitter ( chunk_size = 2000 ) # This text splitter is used to create the child documents # It should create documents smaller than the parent child_splitter = RecursiveCharacterTextSplitter ( chunk_size = 400 ) # The vectorstore to use to index the child chunks vectorstore = Chroma ( collection_name = \"split_parents\" , embedding_function = OpenAIEmbeddings ( ) ) # The storage layer for the parent documents store = InMemoryStore ( ) retriever = ParentDocumentRetriever ( vectorstore = vectorstore , docstore = store , child_splitter = child_splitter , parent_splitter = parent_splitter , ) retriever . add_documents ( docs ) We can see that there are much more than two documents now - these are\nthe larger chunks. len ( list ( store . yield_keys ( ) ) ) 66 Let’s make sure the underlying vector store still retrieves the small\nchunks. sub_docs = vectorstore . similarity_search ( \"justice breyer\" ) print ( sub_docs [ 0 ] . page_content ) Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. retrieved_docs = retriever . get_relevant_documents ( \"justice breyer\" ) len ( retrieved_docs [ 0 ] . page_content ) 1849 print ( retrieved_docs [ 0 ] . page_content ) In state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. We cannot let this happen. Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence. A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. We can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling. We’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers. We’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. We’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders. Previous MultiVector Retriever Next Self-querying"
    },
    {
        "title": "Ensemble Retriever | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble",
        "text_content": "Modules Retrieval Retrievers Ensemble Retriever Ensemble Retriever The EnsembleRetriever takes a list of retrievers as input and ensemble\nthe results of their get_relevant_documents() methods and rerank the\nresults based on the Reciprocal Rank\nFusion algorithm. By leveraging the strengths of different algorithms, the EnsembleRetriever can achieve better performance than any single\nalgorithm. The most common pattern is to combine a sparse retriever (like BM25)\nwith a dense retriever (like embedding similarity), because their\nstrengths are complementary. It is also known as “hybrid search”. The\nsparse retriever is good at finding relevant documents based on\nkeywords, while the dense retriever is good at finding relevant\ndocuments based on semantic similarity. from langchain . embeddings import OpenAIEmbeddings from langchain . retrievers import BM25Retriever , EnsembleRetriever from langchain . vectorstores import FAISS doc_list = [ \"I like apples\" , \"I like oranges\" , \"Apples and oranges are fruits\" , ] # initialize the bm25 retriever and faiss retriever bm25_retriever = BM25Retriever . from_texts ( doc_list ) bm25_retriever . k = 2 embedding = OpenAIEmbeddings ( ) faiss_vectorstore = FAISS . from_texts ( doc_list , embedding ) faiss_retriever = faiss_vectorstore . as_retriever ( search_kwargs = { \"k\" : 2 } ) # initialize the ensemble retriever ensemble_retriever = EnsembleRetriever ( retrievers = [ bm25_retriever , faiss_retriever ] , weights = [ 0.5 , 0.5 ] ) docs = ensemble_retriever . get_relevant_documents ( \"apples\" ) docs [Document(page_content='I like apples', metadata={}), Document(page_content='Apples and oranges are fruits', metadata={})] Previous Contextual compression Next MultiVector Retriever"
    },
    {
        "title": "XML parser | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/model_io/output_parsers/xml",
        "text_content": "Modules Model I/O Output parsers XML parser XML parser This output parser allows users to obtain results from LLM in the\npopular XML format. Keep in mind that large language models are leaky abstractions! You’ll\nhave to use an LLM with sufficient capacity to generate well-formed XML. In the following example we use Claude model\n( https://docs.anthropic.com/claude/docs ) which works really well with\nXML tags. from langchain . llms import Anthropic from langchain . output_parsers import XMLOutputParser from langchain . prompts import PromptTemplate model = Anthropic ( model = \"claude-2\" , max_tokens_to_sample = 512 , temperature = 0.1 ) /Users/harrisonchase/workplace/langchain/libs/langchain/langchain/llms/anthropic.py:171: UserWarning: This Anthropic LLM is deprecated. Please use `from langchain.chat_models import ChatAnthropic` instead warnings.warn( Let’s start with the simple request to the model. actor_query = \"Generate the shortened filmography for Tom Hanks.\" output = model ( f\"\"\" Human: { actor_query } Please enclose the movies in <movie></movie> tags Assistant: \"\"\" ) print ( output ) Here is the shortened filmography for Tom Hanks enclosed in <movie> tags: <movie>Splash (1984)</movie> <movie>Big (1988)</movie> <movie>A League of Their Own (1992)</movie> <movie>Sleepless in Seattle (1993)</movie> <movie>Forrest Gump (1994)</movie> <movie>Apollo 13 (1995)</movie> <movie>Toy Story (1995)</movie> <movie>Saving Private Ryan (1998)</movie> <movie>Cast Away (2000)</movie> <movie>The Da Vinci Code (2006)</movie> <movie>Toy Story 3 (2010)</movie> <movie>Captain Phillips (2013)</movie> <movie>Bridge of Spies (2015)</movie> <movie>Toy Story 4 (2019)</movie> Now we will use the XMLOutputParser in order to get the structured\noutput. parser = XMLOutputParser ( ) prompt = PromptTemplate ( template = \"\"\" Human: {query} {format_instructions} Assistant:\"\"\" , input_variables = [ \"query\" ] , partial_variables = { \"format_instructions\" : parser . get_format_instructions ( ) } , ) chain = prompt | model | parser output = chain . invoke ( { \"query\" : actor_query } ) print ( output ) {'filmography': [{'movie': [{'title': 'Splash'}, {'year': '1984'}]}, {'movie': [{'title': 'Big'}, {'year': '1988'}]}, {'movie': [{'title': 'A League of Their Own'}, {'year': '1992'}]}, {'movie': [{'title': 'Sleepless in Seattle'}, {'year': '1993'}]}, {'movie': [{'title': 'Forrest Gump'}, {'year': '1994'}]}, {'movie': [{'title': 'Toy Story'}, {'year': '1995'}]}, {'movie': [{'title': 'Apollo 13'}, {'year': '1995'}]}, {'movie': [{'title': 'Saving Private Ryan'}, {'year': '1998'}]}, {'movie': [{'title': 'Cast Away'}, {'year': '2000'}]}, {'movie': [{'title': 'Catch Me If You Can'}, {'year': '2002'}]}, {'movie': [{'title': 'The Polar Express'}, {'year': '2004'}]}, {'movie': [{'title': 'Bridge of Spies'}, {'year': '2015'}]}]} Finally, let’s add some tags to tailor the output to our needs. parser = XMLOutputParser ( tags = [ \"movies\" , \"actor\" , \"film\" , \"name\" , \"genre\" ] ) prompt = PromptTemplate ( template = \"\"\" Human: {query} {format_instructions} Assistant:\"\"\" , input_variables = [ \"query\" ] , partial_variables = { \"format_instructions\" : parser . get_format_instructions ( ) } , ) chain = prompt | model | parser output = chain . invoke ( { \"query\" : actor_query } ) print ( output ) {'movies': [{'actor': [{'name': 'Tom Hanks'}, {'film': [{'name': 'Splash'}, {'genre': 'Comedy'}]}, {'film': [{'name': 'Big'}, {'genre': 'Comedy'}]}, {'film': [{'name': 'A League of Their Own'}, {'genre': 'Comedy'}]}, {'film': [{'name': 'Sleepless in Seattle'}, {'genre': 'Romance'}]}, {'film': [{'name': 'Forrest Gump'}, {'genre': 'Drama'}]}, {'film': [{'name': 'Toy Story'}, {'genre': 'Animation'}]}, {'film': [{'name': 'Apollo 13'}, {'genre': 'Drama'}]}, {'film': [{'name': 'Saving Private Ryan'}, {'genre': 'War'}]}, {'film': [{'name': 'Cast Away'}, {'genre': 'Adventure'}]}, {'film': [{'name': 'The Green Mile'}, {'genre': 'Drama'}]}]}]} Previous Structured output parser Next Retrieval"
    },
    {
        "title": "Agent Types | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/agents/agent_types/",
        "text_content": "Modules Agents Agent Types On this page Agent Types Agents use an LLM to determine which actions to take and in what order.\nAn action can either be using a tool and observing its output, or returning a response to the user.\nHere are the agents available in LangChain. Zero-shot ReAct ​ This agent uses the ReAct framework to determine which tool to use\nbased solely on the tool's description. Any number of tools can be provided.\nThis agent requires that a description is provided for each tool. Note : This is the most general purpose action agent. Structured input ReAct ​ The structured tool chat agent is capable of using multi-input tools.\nOlder agents are configured to specify an action input as a single string, but this agent can use a tools' argument\nschema to create a structured action input. This is useful for more complex tool usage, like precisely\nnavigating around a browser. OpenAI Functions ​ Certain OpenAI models (like gpt-3.5-turbo-0613 and gpt-4-0613) have been explicitly fine-tuned to detect when a\nfunction should be called and respond with the inputs that should be passed to the function.\nThe OpenAI Functions Agent is designed to work with these models. Conversational ​ This agent is designed to be used in conversational settings.\nThe prompt is designed to make the agent helpful and conversational.\nIt uses the ReAct framework to decide which tool to use, and uses memory to remember the previous conversation interactions. Self-ask with search ​ This agent utilizes a single tool that should be named Intermediate Answer .\nThis tool should be able to look up factual answers to questions. This agent\nis equivalent to the original self-ask with search paper ,\nwhere a Google search API was provided as the tool. ReAct document store ​ This agent uses the ReAct framework to interact with a docstore. Two tools must\nbe provided: a Search tool and a Lookup tool (they must be named exactly as so).\nThe Search tool should search for a document, while the Lookup tool should look up\na term in the most recently found document.\nThis agent is equivalent to the\noriginal ReAct paper , specifically the Wikipedia example. Previous Agents Next Conversational"
    },
    {
        "title": "Add Memory to OpenAI Functions Agent | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/agents/how_to/add_memory_openai_functions",
        "text_content": "Modules Agents How-to Add Memory to OpenAI Functions Agent Add Memory to OpenAI Functions Agent This notebook goes over how to add memory to an OpenAI Functions agent. from langchain . agents import AgentType , Tool , initialize_agent from langchain . chains import LLMMathChain from langchain . chat_models import ChatOpenAI from langchain . utilities import SerpAPIWrapper , SQLDatabase from langchain_experimental . sql import SQLDatabaseChain llm = ChatOpenAI ( temperature = 0 , model = \"gpt-3.5-turbo-0613\" ) search = SerpAPIWrapper ( ) llm_math_chain = LLMMathChain . from_llm ( llm = llm , verbose = True ) db = SQLDatabase . from_uri ( \"sqlite:///../../../../../notebooks/Chinook.db\" ) db_chain = SQLDatabaseChain . from_llm ( llm , db , verbose = True ) tools = [ Tool ( name = \"Search\" , func = search . run , description = \"useful for when you need to answer questions about current events. You should ask targeted questions\" , ) , Tool ( name = \"Calculator\" , func = llm_math_chain . run , description = \"useful for when you need to answer questions about math\" , ) , Tool ( name = \"FooBar-DB\" , func = db_chain . run , description = \"useful for when you need to answer questions about FooBar. Input should be in the form of a question containing full context\" , ) , ] from langchain . memory import ConversationBufferMemory from langchain . prompts import MessagesPlaceholder agent_kwargs = { \"extra_prompt_messages\" : [ MessagesPlaceholder ( variable_name = \"memory\" ) ] , } memory = ConversationBufferMemory ( memory_key = \"memory\" , return_messages = True ) agent = initialize_agent ( tools , llm , agent = AgentType . OPENAI_FUNCTIONS , verbose = True , agent_kwargs = agent_kwargs , memory = memory , ) agent . run ( \"hi\" ) > Entering new  chain... Hello! How can I assist you today? > Finished chain. 'Hello! How can I assist you today?' agent . run ( \"my name is bob\" ) > Entering new  chain... Nice to meet you, Bob! How can I help you today? > Finished chain. 'Nice to meet you, Bob! How can I help you today?' agent . run ( \"whats my name\" ) > Entering new  chain... Your name is Bob. > Finished chain. 'Your name is Bob.' Previous XML Agent Next Running Agent as an Iterator"
    },
    {
        "title": "Tools | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/agents/tools/",
        "text_content": "Modules Agents Tools On this page Tools info For documentation on built-in tool integrations, visit Integrations . Tools are interfaces that an agent can use to interact with the world. Getting Started ​ Tools are functions that agents can use to interact with the world.\nThese tools can be generic utilities (e.g. search), other chains, or even other agents. Currently, tools can be loaded using the following snippet: from langchain . agents import load_tools tool_names = [ . . . ] tools = load_tools ( tool_names ) Some tools (e.g. chains, agents) may require a base LLM to use to initialize them.\nIn that case, you can pass in an LLM as well: from langchain . agents import load_tools tool_names = [ . . . ] llm = . . . tools = load_tools ( tool_names , llm = llm ) Previous Vectorstore Next Toolkits"
    },
    {
        "title": "Tools | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/tools/",
        "text_content": ""
    },
    {
        "title": "Agents and toolkits | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/toolkits/",
        "text_content": ""
    },
    {
        "title": "How to | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/chains/how_to/",
        "text_content": "Modules More Chains How to How to 📄️ Async API LangChain provides async support by leveraging the 📄️ Different call methods All classes inherited from Chain offer a few ways of running chain 📄️ Custom chain To implement your own custom chain you can subclass Chain and 📄️ Adding memory (state) Chains can be initialized with a Memory object, which will persist data across calls to the chain. This makes a Chain stateful. 📄️ Using OpenAI functions This walkthrough demonstrates how to incorporate OpenAI function-calling Previous Chains Next Async API"
    },
    {
        "title": "Foundational | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/chains/foundational/",
        "text_content": "Modules More Chains Foundational Foundational 📄️ LLM The most common type of chaining in any LLM application is combining a 📄️ Router Routing allows you to create non-deterministic chains where the output 📄️ Sequential The next step after calling a language model is to make a series of 📄️ Transformation Often we want to transform inputs as they are passed from one component Previous Using OpenAI functions Next LLM"
    },
    {
        "title": "Documents | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/chains/document/",
        "text_content": "Modules More Chains Documents Documents These are the core chains for working with documents. They are useful for summarizing documents, answering questions over documents, extracting information from documents, and more. These chains all implement a common interface: class BaseCombineDocumentsChain ( Chain , ABC ) : \"\"\"Base interface for chains combining documents.\"\"\" @abstractmethod def combine_docs ( self , docs : List [ Document ] , ** kwargs : Any ) - > Tuple [ str , dict ] : \"\"\"Combine documents into a single string.\"\"\" 📄️ Stuff The stuff documents chain (“stuff” as in “to stuff” or “to fill”) is the 📄️ Refine refine} 📄️ Map reduce The map reduce documents chain first applies an LLM chain to each 📄️ Map re-rank The map re-rank documents chain runs an initial prompt on each document, Previous Transformation Next Stuff"
    },
    {
        "title": "Token counting | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/callbacks/token_counting",
        "text_content": "Modules More Callbacks Token counting Token counting LangChain offers a context manager that allows you to count tokens. import asyncio from langchain . callbacks import get_openai_callback from langchain . llms import OpenAI llm = OpenAI ( temperature = 0 ) with get_openai_callback ( ) as cb : llm ( \"What is the square root of 4?\" ) total_tokens = cb . total_tokens assert total_tokens > 0 with get_openai_callback ( ) as cb : llm ( \"What is the square root of 4?\" ) llm ( \"What is the square root of 4?\" ) assert cb . total_tokens == total_tokens * 2 # You can kick off concurrent runs from within the context manager with get_openai_callback ( ) as cb : await asyncio . gather ( * [ llm . agenerate ( [ \"What is the square root of 4?\" ] ) for _ in range ( 3 ) ] ) assert cb . total_tokens == total_tokens * 3 # The context manager is concurrency safe task = asyncio . create_task ( llm . agenerate ( [ \"What is the square root of 4?\" ] ) ) with get_openai_callback ( ) as cb : await llm . agenerate ( [ \"What is the square root of 4?\" ] ) await task assert cb . total_tokens == total_tokens Previous Tags Next 🦜️🏓 LangServe"
    },
    {
        "title": "LangSmith Walkthrough | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/langsmith/walkthrough",
        "text_content": "LangSmith LangSmith Walkthrough On this page LangSmith Walkthrough Open In Colab LangChain makes it easy to prototype LLM applications and Agents.\nHowever, delivering LLM applications to production can be deceptively\ndifficult. You will likely have to heavily customize and iterate on your\nprompts, chains, and other components to create a high-quality product. To aid in this process, we’ve launched LangSmith, a unified platform for\ndebugging, testing, and monitoring your LLM applications. When might this come in handy? You may find it useful when you want to: Quickly debug a new chain, agent, or set of tools Visualize how components (chains, llms, retrievers, etc.) relate and\nare used Evaluate different prompts and LLMs for a single component Run a given chain several times over a dataset to ensure it\nconsistently meets a quality bar Capture usage traces and using LLMs or analytics pipelines to\ngenerate insights Prerequisites ​ Create a LangSmith account and create\nan API key (see bottom left corner). Familiarize yourself with the\nplatform by looking through the docs Note LangSmith is in closed beta; we’re in the process of rolling it out\nto more users. However, you can fill out the form on the website for\nexpedited access. Now, let’s get started! Log runs to LangSmith ​ First, configure your environment variables to tell LangChain to log\ntraces. This is done by setting the LANGCHAIN_TRACING_V2 environment\nvariable to true. You can tell LangChain which project to log to by\nsetting the LANGCHAIN_PROJECT environment variable (if this isn’t set,\nruns will be logged to the default project). This will automatically\ncreate the project for you if it doesn’t exist. You must also set the LANGCHAIN_ENDPOINT and LANGCHAIN_API_KEY environment variables. For more information on other ways to set up tracing, please reference\nthe LangSmith documentation . NOTE: You must also set your OPENAI_API_KEY environment variables\nin order to run the following tutorial. NOTE: You can only access an API key when you first create it. Keep\nit somewhere safe. NOTE: You can also use a context manager in python to log traces\nusing from langchain . callbacks . manager import tracing_v2_enabled with tracing_v2_enabled ( project_name = \"My Project\" ) : agent . run ( \"How many people live in canada as of 2023?\" ) However, in this example, we will use environment variables. % pip install - U langchain langsmith langchainhub - - quiet % pip install openai tiktoken pandas duckduckgo - search - - quiet import os from uuid import uuid4 unique_id = uuid4 ( ) . hex [ 0 : 8 ] os . environ [ \"LANGCHAIN_TRACING_V2\" ] = \"true\" os . environ [ \"LANGCHAIN_PROJECT\" ] = f\"Tracing Walkthrough - { unique_id } \" os . environ [ \"LANGCHAIN_ENDPOINT\" ] = \"https://api.smith.langchain.com\" os . environ [ \"LANGCHAIN_API_KEY\" ] = \"<YOUR-API-KEY>\" # Update to your API key # Used by the agent in this tutorial os . environ [ \"OPENAI_API_KEY\" ] = \"<YOUR-OPENAI-API-KEY>\" Create the langsmith client to interact with the API from langsmith import Client client = Client ( ) Create a LangChain component and log runs to the platform. In this\nexample, we will create a ReAct-style agent with access to a general\nsearch tool (DuckDuckGo). The agent’s prompt can be viewed in the Hub\nhere . from langchain import hub from langchain . agents import AgentExecutor from langchain . agents . format_scratchpad import format_to_openai_function_messages from langchain . agents . output_parsers import OpenAIFunctionsAgentOutputParser from langchain . chat_models import ChatOpenAI from langchain . tools import DuckDuckGoSearchResults from langchain . tools . render import format_tool_to_openai_function # Fetches the latest version of this prompt prompt = hub . pull ( \"wfh/langsmith-agent-prompt:latest\" ) llm = ChatOpenAI ( model = \"gpt-3.5-turbo-16k\" , temperature = 0 , ) tools = [ DuckDuckGoSearchResults ( name = \"duck_duck_go\" ) , # General internet search using DuckDuckGo ] llm_with_tools = llm . bind ( functions = [ format_tool_to_openai_function ( t ) for t in tools ] ) runnable_agent = ( { \"input\" : lambda x : x [ \"input\" ] , \"agent_scratchpad\" : lambda x : format_to_openai_function_messages ( x [ \"intermediate_steps\" ] ) , } | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser ( ) ) agent_executor = AgentExecutor ( agent = runnable_agent , tools = tools , handle_parsing_errors = True ) We are running the agent concurrently on multiple inputs to reduce\nlatency. Runs get logged to LangSmith in the background so execution\nlatency is unaffected. inputs = [ \"What is LangChain?\" , \"What's LangSmith?\" , \"When was Llama-v2 released?\" , \"What is the langsmith cookbook?\" , \"When did langchain first announce the hub?\" , ] results = agent_executor . batch ( [ { \"input\" : x } for x in inputs ] , return_exceptions = True ) results [ : 2 ] [{'input': 'What is LangChain?', 'output': 'I\\'m sorry, but I couldn\\'t find any information about \"LangChain\". Could you please provide more context or clarify your question?'}, {'input': \"What's LangSmith?\", 'output': 'I\\'m sorry, but I couldn\\'t find any information about \"LangSmith\". It could be a specific term or a company that is not widely known. Can you provide more context or clarify what you are referring to?'}] Assuming you’ve successfully set up your environment, your agent traces\nshould show up in the Projects section in the app . Congrats! It looks like the agent isn’t effectively using the tools though. Let’s\nevaluate this so we have a baseline. Evaluate Agent ​ In addition to logging runs, LangSmith also allows you to test and\nevaluate your LLM applications. In this section, you will leverage LangSmith to create a benchmark\ndataset and run AI-assisted evaluators on an agent. You will do so in a\nfew steps: Create a dataset Initialize a new agent to benchmark Configure evaluators to grade an agent’s output Run the agent over the dataset and evaluate the results 1. Create a LangSmith dataset ​ Below, we use the LangSmith client to create a dataset from the input\nquestions from above and a list labels. You will use these later to\nmeasure performance for a new agent. A dataset is a collection of\nexamples, which are nothing more than input-output pairs you can use as\ntest cases to your application. For more information on datasets, including how to create them from CSVs\nor other files or how to create them in the platform, please refer to\nthe LangSmith documentation . outputs = [ \"LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.\" , \"LangSmith is a unified platform for debugging, testing, and monitoring language model applications and agents powered by LangChain\" , \"July 18, 2023\" , \"The langsmith cookbook is a github repository containing detailed examples of how to use LangSmith to debug, evaluate, and monitor large language model-powered applications.\" , \"September 5, 2023\" , ] dataset_name = f\"agent-qa- { unique_id } \" dataset = client . create_dataset ( dataset_name , description = \"An example dataset of questions over the LangSmith documentation.\" , ) for query , answer in zip ( inputs , outputs ) : client . create_example ( inputs = { \"input\" : query } , outputs = { \"output\" : answer } , dataset_id = dataset . id ) 2. Initialize a new agent to benchmark ​ LangSmith lets you evaluate any LLM, chain, agent, or even a custom\nfunction. Conversational agents are stateful (they have memory); to\nensure that this state isn’t shared between dataset runs, we will pass\nin a chain_factory (aka a constructor ) function to initialize for\neach call. In this case, we will test an agent that uses OpenAI’s function calling\nendpoints. from langchain import hub from langchain . agents import AgentExecutor , AgentType , initialize_agent , load_tools from langchain . agents . format_scratchpad import format_to_openai_function_messages from langchain . agents . output_parsers import OpenAIFunctionsAgentOutputParser from langchain . chat_models import ChatOpenAI from langchain . tools . render import format_tool_to_openai_function # Since chains can be stateful (e.g. they can have memory), we provide # a way to initialize a new chain for each row in the dataset. This is done # by passing in a factory function that returns a new chain for each row. def agent_factory ( prompt ) : llm_with_tools = llm . bind ( functions = [ format_tool_to_openai_function ( t ) for t in tools ] ) runnable_agent = ( { \"input\" : lambda x : x [ \"input\" ] , \"agent_scratchpad\" : lambda x : format_to_openai_function_messages ( x [ \"intermediate_steps\" ] ) , } | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser ( ) ) return AgentExecutor ( agent = runnable_agent , tools = tools , handle_parsing_errors = True ) 3. Configure evaluation ​ Manually comparing the results of chains in the UI is effective, but it\ncan be time consuming. It can be helpful to use automated metrics and\nAI-assisted feedback to evaluate your component’s performance. Below, we will create some pre-implemented run evaluators that do the\nfollowing: - Compare results against ground truth labels. - Measure\nsemantic (dis)similarity using embedding distance - Evaluate ‘aspects’\nof the agent’s response in a reference-free manner using custom criteria For a longer discussion of how to select an appropriate evaluator for\nyour use case and how to create your own custom evaluators, please refer\nto the LangSmith documentation . from langchain . evaluation import EvaluatorType from langchain . smith import RunEvalConfig evaluation_config = RunEvalConfig ( # Evaluators can either be an evaluator type (e.g., \"qa\", \"criteria\", \"embedding_distance\", etc.) or a configuration for that evaluator evaluators = [ # Measures whether a QA response is \"Correct\", based on a reference answer # You can also select via the raw string \"qa\" EvaluatorType . QA , # Measure the embedding distance between the output and the reference answer # Equivalent to: EvalConfig.EmbeddingDistance(embeddings=OpenAIEmbeddings()) EvaluatorType . EMBEDDING_DISTANCE , # Grade whether the output satisfies the stated criteria. # You can select a default one such as \"helpfulness\" or provide your own. RunEvalConfig . LabeledCriteria ( \"helpfulness\" ) , # The LabeledScoreString evaluator outputs a score on a scale from 1-10. # You can use default criteria or write our own rubric RunEvalConfig . LabeledScoreString ( { \"accuracy\" : \"\"\" Score 1: The answer is completely unrelated to the reference. Score 3: The answer has minor relevance but does not align with the reference. Score 5: The answer has moderate relevance but contains inaccuracies. Score 7: The answer aligns with the reference but has minor errors or omissions. Score 10: The answer is completely accurate and aligns perfectly with the reference.\"\"\" } , normalize_by = 10 , ) , ] , # You can add custom StringEvaluator or RunEvaluator objects here as well, which will automatically be # applied to each prediction. Check out the docs for examples. custom_evaluators = [ ] , ) 4. Run the agent and evaluators ​ Use the run_on_dataset (or asynchronous arun_on_dataset )\nfunction to evaluate your model. This will: 1. Fetch example rows from\nthe specified dataset. 2. Run your agent (or any custom function) on\neach example. 3. Apply evaluators to the resulting run traces and\ncorresponding reference examples to generate automated feedback. The results will be visible in the LangSmith app. from langchain import hub # We will test this version of the prompt prompt = hub . pull ( \"wfh/langsmith-agent-prompt:798e7324\" ) import functools from langchain . smith import ( arun_on_dataset , run_on_dataset , ) chain_results = run_on_dataset ( dataset_name = dataset_name , llm_or_chain_factory = functools . partial ( agent_factory , prompt = prompt ) , evaluation = evaluation_config , verbose = True , client = client , project_name = f\"runnable-agent-test-5d466cbc- { unique_id } \" , tags = [ \"testing-notebook\" , \"prompt:5d466cbc\" , ] , # Optional, adds a tag to the resulting chain runs ) # Sometimes, the agent will error due to parsing issues, incompatible tool inputs, etc. # These are logged as warnings here and captured as errors in the tracing UI. View the evaluation results for project 'runnable-agent-test-5d466cbc-bf2162aa' at: https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/0c3d22fa-f8b0-4608-b086-2187c18361a5 [>                                                 ] 0/5[------------------------------------------------->] 5/5 Eval quantiles: 0.25       0.5      0.75      mean      mode embedding_cosine_distance  0.086614  0.118841  0.183672  0.151444  0.050158 correctness                0.000000  0.500000  1.000000  0.500000  0.000000 score_string:accuracy      0.775000  1.000000  1.000000  0.775000  1.000000 helpfulness                0.750000  1.000000  1.000000  0.750000  1.000000 Chain failed for example 54b4fce8-4492-409d-94af-708f51698b39 with inputs {'input': 'Who trained Llama-v2?'} Error Type: TypeError, Message: DuckDuckGoSearchResults._run() got an unexpected keyword argument 'arg1' Review the test results ​ You can review the test results tracing UI below by clicking the URL in\nthe output above or navigating to the “Testing & Datasets” page in\nLangSmith “agent-qa-{unique_id}” dataset. This will show the new runs and the feedback logged from the selected\nevaluators. You can also explore a summary of the results in tabular\nformat below. chain_results . to_dataframe ( ) embedding_cosine_distance correctness score_string:accuracy helpfulness input output reference 42b639a2-17c4-4031-88a9-0ce2c45781ce 0.317938 0.0 1.0 1.0 {'input': 'What is the langsmith cookbook?'} {'input': 'What is the langsmith cookbook?', '... {'output': 'September 5, 2023'} 54b4fce8-4492-409d-94af-708f51698b39 NaN NaN NaN NaN {'input': 'Who trained Llama-v2?'} {'Error': 'TypeError(\"DuckDuckGoSearchResults.... {'output': 'The langsmith cookbook is a github... 8ae5104e-bbb4-42cc-a84e-f9b8cfc92b8e 0.138916 1.0 1.0 1.0 {'input': 'When was Llama-v2 released?'} {'input': 'When was Llama-v2 released?', 'outp... {'output': 'July 18, 2023'} 678c0363-3ed1-410a-811f-ebadef2e783a 0.050158 1.0 1.0 1.0 {'input': 'What's LangSmith?'} {'input': 'What's LangSmith?', 'output': 'Lang... {'output': 'LangSmith is a unified platform fo... 762a616c-7aab-419c-9001-b43ab6200d26 0.098766 0.0 0.1 0.0 {'input': 'What is LangChain?'} {'input': 'What is LangChain?', 'output': 'Lan... {'output': 'LangChain is an open-source framew... (Optional) Compare to another prompt ​ Now that we have our test run results, we can make changes to our agent\nand benchmark them. Let’s try this again with a different prompt and see\nthe results. candidate_prompt = hub . pull ( \"wfh/langsmith-agent-prompt:39f3bbd0\" ) chain_results = run_on_dataset ( dataset_name = dataset_name , llm_or_chain_factory = functools . partial ( agent_factory , prompt = candidate_prompt ) , evaluation = evaluation_config , verbose = True , client = client , project_name = f\"runnable-agent-test-39f3bbd0- { unique_id } \" , tags = [ \"testing-notebook\" , \"prompt:39f3bbd0\" , ] , # Optional, adds a tag to the resulting chain runs ) View the evaluation results for project 'runnable-agent-test-39f3bbd0-bf2162aa' at: https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/fa721ccc-dd0f-41c9-bf80-22215c44efd4 [------------------------------------------------->] 5/5 Eval quantiles: 0.25       0.5      0.75      mean      mode embedding_cosine_distance  0.059506  0.155538  0.212864  0.157915  0.043119 correctness                0.000000  0.000000  1.000000  0.400000  0.000000 score_string:accuracy      0.700000  1.000000  1.000000  0.880000  1.000000 helpfulness                1.000000  1.000000  1.000000  0.800000  1.000000 Exporting datasets and runs ​ LangSmith lets you export data to common formats such as CSV or JSONL\ndirectly in the web app. You can also use the client to fetch runs for\nfurther analysis, to store in your own database, or to share with\nothers. Let’s fetch the run traces from the evaluation run. Note: It may be a few moments before all the runs are accessible. runs = client . list_runs ( project_name = chain_results [ \"project_name\" ] , execution_order = 1 ) # After some time, these will be populated. client . read_project ( project_name = chain_results [ \"project_name\" ] ) . feedback_stats Conclusion ​ Congratulations! You have successfully traced and evaluated an agent\nusing LangSmith! This was a quick guide to get started, but there are many more ways to\nuse LangSmith to speed up your developer flow and produce better\nresults. For more information on how you can get the most out of LangSmith, check\nout LangSmith documentation , and\nplease reach out with questions, feature requests, or feedback at support@langchain.dev . Previous LangSmith"
    },
    {
        "title": "RAG over code | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/question_answering/code_understanding",
        "text_content": "Retrieval-augmented generation (RAG) RAG over code On this page RAG over code Open In Collab Use case ​ Source code analysis is one of the most popular LLM applications (e.g., GitHub Co-Pilot , Code\nInterpreter , Codium , and Codeium ) for use-cases such as: Q&A over the code base to understand how it works Using LLMs for suggesting refactors or improvements Using LLMs for documenting the code Overview ​ The pipeline for QA over code follows the steps we do for document\nquestion answering , with some\ndifferences: In particular, we can employ a splitting\nstrategy that does a few things: Keeps each top-level function and class in the code is loaded into\nseparate documents. Puts remaining into a separate document. Retains metadata about where each split comes from Quickstart ​ !pip install openai tiktoken chromadb langchain # Set env var OPENAI_API_KEY or load from a .env file # import dotenv # dotenv.load_dotenv() We’ll follow the structure of this\nnotebook and employ context aware code\nsplitting . Loading ​ We will upload all python project files using the langchain.document_loaders.TextLoader . The following script iterates over the files in the LangChain repository\nand loads every .py file (a.k.a. documents ): # from git import Repo from langchain . document_loaders . generic import GenericLoader from langchain . document_loaders . parsers import LanguageParser from langchain . text_splitter import Language # Clone repo_path = \"/Users/rlm/Desktop/test_repo\" # repo = Repo.clone_from(\"https://github.com/langchain-ai/langchain\", to_path=repo_path) We load the py code using LanguageParser ,\nwhich will: Keep top-level functions and classes together (into a single\ndocument) Put remaining code into a separate document Retains metadata about where each split comes from # Load loader = GenericLoader . from_filesystem ( repo_path + \"/libs/langchain/langchain\" , glob = \"**/*\" , suffixes = [ \".py\" ] , parser = LanguageParser ( language = Language . PYTHON , parser_threshold = 500 ) , ) documents = loader . load ( ) len ( documents ) 1293 Splitting ​ Split the Document into chunks for embedding and vector storage. We can use RecursiveCharacterTextSplitter w/ language specified. from langchain . text_splitter import RecursiveCharacterTextSplitter python_splitter = RecursiveCharacterTextSplitter . from_language ( language = Language . PYTHON , chunk_size = 2000 , chunk_overlap = 200 ) texts = python_splitter . split_documents ( documents ) len ( texts ) 3748 RetrievalQA ​ We need to store the documents in a way we can semantically search for\ntheir content. The most common approach is to embed the contents of each document then\nstore the embedding and document in a vector store. When setting up the vectorstore retriever: We test max marginal\nrelevance for retrieval And 8 documents returned Go deeper ​ Browse the > 40 vectorstores integrations here . See further documentation on vectorstores here . Browse the > 30 text embedding integrations here . See further documentation on embedding models here . from langchain . embeddings . openai import OpenAIEmbeddings from langchain . vectorstores import Chroma db = Chroma . from_documents ( texts , OpenAIEmbeddings ( disallowed_special = ( ) ) ) retriever = db . as_retriever ( search_type = \"mmr\" , # Also test \"similarity\" search_kwargs = { \"k\" : 8 } , ) Chat ​ Test chat, just as we do for chatbots . Go deeper ​ Browse the > 55 LLM and chat model integrations here . See further documentation on LLMs and chat models here . Use local LLMS: The popularity of PrivateGPT and GPT4All underscore the\nimportance of running LLMs locally. from langchain . chains import ConversationalRetrievalChain from langchain . chat_models import ChatOpenAI from langchain . memory import ConversationSummaryMemory llm = ChatOpenAI ( model_name = \"gpt-4\" ) memory = ConversationSummaryMemory ( llm = llm , memory_key = \"chat_history\" , return_messages = True ) qa = ConversationalRetrievalChain . from_llm ( llm , retriever = retriever , memory = memory ) question = \"How can I initialize a ReAct agent?\" result = qa ( question ) result [ \"answer\" ] 'To initialize a ReAct agent, you need to follow these steps:\\n\\n1. Initialize a language model `llm` of type `BaseLanguageModel`.\\n\\n2. Initialize a document store `docstore` of type `Docstore`.\\n\\n3. Create a `DocstoreExplorer` with the initialized `docstore`. The `DocstoreExplorer` is used to search for and look up terms in the document store.\\n\\n4. Create an array of `Tool` objects. The `Tool` objects represent the actions that the agent can perform. In the case of `ReActDocstoreAgent`, the tools must be \"Search\" and \"Lookup\" with their corresponding functions from the `DocstoreExplorer`.\\n\\n5. Initialize the `ReActDocstoreAgent` using the `from_llm_and_tools` method with the `llm` (language model) and `tools` as parameters.\\n\\n6. Initialize the `ReActChain` (which is the `AgentExecutor`) using the `ReActDocstoreAgent` and `tools` as parameters.\\n\\nHere is an example of how to do this:\\n\\n```python\\nfrom langchain.chains import ReActChain, OpenAI\\nfrom langchain.docstore.base import Docstore\\nfrom langchain.docstore.document import Document\\nfrom langchain_core.tools import BaseTool\\n\\n# Initialize the LLM and a docstore\\nllm = OpenAI()\\ndocstore = Docstore()\\n\\ndocstore_explorer = DocstoreExplorer(docstore)\\ntools = [\\n    Tool(\\n        name=\"Search\",\\n        func=docstore_explorer.search,\\n        description=\"Search for a term in the docstore.\",\\n    ),\\n    Tool(\\n        name=\"Lookup\",\\n        func=docstore_explorer.lookup,\\n        description=\"Lookup a term in the docstore.\",\\n    ),\\n]\\nagent = ReActDocstoreAgent.from_llm_and_tools(llm, tools)\\nreact = ReActChain(agent=agent, tools=tools)\\n```\\n\\nKeep in mind that this is a simplified example and you might need to adapt it to your specific needs.' questions = [ \"What is the class hierarchy?\" , \"What classes are derived from the Chain class?\" , \"What one improvement do you propose in code in relation to the class hierarchy for the Chain class?\" , ] for question in questions : result = qa ( question ) print ( f\"-> **Question**: { question } \\n\" ) print ( f\"**Answer**: { result [ 'answer' ] } \\n\" ) -> **Question**: What is the class hierarchy? **Answer**: The class hierarchy in object-oriented programming is the structure that forms when classes are derived from other classes. The derived class is a subclass of the base class also known as the superclass. This hierarchy is formed based on the concept of inheritance in object-oriented programming where a subclass inherits the properties and functionalities of the superclass. In the given context, we have the following examples of class hierarchies: 1. `BaseCallbackHandler --> <name>CallbackHandler` means `BaseCallbackHandler` is a base class and `<name>CallbackHandler` (like `AimCallbackHandler`, `ArgillaCallbackHandler` etc.) are derived classes that inherit from `BaseCallbackHandler`. 2. `BaseLoader --> <name>Loader` means `BaseLoader` is a base class and `<name>Loader` (like `TextLoader`, `UnstructuredFileLoader` etc.) are derived classes that inherit from `BaseLoader`. 3. `ToolMetaclass --> BaseTool --> <name>Tool` means `ToolMetaclass` is a base class, `BaseTool` is a derived class that inherits from `ToolMetaclass`, and `<name>Tool` (like `AIPluginTool`, `BaseGraphQLTool` etc.) are further derived classes that inherit from `BaseTool`. -> **Question**: What classes are derived from the Chain class? **Answer**: The classes that are derived from the Chain class are: 1. LLMSummarizationCheckerChain 2. MapReduceChain 3. OpenAIModerationChain 4. NatBotChain 5. QAGenerationChain 6. QAWithSourcesChain 7. RetrievalQAWithSourcesChain 8. VectorDBQAWithSourcesChain 9. RetrievalQA 10. VectorDBQA 11. LLMRouterChain 12. MultiPromptChain 13. MultiRetrievalQAChain 14. MultiRouteChain 15. RouterChain 16. SequentialChain 17. SimpleSequentialChain 18. TransformChain 19. BaseConversationalRetrievalChain 20. ConstitutionalChain -> **Question**: What one improvement do you propose in code in relation to the class hierarchy for the Chain class? **Answer**: As an AI model, I don't have personal opinions. However, one suggestion could be to improve the documentation of the Chain class hierarchy. The current comments and docstrings provide some details but it could be helpful to include more explicit explanations about the hierarchy, roles of each subclass, and their relationships with one another. Also, incorporating UML diagrams or other visuals could help developers better understand the structure and interactions of the classes. The can look at the LangSmith\ntrace to see what is happening under the hood: In particular, the code well structured and kept together in the\nretrieval output The retrieved code and chat history are passed to the LLM for answer\ndistillation Open source LLMs ​ We can use Code\nLLaMA via\nLLamaCPP or Ollama\nintegration . Note: be sure to upgrade llama-cpp-python in order to use the new gguf file\nformat . CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 /Users/rlm/miniforge3/envs/llama2/bin/pip install -U llama-cpp-python --no-cache-dir Check out the latest code-llama models here . from langchain . callbacks . manager import CallbackManager from langchain . callbacks . streaming_stdout import StreamingStdOutCallbackHandler from langchain . chains import ConversationalRetrievalChain , LLMChain from langchain . llms import LlamaCpp from langchain . memory import ConversationSummaryMemory from langchain . prompts import PromptTemplate callback_manager = CallbackManager ( [ StreamingStdOutCallbackHandler ( ) ] ) llm = LlamaCpp ( model_path = \"/Users/rlm/Desktop/Code/llama/code-llama/codellama-13b-instruct.Q4_K_M.gguf\" , n_ctx = 5000 , n_gpu_layers = 1 , n_batch = 512 , f16_kv = True , # MUST set to True, otherwise you will run into problem after a couple of calls callback_manager = callback_manager , verbose = True , ) llama_model_loader: loaded meta data with 17 key-value pairs and 363 tensors from /Users/rlm/Desktop/Code/llama/code-llama/codellama-13b-instruct.Q4_K_M.gguf (version GGUF V1 (latest)) llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  5120, 32016,     1,     1 ] llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor    2:                    output.weight f16      [  5120, 32016,     1,     1 ] llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor    5:              blk.0.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   14:              blk.1.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   23:              blk.2.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   32:              blk.3.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   41:              blk.4.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   68:              blk.7.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   95:             blk.10.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  122:             blk.13.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  149:             blk.16.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  176:             blk.19.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  203:             blk.22.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  230:             blk.25.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  257:             blk.28.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  284:             blk.31.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  311:             blk.34.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  320:             blk.35.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  329:             blk.36.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  338:             blk.37.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  347:             blk.38.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  356:             blk.39.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - kv   0:                       general.architecture str llama_model_loader: - kv   1:                               general.name str llama_model_loader: - kv   2:                       llama.context_length u32 llama_model_loader: - kv   3:                     llama.embedding_length u32 llama_model_loader: - kv   4:                          llama.block_count u32 llama_model_loader: - kv   5:                  llama.feed_forward_length u32 llama_model_loader: - kv   6:                 llama.rope.dimension_count u32 llama_model_loader: - kv   7:                 llama.attention.head_count u32 llama_model_loader: - kv   8:              llama.attention.head_count_kv u32 llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32 llama_model_loader: - kv  10:                       llama.rope.freq_base f32 llama_model_loader: - kv  11:                          general.file_type u32 llama_model_loader: - kv  12:                       tokenizer.ggml.model str llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr llama_model_loader: - kv  16:               general.quantization_version u32 llama_model_loader: - type  f32:   81 tensors llama_model_loader: - type  f16:    1 tensors llama_model_loader: - type q4_0:    1 tensors llama_model_loader: - type q4_K:  240 tensors llama_model_loader: - type q6_K:   40 tensors llm_load_print_meta: format         = GGUF V1 (latest) llm_load_print_meta: arch           = llama llm_load_print_meta: vocab type     = SPM llm_load_print_meta: n_vocab        = 32016 llm_load_print_meta: n_merges       = 0 llm_load_print_meta: n_ctx_train    = 16384 llm_load_print_meta: n_ctx          = 5000 llm_load_print_meta: n_embd         = 5120 llm_load_print_meta: n_head         = 40 llm_load_print_meta: n_head_kv      = 40 llm_load_print_meta: n_layer        = 40 llm_load_print_meta: n_rot          = 128 llm_load_print_meta: n_gqa          = 1 llm_load_print_meta: f_norm_eps     = 1.0e-05 llm_load_print_meta: f_norm_rms_eps = 1.0e-05 llm_load_print_meta: n_ff           = 13824 llm_load_print_meta: freq_base      = 1000000.0 llm_load_print_meta: freq_scale     = 1 llm_load_print_meta: model type     = 13B llm_load_print_meta: model ftype    = mostly Q4_K - Medium llm_load_print_meta: model size     = 13.02 B llm_load_print_meta: general.name   = LLaMA llm_load_print_meta: BOS token = 1 '<s>' llm_load_print_meta: EOS token = 2 '</s>' llm_load_print_meta: UNK token = 0 '<unk>' llm_load_print_meta: LF token  = 13 '<0x0A>' llm_load_tensors: ggml ctx size =    0.11 MB llm_load_tensors: mem required  = 7685.49 MB (+ 3906.25 MB per state) ................................................................................................. llama_new_context_with_model: kv self size  = 3906.25 MB ggml_metal_init: allocating ggml_metal_init: loading '/Users/rlm/miniforge3/envs/llama2/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal' ggml_metal_init: loaded kernel_add                            0x12126dd00 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_add_row                        0x12126d610 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_mul                            0x12126f2a0 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_mul_row                        0x12126f500 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_scale                          0x12126f760 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_silu                           0x12126fe40 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_relu                           0x1212700a0 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_gelu                           0x121270300 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_soft_max                       0x121270560 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_diag_mask_inf                  0x1212707c0 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_get_rows_f16                   0x121270a20 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_get_rows_q4_0                  0x121270c80 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_get_rows_q4_1                  0x121270ee0 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_get_rows_q8_0                  0x121271140 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_get_rows_q2_K                  0x1212713a0 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_get_rows_q3_K                  0x121271600 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_get_rows_q4_K                  0x121271860 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_get_rows_q5_K                  0x121271ac0 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_get_rows_q6_K                  0x121271d20 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_rms_norm                       0x121271f80 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_norm                           0x1212721e0 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x121272440 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1212726a0 | th_max =  896 | th_width =   32 ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x121272900 | th_max =  896 | th_width =   32 ggml_metal_init: loaded kernel_mul_mat_q8_0_f32               0x121272b60 | th_max =  768 | th_width =   32 ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x121272dc0 | th_max =  640 | th_width =   32 ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x121273020 | th_max =  704 | th_width =   32 ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x121273280 | th_max =  576 | th_width =   32 ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1212734e0 | th_max =  576 | th_width =   32 ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x121273740 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x1212739a0 | th_max =  768 | th_width =   32 ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x121273c00 | th_max =  768 | th_width =   32 ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                0x121273e60 | th_max =  768 | th_width =   32 ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x1212740c0 | th_max =  768 | th_width =   32 ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x121274320 | th_max =  768 | th_width =   32 ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x121274580 | th_max =  768 | th_width =   32 ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x1212747e0 | th_max =  768 | th_width =   32 ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x121274a40 | th_max =  704 | th_width =   32 ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x121274ca0 | th_max =  704 | th_width =   32 ggml_metal_init: loaded kernel_rope                           0x121274f00 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_alibi_f32                      0x121275160 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1212753c0 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_cpy_f32_f32                    0x121275620 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_cpy_f16_f16                    0x121275880 | th_max = 1024 | th_width =   32 ggml_metal_init: recommendedMaxWorkingSetSize  = 21845.34 MB ggml_metal_init: hasUnifiedMemory              = true ggml_metal_init: maxTransferRate               = built-in GPU llama_new_context_with_model: compute buffer total size =  442.03 MB llama_new_context_with_model: max tensor size =   312.66 MB ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7686.00 MB, (20243.77 / 21845.34) ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.42 MB, (20245.19 / 21845.34) ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  3908.25 MB, (24153.44 / 21845.34), warning: current allocated size is greater than the recommended max working set size AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   440.64 MB, (24594.08 / 21845.34), warning: current allocated size is greater than the recommended max working set size llm ( \"Question: In bash, how do I list all the text files in the current directory that have been modified in the last month? Answer:\" ) Llama.generate: prefix-match hit llama_print_timings:        load time =  1074.43 ms llama_print_timings:      sample time =   180.71 ms /   256 runs   (    0.71 ms per token,  1416.67 tokens per second) llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second) llama_print_timings:        eval time =  9593.04 ms /   256 runs   (   37.47 ms per token,    26.69 tokens per second) llama_print_timings:       total time = 10139.91 ms You can use the find command with a few options to this task. Here is an example of how you might go about it: find . -type f -mtime +28 -exec ls {} \\; This command only for plain files (not), and limits the search to files that were more than 28 days ago, then the \"ls\" command on each file found. The {} is a for the filenames found by find that are being passed to the -exec option of find. You can also use find in with other unix utilities like sort and grep to the list of files before they are: find . -type f -mtime +28 | sort | grep pattern This will find all plain files that match a given pattern, then sort the listically and filter it for only the matches. Answer: `find` is pretty with its search. The should work as well: \\begin{code} ls -l $(find . -mtime +28) \\end{code} (It's a bad idea to parse output from `ls`, though, as you may ' You can use the find command with a few options to this task. Here is an example of how you might go about it:\\n\\nfind . -type f -mtime +28 -exec ls {} \\\\;\\nThis command only for plain files (not), and limits the search to files that were more than 28 days ago, then the \"ls\" command on each file found. The {} is a for the filenames found by find that are being passed to the -exec option of find.\\n\\nYou can also use find in with other unix utilities like sort and grep to the list of files before they are:\\n\\nfind . -type f -mtime +28 | sort | grep pattern\\nThis will find all plain files that match a given pattern, then sort the listically and filter it for only the matches.\\n\\nAnswer: `find` is pretty with its search. The should work as well:\\n\\n\\\\begin{code}\\nls -l $(find . -mtime +28)\\n\\\\end{code}\\n\\n(It\\'s a bad idea to parse output from `ls`, though, as you may' from langchain . chains . question_answering import load_qa_chain # Prompt template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum and keep the answer as concise as possible. {context} Question: {question} Helpful Answer:\"\"\" QA_CHAIN_PROMPT = PromptTemplate ( input_variables = [ \"context\" , \"question\" ] , template = template , ) We can also use the LangChain Prompt Hub to store and fetch prompts. This will work with your LangSmith API\nkey . Let’s try with a default RAG prompt, here . from langchain import hub QA_CHAIN_PROMPT = hub . pull ( \"rlm/rag-prompt-default\" ) # Docs question = \"How can I initialize a ReAct agent?\" docs = retriever . get_relevant_documents ( question ) # Chain chain = load_qa_chain ( llm , chain_type = \"stuff\" , prompt = QA_CHAIN_PROMPT ) # Run chain ( { \"input_documents\" : docs , \"question\" : question } , return_only_outputs = True ) Llama.generate: prefix-match hit llama_print_timings:        load time =  1074.43 ms llama_print_timings:      sample time =    65.46 ms /    94 runs   (    0.70 ms per token,  1435.95 tokens per second) llama_print_timings: prompt eval time = 15975.57 ms /  1408 tokens (   11.35 ms per token,    88.13 tokens per second) llama_print_timings:        eval time =  4772.57 ms /    93 runs   (   51.32 ms per token,    19.49 tokens per second) llama_print_timings:       total time = 20959.57 ms You can use the `ReActAgent` class and pass it the desired tools as, for example, you would do like this to create an agent with the `Lookup` and `Search` tool: ```python from langchain.agents.react import ReActAgent from langchain.tools.lookup import Lookup from langchain.tools.search import Search ReActAgent(Lookup(), Search()) ``` {'output_text': ' You can use the `ReActAgent` class and pass it the desired tools as, for example, you would do like this to create an agent with the `Lookup` and `Search` tool:\\n```python\\nfrom langchain.agents.react import ReActAgent\\nfrom langchain.tools.lookup import Lookup\\nfrom langchain.tools.search import Search\\nReActAgent(Lookup(), Search())\\n```'} Here’s the trace RAG ,\nshowing the retrieved docs. Previous Retrieval-augmented generation (RAG) Next RAG with Agents"
    },
    {
        "title": "RAG with Agents | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents",
        "text_content": "Retrieval-augmented generation (RAG) RAG with Agents On this page RAG with Agents This is an agent specifically optimized for doing retrieval when\nnecessary and also holding a conversation. To start, we will set up the retriever we want to use, and then turn it\ninto a retriever tool. Next, we will use the high level constructor for\nthis type of agent. Finally, we will walk through how to construct a\nconversational retrieval agent from components. The Retriever ​ To start, we need a retriever to use! The code here is mostly just\nexample code. Feel free to use your own retriever and skip to the\nsection on creating a retriever tool. from langchain . document_loaders import TextLoader loader = TextLoader ( \"../../../../../docs/docs/modules/state_of_the_union.txt\" ) from langchain . embeddings import OpenAIEmbeddings from langchain . text_splitter import CharacterTextSplitter from langchain . vectorstores import FAISS documents = loader . load ( ) text_splitter = CharacterTextSplitter ( chunk_size = 1000 , chunk_overlap = 0 ) texts = text_splitter . split_documents ( documents ) embeddings = OpenAIEmbeddings ( ) db = FAISS . from_documents ( texts , embeddings ) retriever = db . as_retriever ( ) Retriever Tool ​ Now we need to create a tool for our retriever. The main things we need\nto pass in are a name for the retriever as well as a description. These\nwill both be used by the language model, so they should be informative. from langchain . agents . agent_toolkits import create_retriever_tool tool = create_retriever_tool ( retriever , \"search_state_of_union\" , \"Searches and returns documents regarding the state-of-the-union.\" , ) tools = [ tool ] Agent Constructor ​ Here, we will use the high level create_conversational_retrieval_agent API to construct the agent. Notice that beside the list of tools, the only thing we need to pass in\nis a language model to use. Under the hood, this agent is using the\nOpenAIFunctionsAgent, so we need to use an ChatOpenAI model. from langchain . agents . agent_toolkits import create_conversational_retrieval_agent from langchain . chat_models import ChatOpenAI llm = ChatOpenAI ( temperature = 0 ) agent_executor = create_conversational_retrieval_agent ( llm , tools , verbose = True ) We can now try it out! result = agent_executor ( { \"input\" : \"hi, im bob\" } ) > Entering new AgentExecutor chain... Hello Bob! How can I assist you today? > Finished chain. result [ \"output\" ] 'Hello Bob! How can I assist you today?' Notice that it remembers your name result = agent_executor ( { \"input\" : \"whats my name?\" } ) > Entering new AgentExecutor chain... Your name is Bob. > Finished chain. result [ \"output\" ] 'Your name is Bob.' Notice that it now does retrieval result = agent_executor ( { \"input\" : \"what did the president say about kentaji brown jackson in the most recent state of the union?\" } ) > Entering new AgentExecutor chain... Invoking: `search_state_of_union` with `{'query': 'Kentaji Brown Jackson'}` [Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': '../../../../../docs/docs/modules/state_of_the_union.txt'}), Document(page_content='One was stationed at bases and breathing in toxic smoke from “burn pits” that incinerated wastes of war—medical and hazard material, jet fuel, and more. \\n\\nWhen they came home, many of the world’s fittest and best trained warriors were never the same. \\n\\nHeadaches. Numbness. Dizziness. \\n\\nA cancer that would put them in a flag-draped coffin. \\n\\nI know. \\n\\nOne of those soldiers was my son Major Beau Biden. \\n\\nWe don’t know for sure if a burn pit was the cause of his brain cancer, or the diseases of so many of our troops. \\n\\nBut I’m committed to finding out everything we can. \\n\\nCommitted to military families like Danielle Robinson from Ohio. \\n\\nThe widow of Sergeant First Class Heath Robinson.  \\n\\nHe was born a soldier. Army National Guard. Combat medic in Kosovo and Iraq. \\n\\nStationed near Baghdad, just yards from burn pits the size of football fields. \\n\\nHeath’s widow Danielle is here with us tonight. They loved going to Ohio State football games. He loved building Legos with their daughter.', metadata={'source': '../../../../../docs/docs/modules/state_of_the_union.txt'}), Document(page_content='A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\n\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \\n\\nWe can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \\n\\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \\n\\nWe’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \\n\\nWe’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.', metadata={'source': '../../../../../docs/docs/modules/state_of_the_union.txt'}), Document(page_content='We can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \\n\\nI’ve worked on these issues a long time. \\n\\nI know what works: Investing in crime prevention and community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety.', metadata={'source': '../../../../../docs/docs/modules/state_of_the_union.txt'})]In the most recent state of the union, the President mentioned Kentaji Brown Jackson. The President nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to serve on the United States Supreme Court. The President described Judge Ketanji Brown Jackson as one of our nation's top legal minds who will continue Justice Breyer's legacy of excellence. > Finished chain. result [ \"output\" ] \"In the most recent state of the union, the President mentioned Kentaji Brown Jackson. The President nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to serve on the United States Supreme Court. The President described Judge Ketanji Brown Jackson as one of our nation's top legal minds who will continue Justice Breyer's legacy of excellence.\" Notice that the follow up question asks about information previously\nretrieved, so no need to do another retrieval result = agent_executor ( { \"input\" : \"how long ago did he nominate her?\" } ) > Entering new AgentExecutor chain... The President nominated Judge Ketanji Brown Jackson four days ago. > Finished chain. result [ \"output\" ] 'The President nominated Judge Ketanji Brown Jackson four days ago.' Creating from components ​ What actually is going on underneath the hood? Let’s take a look so we\ncan understand how to modify going forward. There are a few components: The memory The prompt template The agent The agent executor # This is needed for both the memory and the prompt memory_key = \"history\" The Memory ​ In this example, we want the agent to remember not only previous\nconversations, but also previous intermediate steps. For that, we can\nuse AgentTokenBufferMemory . Note that if you want to change whether\nthe agent remembers intermediate steps, or how the long the buffer is,\nor anything like that you should change this part. from langchain . agents . openai_functions_agent . agent_token_buffer_memory import ( AgentTokenBufferMemory , ) memory = AgentTokenBufferMemory ( memory_key = memory_key , llm = llm ) The Prompt Template ​ For the prompt template, we will use the OpenAIFunctionsAgent default\nway of creating one, but pass in a system prompt and a placeholder for\nmemory. from langchain . agents . openai_functions_agent . base import OpenAIFunctionsAgent from langchain . prompts import MessagesPlaceholder from langchain_core . messages import SystemMessage system_message = SystemMessage ( content = ( \"Do your best to answer the questions. \" \"Feel free to use any tools available to look up \" \"relevant information, only if necessary\" ) ) prompt = OpenAIFunctionsAgent . create_prompt ( system_message = system_message , extra_prompt_messages = [ MessagesPlaceholder ( variable_name = memory_key ) ] , ) The Agent ​ We will use the OpenAIFunctionsAgent agent = OpenAIFunctionsAgent ( llm = llm , tools = tools , prompt = prompt ) The Agent Executor ​ Importantly, we pass in return_intermediate_steps=True since we are\nrecording that with our memory object from langchain . agents import AgentExecutor agent_executor = AgentExecutor ( agent = agent , tools = tools , memory = memory , verbose = True , return_intermediate_steps = True , ) result = agent_executor ( { \"input\" : \"hi, im bob\" } ) > Entering new AgentExecutor chain... Hello Bob! How can I assist you today? > Finished chain. result = agent_executor ( { \"input\" : \"whats my name\" } ) > Entering new AgentExecutor chain... Your name is Bob. > Finished chain. Previous RAG over code Next Text splitting by header"
    },
    {
        "title": "Text splitting by header | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/question_answering/document-context-aware-QA",
        "text_content": "Retrieval-augmented generation (RAG) Text splitting by header Text splitting by header Text splitting for vector storage often uses sentences or other\ndelimiters to keep related text\ntogether . But many documents (such as Markdown files) have structure (headers)\nthat can be explicitly used in splitting. The MarkdownHeaderTextSplitter lets a user split Markdown files\nfiles based on specified headers. This results in chunks that retain the header(s) that it came from in\nthe metadata. This works nicely w/ SelfQueryRetriever . First, tell the retriever about our splits. Then, query based on the doc structure (e.g., “summarize the doc\nintroduction”). Chunks only from that section of the Document will be filtered and used\nin chat / Q+A. Let’s test this out on an example Notion\npage ! First, I download the page to Markdown as explained here . # Load Notion page as a markdownfile file from langchain . document_loaders import NotionDirectoryLoader path = \"../Notion_DB/\" loader = NotionDirectoryLoader ( path ) docs = loader . load ( ) md_file = docs [ 0 ] . page_content # Let's create groups based on the section headers in our page from langchain . text_splitter import MarkdownHeaderTextSplitter headers_to_split_on = [ ( \"###\" , \"Section\" ) , ] markdown_splitter = MarkdownHeaderTextSplitter ( headers_to_split_on = headers_to_split_on ) md_header_splits = markdown_splitter . split_text ( md_file ) Now, perform text splitting on the header grouped documents. # Define our text splitter from langchain . text_splitter import RecursiveCharacterTextSplitter chunk_size = 500 chunk_overlap = 0 text_splitter = RecursiveCharacterTextSplitter ( chunk_size = chunk_size , chunk_overlap = chunk_overlap ) all_splits = text_splitter . split_documents ( md_header_splits ) This sets us up well do perform metadata filtering based on the document\nstructure. Let’s bring this all together by building a vectorstore first. ! pip install chromadb # Build vectorstore and keep the metadata from langchain . embeddings import OpenAIEmbeddings from langchain . vectorstores import Chroma vectorstore = Chroma . from_documents ( documents = all_splits , embedding = OpenAIEmbeddings ( ) ) Let’s create a SelfQueryRetriever that can filter based upon metadata\nwe defined. # Create retriever from langchain . chains . query_constructor . base import AttributeInfo from langchain . llms import OpenAI from langchain . retrievers . self_query . base import SelfQueryRetriever # Define our metadata metadata_field_info = [ AttributeInfo ( name = \"Section\" , description = \"Part of the document that the text comes from\" , type = \"string or list[string]\" , ) , ] document_content_description = \"Major sections of the document\" # Define self query retriever llm = OpenAI ( temperature = 0 ) retriever = SelfQueryRetriever . from_llm ( llm , vectorstore , document_content_description , metadata_field_info , verbose = True ) We can see that we can query only for texts in the Introduction of\nthe document! # Test retriever . get_relevant_documents ( \"Summarize the Introduction section of the document\" ) query='Introduction' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='Section', value='Introduction') limit=None [Document(page_content='![Untitled](Auto-Evaluation%20of%20Metadata%20Filtering%2018502448c85240828f33716740f9574b/Untitled.png)', metadata={'Section': 'Introduction'}), Document(page_content='Q+A systems often use a two-step approach: retrieve relevant text chunks and then synthesize them into an answer. There many ways to approach this. For example, we recently [discussed](https://blog.langchain.dev/auto-evaluation-of-anthropic-100k-context-window/) the Retriever-Less option (at bottom in the below diagram), highlighting the Anthropic 100k context window model. Metadata filtering is an alternative approach that pre-filters chunks based on a user-defined criteria in a VectorDB using', metadata={'Section': 'Introduction'}), Document(page_content='metadata tags prior to semantic search.', metadata={'Section': 'Introduction'})] # Test retriever . get_relevant_documents ( \"Summarize the Introduction section of the document\" ) query='Introduction' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='Section', value='Introduction') limit=None [Document(page_content='![Untitled](Auto-Evaluation%20of%20Metadata%20Filtering%2018502448c85240828f33716740f9574b/Untitled.png)', metadata={'Section': 'Introduction'}), Document(page_content='Q+A systems often use a two-step approach: retrieve relevant text chunks and then synthesize them into an answer. There many ways to approach this. For example, we recently [discussed](https://blog.langchain.dev/auto-evaluation-of-anthropic-100k-context-window/) the Retriever-Less option (at bottom in the below diagram), highlighting the Anthropic 100k context window model. Metadata filtering is an alternative approach that pre-filters chunks based on a user-defined criteria in a VectorDB using', metadata={'Section': 'Introduction'}), Document(page_content='metadata tags prior to semantic search.', metadata={'Section': 'Introduction'})] We can also look at other parts of the document. retriever . get_relevant_documents ( \"Summarize the Testing section of the document\" ) query='Testing' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='Section', value='Testing') limit=None [Document(page_content='![Untitled](Auto-Evaluation%20of%20Metadata%20Filtering%2018502448c85240828f33716740f9574b/Untitled%202.png)', metadata={'Section': 'Testing'}), Document(page_content='`SelfQueryRetriever` works well in [many cases](https://twitter.com/hwchase17/status/1656791488569954304/photo/1). For example, given [this test case](https://twitter.com/hwchase17/status/1656791488569954304?s=20):  \\n![Untitled](Auto-Evaluation%20of%20Metadata%20Filtering%2018502448c85240828f33716740f9574b/Untitled%201.png)  \\nThe query can be nicely broken up into semantic query and metadata filter:  \\n```python\\nsemantic query: \"prompt injection\"', metadata={'Section': 'Testing'}), Document(page_content='Below, we can see detailed results from the app:  \\n- Kor extraction is above to perform the transformation between query and metadata format ✅\\n- Self-querying attempts to filter using the episode ID (`252`) in the query and fails 🚫\\n- Baseline returns docs from 3 different episodes (one from `252`), confusing the answer 🚫', metadata={'Section': 'Testing'}), Document(page_content='will use in retrieval [here](https://github.com/langchain-ai/auto-evaluator/blob/main/streamlit/kor_retriever_lex.py).', metadata={'Section': 'Testing'})] Now, we can create chat or Q+A apps that are aware of the explicit\ndocument structure. The ability to retain document structure for metadata filtering can be\nhelpful for complicated or longer documents. from langchain . chains import RetrievalQA from langchain . chat_models import ChatOpenAI llm = ChatOpenAI ( model_name = \"gpt-3.5-turbo\" , temperature = 0 ) qa_chain = RetrievalQA . from_chain_type ( llm , retriever = retriever ) qa_chain . run ( \"Summarize the Testing section of the document\" ) query='Testing' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='Section', value='Testing') limit=None 'The Testing section of the document describes the evaluation of the `SelfQueryRetriever` component in comparison to a baseline model. The evaluation was performed on a test case where the query was broken down into a semantic query and a metadata filter. The results showed that the `SelfQueryRetriever` component was able to perform the transformation between query and metadata format, but failed to filter using the episode ID in the query. The baseline model returned documents from three different episodes, which confused the answer. The `SelfQueryRetriever` component was deemed to work well in many cases and will be used in retrieval.' Previous RAG with Agents Next RAG using local models"
    },
    {
        "title": "RAG using local models | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa",
        "text_content": "Retrieval-augmented generation (RAG) RAG using local models On this page RAG using local models The popularity of projects like PrivateGPT , llama.cpp , and GPT4All underscore the importance\nof running LLMs locally. LangChain has integrations with\nmany open-source LLMs that can be run locally. See here for setup instructions for these\nLLMs. For example, here we show how to run GPT4All or LLaMA2 locally\n(e.g., on your laptop) using local embeddings and a local LLM. Document Loading ​ First, install packages needed for local embeddings and vector storage. pip install gpt4all chromadb langchainhub Load and split an example document. We’ll use a blog post on agents as an example. from langchain . document_loaders import WebBaseLoader loader = WebBaseLoader ( \"https://lilianweng.github.io/posts/2023-06-23-agent/\" ) data = loader . load ( ) from langchain . text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter ( chunk_size = 500 , chunk_overlap = 0 ) all_splits = text_splitter . split_documents ( data ) Next, the below steps will download the GPT4All embeddings locally (if\nyou don’t already have them). from langchain . embeddings import GPT4AllEmbeddings from langchain . vectorstores import Chroma vectorstore = Chroma . from_documents ( documents = all_splits , embedding = GPT4AllEmbeddings ( ) ) Found model file at  /Users/rlm/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin objc[49534]: Class GGMLMetalClass is implemented in both /Users/rlm/miniforge3/envs/llama2/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x131614208) and /Users/rlm/miniforge3/envs/llama2/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x131988208). One of the two will be used. Which one is undefined. Test similarity search is working with our local embeddings. question = \"What are the approaches to Task Decomposition?\" docs = vectorstore . similarity_search ( question ) len ( docs ) 4 docs [ 0 ] Document(page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}) Model ​ LLaMA2 ​ Note: new versions of llama-cpp-python use GGUF model files (see here ). If you have an existing GGML model, see here for instructions for conversion\nfor GGUF. And / or, you can download a GGUF converted model (e.g., here ). Finally, as noted in detail here install llama-cpp-python pip install llama - cpp - python To enable use of GPU on Apple Silicon, follow the steps here to use the Python binding with Metal support . In particular, ensure that conda is using the correct virtual\nenvironment that you created ( miniforge3 ). E.g., for me: conda activate /Users/rlm/miniforge3/envs/llama With this confirmed: ! CMAKE_ARGS = \"-DLLAMA_METAL=on\" FORCE_CMAKE = 1 / Users / rlm / miniforge3 / envs / llama / bin / pip install - U llama - cpp - python - - no - cache - dir from langchain . callbacks . manager import CallbackManager from langchain . callbacks . streaming_stdout import StreamingStdOutCallbackHandler from langchain . llms import LlamaCpp Setting model parameters as noted in the llama.cpp\ndocs . n_gpu_layers = 1 # Metal set to 1 is enough. n_batch = 512 # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip. callback_manager = CallbackManager ( [ StreamingStdOutCallbackHandler ( ) ] ) # Make sure the model path is correct for your system! llm = LlamaCpp ( model_path = \"/Users/rlm/Desktop/Code/llama.cpp/models/llama-2-13b-chat.ggufv3.q4_0.bin\" , n_gpu_layers = n_gpu_layers , n_batch = n_batch , n_ctx = 2048 , f16_kv = True , # MUST set to True, otherwise you will run into problem after a couple of calls callback_manager = callback_manager , verbose = True , ) Note that these indicate that Metal was enabled\nproperly : ggml_metal_init: allocating ggml_metal_init: using MPS llm ( \"Simulate a rap battle between Stephen Colbert and John Oliver\" ) Llama.generate: prefix-match hit llama_print_timings:        load time =  4481.74 ms llama_print_timings:      sample time =   183.05 ms /   256 runs   (    0.72 ms per token,  1398.53 tokens per second) llama_print_timings: prompt eval time =   456.05 ms /    13 tokens (   35.08 ms per token,    28.51 tokens per second) llama_print_timings:        eval time =  7375.20 ms /   255 runs   (   28.92 ms per token,    34.58 tokens per second) llama_print_timings:       total time =  8388.92 ms by jonathan Here's the hypothetical rap battle: [Stephen Colbert]: Yo, this is Stephen Colbert, known for my comedy show. I'm here to put some sense in your mind, like an enema do-go. Your opponent? A man of laughter and witty quips, John Oliver! Now let's see who gets the most laughs while taking shots at each other [John Oliver]: Yo, this is John Oliver, known for my own comedy show. I'm here to take your mind on an adventure through wit and humor. But first, allow me to you to our contestant: Stephen Colbert! His show has been around since the '90s, but it's time to see who can out-rap whom [Stephen Colbert]: You claim to be a witty man, John Oliver, with your British charm and clever remarks. But my knows that I'm America's funnyman! Who's the one taking you? Nobody! [John Oliver]: Hey Stephen Colbert, don't get too cocky. You may \"by jonathan \\n\\nHere's the hypothetical rap battle:\\n\\n[Stephen Colbert]: Yo, this is Stephen Colbert, known for my comedy show. I'm here to put some sense in your mind, like an enema do-go. Your opponent? A man of laughter and witty quips, John Oliver! Now let's see who gets the most laughs while taking shots at each other\\n\\n[John Oliver]: Yo, this is John Oliver, known for my own comedy show. I'm here to take your mind on an adventure through wit and humor. But first, allow me to you to our contestant: Stephen Colbert! His show has been around since the '90s, but it's time to see who can out-rap whom\\n\\n[Stephen Colbert]: You claim to be a witty man, John Oliver, with your British charm and clever remarks. But my knows that I'm America's funnyman! Who's the one taking you? Nobody!\\n\\n[John Oliver]: Hey Stephen Colbert, don't get too cocky. You may\" GPT4All ​ Similarly, we can use GPT4All . Download the GPT4All model\nbinary . The Model Explorer on the GPT4All is a\ngreat way to choose and download a model. Then, specify the path that you downloaded to to. E.g., for me, the model lives here: /Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin from langchain . llms import GPT4All llm = GPT4All ( model = \"/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin\" , max_tokens = 2048 , ) LLMChain ​ Run an LLMChain (see here )\nwith either model by passing in the retrieved docs and a simple prompt. It formats the prompt template using the input key values provided and\npasses the formatted string to GPT4All , LLama-V2 , or another\nspecified LLM. In this case, the list of retrieved documents ( docs ) above are pass\ninto {context} . from langchain . chains import LLMChain from langchain . prompts import PromptTemplate # Prompt prompt = PromptTemplate . from_template ( \"Summarize the main themes in these retrieved docs: {docs}\" ) # Chain llm_chain = LLMChain ( llm = llm , prompt = prompt ) # Run question = \"What are the approaches to Task Decomposition?\" docs = vectorstore . similarity_search ( question ) result = llm_chain ( docs ) # Output result [ \"text\" ] Llama.generate: prefix-match hit llama_print_timings:        load time =  1191.88 ms llama_print_timings:      sample time =   134.47 ms /   193 runs   (    0.70 ms per token,  1435.25 tokens per second) llama_print_timings: prompt eval time = 39470.18 ms /  1055 tokens (   37.41 ms per token,    26.73 tokens per second) llama_print_timings:        eval time =  8090.85 ms /   192 runs   (   42.14 ms per token,    23.73 tokens per second) llama_print_timings:       total time = 47943.12 ms Based on the retrieved documents, the main themes are: 1. Task decomposition: The ability to break down complex tasks into smaller subtasks, which can be handled by an LLM or other components of the agent system. 2. LLM as the core controller: The use of a large language model (LLM) as the primary controller of an autonomous agent system, complemented by other key components such as a knowledge graph and a planner. 3. Potentiality of LLM: The idea that LLMs have the potential to be used as powerful general problem solvers, not just for generating well-written copies but also for solving complex tasks and achieving human-like intelligence. 4. Challenges in long-term planning: The challenges in planning over a lengthy history and effectively exploring the solution space, which are important limitations of current LLM-based autonomous agent systems. '\\nBased on the retrieved documents, the main themes are:\\n1. Task decomposition: The ability to break down complex tasks into smaller subtasks, which can be handled by an LLM or other components of the agent system.\\n2. LLM as the core controller: The use of a large language model (LLM) as the primary controller of an autonomous agent system, complemented by other key components such as a knowledge graph and a planner.\\n3. Potentiality of LLM: The idea that LLMs have the potential to be used as powerful general problem solvers, not just for generating well-written copies but also for solving complex tasks and achieving human-like intelligence.\\n4. Challenges in long-term planning: The challenges in planning over a lengthy history and effectively exploring the solution space, which are important limitations of current LLM-based autonomous agent systems.' QA Chain ​ We can use a QA chain to handle our question above. chain_type=\"stuff\" (see here )\nmeans that all the docs will be added (stuffed) into a prompt. We can also use the LangChain Prompt Hub to store and fetch prompts that\nare model-specific. This will work with your LangSmith API\nkey . Let’s try with a default RAG prompt, here . pip install langchainhub # Prompt from langchain import hub rag_prompt = hub . pull ( \"rlm/rag-prompt\" ) from langchain . chains . question_answering import load_qa_chain # Chain chain = load_qa_chain ( llm , chain_type = \"stuff\" , prompt = rag_prompt ) # Run chain ( { \"input_documents\" : docs , \"question\" : question } , return_only_outputs = True ) Llama.generate: prefix-match hit llama_print_timings:        load time = 11326.20 ms llama_print_timings:      sample time =    33.03 ms /    47 runs   (    0.70 ms per token,  1422.86 tokens per second) llama_print_timings: prompt eval time =  1387.31 ms /   242 tokens (    5.73 ms per token,   174.44 tokens per second) llama_print_timings:        eval time =  1321.62 ms /    46 runs   (   28.73 ms per token,    34.81 tokens per second) llama_print_timings:       total time =  2801.08 ms Task can be done by down a task into smaller subtasks, using simple prompting like \"Steps for XYZ.\" or task-specific like \"Write a story outline\" for writing a novel. {'output_text': '\\nTask can be done by down a task into smaller subtasks, using simple prompting like \"Steps for XYZ.\" or task-specific like \"Write a story outline\" for writing a novel.'} Now, let’s try with a prompt specifically for\nLLaMA , which includes special\ntokens . # Prompt rag_prompt_llama = hub . pull ( \"rlm/rag-prompt-llama\" ) rag_prompt_llama ChatPromptTemplate(input_variables=['question', 'context'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'context'], output_parser=None, partial_variables={}, template=\"[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>> \\nQuestion: {question} \\nContext: {context} \\nAnswer: [/INST]\", template_format='f-string', validate_template=True), additional_kwargs={})]) # Chain chain = load_qa_chain ( llm , chain_type = \"stuff\" , prompt = rag_prompt_llama ) # Run chain ( { \"input_documents\" : docs , \"question\" : question } , return_only_outputs = True ) Llama.generate: prefix-match hit llama_print_timings:        load time = 11326.20 ms llama_print_timings:      sample time =   144.81 ms /   207 runs   (    0.70 ms per token,  1429.47 tokens per second) llama_print_timings: prompt eval time =  1506.13 ms /   258 tokens (    5.84 ms per token,   171.30 tokens per second) llama_print_timings:        eval time =  6231.92 ms /   206 runs   (   30.25 ms per token,    33.06 tokens per second) llama_print_timings:       total time =  8158.41 ms Sure, I'd be happy to help! Based on the context, here are some to task: 1. LLM with simple prompting: This using a large model (LLM) with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" to decompose tasks into smaller steps. 2. Task-specific: Another is to use task-specific, such as \"Write a story outline\" for writing a novel, to guide the of tasks. 3. Human inputs:, human inputs can be used to supplement the process, in cases where the task a high degree of creativity or expertise. As fores in long-term and task, one major is that LLMs to adjust plans when faced with errors, making them less robust to humans who learn from trial and error. {'output_text': '  Sure, I\\'d be happy to help! Based on the context, here are some to task:\\n\\n1. LLM with simple prompting: This using a large model (LLM) with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" to decompose tasks into smaller steps.\\n2. Task-specific: Another is to use task-specific, such as \"Write a story outline\" for writing a novel, to guide the of tasks.\\n3. Human inputs:, human inputs can be used to supplement the process, in cases where the task a high degree of creativity or expertise.\\n\\nAs fores in long-term and task, one major is that LLMs to adjust plans when faced with errors, making them less robust to humans who learn from trial and error.'} RetrievalQA ​ For an even simpler flow, use RetrievalQA . This will use a QA default prompt (shown here )\nand will retrieve from the vectorDB. But, you can still pass in a prompt, as before, if desired. from langchain . chains import RetrievalQA qa_chain = RetrievalQA . from_chain_type ( llm , retriever = vectorstore . as_retriever ( ) , chain_type_kwargs = { \"prompt\" : rag_prompt_llama } , ) qa_chain ( { \"query\" : question } ) Llama.generate: prefix-match hit llama_print_timings:        load time = 11326.20 ms llama_print_timings:      sample time =   139.20 ms /   200 runs   (    0.70 ms per token,  1436.76 tokens per second) llama_print_timings: prompt eval time =  1532.26 ms /   258 tokens (    5.94 ms per token,   168.38 tokens per second) llama_print_timings:        eval time =  5977.62 ms /   199 runs   (   30.04 ms per token,    33.29 tokens per second) llama_print_timings:       total time =  7916.21 ms Sure! Based on the context, here's my answer to your: There are several to task,: 1. LLM-based with simple prompting, such as \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" 2. Task-specific, like \"Write a story outline\" for writing a novel. 3. Human inputs to guide the process. These can be used to decompose complex tasks into smaller, more manageable subtasks, which can help improve the and effectiveness of task. However, long-term and task can being due to the need to plan over a lengthy history and explore the space., LLMs may to adjust plans when faced with errors, making them less robust to human learners who can learn from trial and error. {'query': 'What are the approaches to Task Decomposition?', 'result': '  Sure! Based on the context, here\\'s my answer to your:\\n\\nThere are several to task,:\\n\\n1. LLM-based with simple prompting, such as \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\"\\n2. Task-specific, like \"Write a story outline\" for writing a novel.\\n3. Human inputs to guide the process.\\n\\nThese can be used to decompose complex tasks into smaller, more manageable subtasks, which can help improve the and effectiveness of task. However, long-term and task can being due to the need to plan over a lengthy history and explore the space., LLMs may to adjust plans when faced with errors, making them less robust to human learners who can learn from trial and error.'} Previous Text splitting by header Next Interacting with APIs"
    },
    {
        "title": "Text embedding models | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/text_embedding/",
        "text_content": ""
    },
    {
        "title": "Vector stores | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/vectorstores/",
        "text_content": ""
    },
    {
        "title": "Grobid | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/grobid",
        "text_content": "Components Document loaders Grobid Grobid GROBID is a machine learning library for extracting, parsing, and\nre-structuring raw documents. It is designed and expected to be used to parse academic papers, where\nit works particularly well. Note: if the articles supplied to Grobid are\nlarge documents (e.g. dissertations) exceeding a certain number of\nelements, they might not be processed. This loader uses Grobid to parse PDFs into Documents that retain\nmetadata associated with the section of text. The best approach is to install Grobid via docker, see https://grobid.readthedocs.io/en/latest/Grobid-docker/ . (Note: additional instructions can be found here .) Once grobid is up-and-running you can interact as described below. Now, we can use the data loader. from langchain . document_loaders . generic import GenericLoader from langchain . document_loaders . parsers import GrobidParser loader = GenericLoader . from_filesystem ( \"../Papers/\" , glob = \"*\" , suffixes = [ \".pdf\" ] , parser = GrobidParser ( segment_sentences = False ) , ) docs = loader . load ( ) docs [ 3 ] . page_content 'Unlike Chinchilla, PaLM, or GPT-3, we only use publicly available data, making our work compatible with open-sourcing, while most existing models rely on data which is either not publicly available or undocumented (e.g.\"Books -2TB\" or \"Social media conversations\").There exist some exceptions, notably OPT (Zhang et al., 2022), GPT-NeoX (Black et al., 2022), BLOOM (Scao et al., 2022) and GLM (Zeng et al., 2022), but none that are competitive with PaLM-62B or Chinchilla.' docs [ 3 ] . metadata {'text': 'Unlike Chinchilla, PaLM, or GPT-3, we only use publicly available data, making our work compatible with open-sourcing, while most existing models rely on data which is either not publicly available or undocumented (e.g.\"Books -2TB\" or \"Social media conversations\").There exist some exceptions, notably OPT (Zhang et al., 2022), GPT-NeoX (Black et al., 2022), BLOOM (Scao et al., 2022) and GLM (Zeng et al., 2022), but none that are competitive with PaLM-62B or Chinchilla.', 'para': '2', 'bboxes': \"[[{'page': '1', 'x': '317.05', 'y': '509.17', 'h': '207.73', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '522.72', 'h': '220.08', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '536.27', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '549.82', 'h': '218.65', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '563.37', 'h': '136.98', 'w': '9.46'}], [{'page': '1', 'x': '446.49', 'y': '563.37', 'h': '78.11', 'w': '9.46'}, {'page': '1', 'x': '304.69', 'y': '576.92', 'h': '138.32', 'w': '9.46'}], [{'page': '1', 'x': '447.75', 'y': '576.92', 'h': '76.66', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '590.47', 'h': '219.63', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '604.02', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '617.56', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '631.11', 'h': '220.18', 'w': '9.46'}]]\", 'pages': \"('1', '1')\", 'section_title': 'Introduction', 'section_number': '1', 'paper_title': 'LLaMA: Open and Efficient Foundation Language Models', 'file_path': '/Users/31treehaus/Desktop/Papers/2302.13971.pdf'} Previous Google Speech-to-Text Audio Transcripts Next Gutenberg"
    },
    {
        "title": "Document transformers | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_transformers/",
        "text_content": ""
    },
    {
        "title": "Text embedding models | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/data_connection/text_embedding",
        "text_content": "Modules Retrieval Text embedding models On this page Text embedding models info Head to Integrations for documentation on built-in integrations with text embedding model providers. The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them. Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space. The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself). Get started ​ Setup ​ To start we'll need to install the OpenAI Python package: pip install openai Accessing the API requires an API key, which you can get by creating an account and heading here . Once we have a key we'll want to set it as an environment variable by running: export OPENAI_API_KEY = \"...\" If you'd prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class: from langchain . embeddings import OpenAIEmbeddings embeddings_model = OpenAIEmbeddings ( openai_api_key = \"...\" ) Otherwise you can initialize without any params: from langchain . embeddings import OpenAIEmbeddings embeddings_model = OpenAIEmbeddings ( ) embed_documents ​ Embed list of texts ​ embeddings = embeddings_model . embed_documents ( [ \"Hi there!\" , \"Oh, hello!\" , \"What's your name?\" , \"My friends call me World\" , \"Hello World!\" ] ) len ( embeddings ) , len ( embeddings [ 0 ] ) (5, 1536) embed_query ​ Embed single query ​ Embed a single piece of text for the purpose of comparing to other embedded pieces of texts. embedded_query = embeddings_model . embed_query ( \"What was the name mentioned in the conversation?\" ) embedded_query [ : 5 ] [0.0053587136790156364, -0.0004999046213924885, 0.038883671164512634, -0.003001077566295862, -0.00900818221271038] Previous Retrieval Next CacheBackedEmbeddings"
    },
    {
        "title": "MultiQueryRetriever | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever",
        "text_content": "Modules Retrieval Retrievers MultiQueryRetriever On this page MultiQueryRetriever Distance-based vector database retrieval embeds (represents) queries in\nhigh-dimensional space and finds similar embedded documents based on\n“distance”. But, retrieval may produce different results with subtle\nchanges in query wording or if the embeddings do not capture the\nsemantics of the data well. Prompt engineering / tuning is sometimes\ndone to manually address these problems, but can be tedious. The MultiQueryRetriever automates the process of prompt tuning by\nusing an LLM to generate multiple queries from different perspectives\nfor a given user input query. For each query, it retrieves a set of\nrelevant documents and takes the unique union across all queries to get\na larger set of potentially relevant documents. By generating multiple\nperspectives on the same question, the MultiQueryRetriever might be\nable to overcome some of the limitations of the distance-based retrieval\nand get a richer set of results. # Build a sample vectorDB from langchain . document_loaders import WebBaseLoader from langchain . embeddings . openai import OpenAIEmbeddings from langchain . text_splitter import RecursiveCharacterTextSplitter from langchain . vectorstores import Chroma # Load blog post loader = WebBaseLoader ( \"https://lilianweng.github.io/posts/2023-06-23-agent/\" ) data = loader . load ( ) # Split text_splitter = RecursiveCharacterTextSplitter ( chunk_size = 500 , chunk_overlap = 0 ) splits = text_splitter . split_documents ( data ) # VectorDB embedding = OpenAIEmbeddings ( ) vectordb = Chroma . from_documents ( documents = splits , embedding = embedding ) Simple usage ​ Specify the LLM to use for query generation, and the retriever will do\nthe rest. from langchain . chat_models import ChatOpenAI from langchain . retrievers . multi_query import MultiQueryRetriever question = \"What are the approaches to Task Decomposition?\" llm = ChatOpenAI ( temperature = 0 ) retriever_from_llm = MultiQueryRetriever . from_llm ( retriever = vectordb . as_retriever ( ) , llm = llm ) # Set logging for the queries import logging logging . basicConfig ( ) logging . getLogger ( \"langchain.retrievers.multi_query\" ) . setLevel ( logging . INFO ) unique_docs = retriever_from_llm . get_relevant_documents ( query = question ) len ( unique_docs ) INFO:langchain.retrievers.multi_query:Generated queries: ['1. How can Task Decomposition be approached?', '2. What are the different methods for Task Decomposition?', '3. What are the various approaches to decomposing tasks?'] 5 Supplying your own prompt ​ You can also supply a prompt along with an output parser to split the\nresults into a list of queries. from typing import List from langchain . chains import LLMChain from langchain . output_parsers import PydanticOutputParser from langchain . prompts import PromptTemplate from pydantic import BaseModel , Field # Output parser will split the LLM result into a list of queries class LineList ( BaseModel ) : # \"lines\" is the key (attribute name) of the parsed output lines : List [ str ] = Field ( description = \"Lines of text\" ) class LineListOutputParser ( PydanticOutputParser ) : def __init__ ( self ) - > None : super ( ) . __init__ ( pydantic_object = LineList ) def parse ( self , text : str ) - > LineList : lines = text . strip ( ) . split ( \"\\n\" ) return LineList ( lines = lines ) output_parser = LineListOutputParser ( ) QUERY_PROMPT = PromptTemplate ( input_variables = [ \"question\" ] , template = \"\"\"You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}\"\"\" , ) llm = ChatOpenAI ( temperature = 0 ) # Chain llm_chain = LLMChain ( llm = llm , prompt = QUERY_PROMPT , output_parser = output_parser ) # Other inputs question = \"What are the approaches to Task Decomposition?\" # Run retriever = MultiQueryRetriever ( retriever = vectordb . as_retriever ( ) , llm_chain = llm_chain , parser_key = \"lines\" ) # \"lines\" is the key (attribute name) of the parsed output # Results unique_docs = retriever . get_relevant_documents ( query = \"What does the course say about regression?\" ) len ( unique_docs ) INFO:langchain.retrievers.multi_query:Generated queries: [\"1. What is the course's perspective on regression?\", '2. Can you provide information on regression as discussed in the course?', '3. How does the course cover the topic of regression?', \"4. What are the course's teachings on regression?\", '5. In relation to the course, what is mentioned about regression?'] 11 Previous Retrievers Next Contextual compression"
    },
    {
        "title": "MultiVector Retriever | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector",
        "text_content": "Modules Retrieval Retrievers MultiVector Retriever On this page MultiVector Retriever It can often be beneficial to store multiple vectors per document. There\nare multiple use cases where this is beneficial. LangChain has a base MultiVectorRetriever which makes querying this type of setup easy. A\nlot of the complexity lies in how to create the multiple vectors per\ndocument. This notebook covers some of the common ways to create those\nvectors and use the MultiVectorRetriever . The methods to create multiple vectors per document include: Smaller chunks: split a document into smaller chunks, and embed\nthose (this is ParentDocumentRetriever). Summary: create a summary for each document, embed that along with\n(or instead of) the document. Hypothetical questions: create hypothetical questions that each\ndocument would be appropriate to answer, embed those along with (or\ninstead of) the document. Note that this also enables another method of adding embeddings -\nmanually. This is great because you can explicitly add questions or\nqueries that should lead to a document being recovered, giving you more\ncontrol. from langchain . retrievers . multi_vector import MultiVectorRetriever from langchain . document_loaders import TextLoader from langchain . embeddings import OpenAIEmbeddings from langchain . storage import InMemoryByteStore from langchain . text_splitter import RecursiveCharacterTextSplitter from langchain . vectorstores import Chroma loaders = [ TextLoader ( \"../../paul_graham_essay.txt\" ) , TextLoader ( \"../../state_of_the_union.txt\" ) , ] docs = [ ] for loader in loaders : docs . extend ( loader . load ( ) ) text_splitter = RecursiveCharacterTextSplitter ( chunk_size = 10000 ) docs = text_splitter . split_documents ( docs ) Smaller chunks ​ Often times it can be useful to retrieve larger chunks of information,\nbut embed smaller chunks. This allows for embeddings to capture the\nsemantic meaning as closely as possible, but for as much context as\npossible to be passed downstream. Note that this is what the ParentDocumentRetriever does. Here we show what is going on under the\nhood. # The vectorstore to use to index the child chunks vectorstore = Chroma ( collection_name = \"full_documents\" , embedding_function = OpenAIEmbeddings ( ) ) # The storage layer for the parent documents store = InMemoryByteStore ( ) id_key = \"doc_id\" # The retriever (empty to start) retriever = MultiVectorRetriever ( vectorstore = vectorstore , byte_store = store , id_key = id_key , ) import uuid doc_ids = [ str ( uuid . uuid4 ( ) ) for _ in docs ] # The splitter to use to create smaller chunks child_text_splitter = RecursiveCharacterTextSplitter ( chunk_size = 400 ) sub_docs = [ ] for i , doc in enumerate ( docs ) : _id = doc_ids [ i ] _sub_docs = child_text_splitter . split_documents ( [ doc ] ) for _doc in _sub_docs : _doc . metadata [ id_key ] = _id sub_docs . extend ( _sub_docs ) retriever . vectorstore . add_documents ( sub_docs ) retriever . docstore . mset ( list ( zip ( doc_ids , docs ) ) ) # Vectorstore alone retrieves the small chunks retriever . vectorstore . similarity_search ( \"justice breyer\" ) [ 0 ] Document(page_content='Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.', metadata={'doc_id': '3f826cfe-78bd-468d-adb8-f5c2719255df', 'source': '../../state_of_the_union.txt'}) # Retriever returns larger chunks len ( retriever . get_relevant_documents ( \"justice breyer\" ) [ 0 ] . page_content ) 9875 The default search type the retriever performs on the vector database is\na similarity search. LangChain Vector Stores also support searching via Max Marginal\nRelevance so if you want this instead you can just set the search_type property\nas follows: from langchain . retrievers . multi_vector import SearchType retriever . search_type = SearchType . mmr len ( retriever . get_relevant_documents ( \"justice breyer\" ) [ 0 ] . page_content ) 9875 Summary ​ Oftentimes a summary may be able to distill more accurately what a chunk\nis about, leading to better retrieval. Here we show how to create\nsummaries, and then embed those. import uuid from langchain . chat_models import ChatOpenAI from langchain . prompts import ChatPromptTemplate from langchain_core . documents import Document from langchain_core . output_parsers import StrOutputParser chain = ( { \"doc\" : lambda x : x . page_content } | ChatPromptTemplate . from_template ( \"Summarize the following document:\\n\\n{doc}\" ) | ChatOpenAI ( max_retries = 0 ) | StrOutputParser ( ) ) summaries = chain . batch ( docs , { \"max_concurrency\" : 5 } ) # The vectorstore to use to index the child chunks vectorstore = Chroma ( collection_name = \"summaries\" , embedding_function = OpenAIEmbeddings ( ) ) # The storage layer for the parent documents store = InMemoryByteStore ( ) id_key = \"doc_id\" # The retriever (empty to start) retriever = MultiVectorRetriever ( vectorstore = vectorstore , byte_store = store , id_key = id_key , ) doc_ids = [ str ( uuid . uuid4 ( ) ) for _ in docs ] summary_docs = [ Document ( page_content = s , metadata = { id_key : doc_ids [ i ] } ) for i , s in enumerate ( summaries ) ] retriever . vectorstore . add_documents ( summary_docs ) retriever . docstore . mset ( list ( zip ( doc_ids , docs ) ) ) # # We can also add the original chunks to the vectorstore if we so want # for i, doc in enumerate(docs): #     doc.metadata[id_key] = doc_ids[i] # retriever.vectorstore.add_documents(docs) sub_docs = vectorstore . similarity_search ( \"justice breyer\" ) sub_docs [ 0 ] Document(page_content=\"The document is a speech given by the President of the United States, highlighting various issues and priorities. The President discusses the nomination of Judge Ketanji Brown Jackson for the Supreme Court and emphasizes the importance of securing the border and fixing the immigration system. The President also mentions the need to protect women's rights, support LGBTQ+ Americans, pass the Equality Act, and sign bipartisan bills into law. Additionally, the President addresses the opioid epidemic, mental health, support for veterans, and the fight against cancer. The speech concludes with a message of unity and optimism for the future of the United States.\", metadata={'doc_id': '1f0bb74d-4878-43ae-9a5d-4c63fb308ca1'}) retrieved_docs = retriever . get_relevant_documents ( \"justice breyer\" ) len ( retrieved_docs [ 0 ] . page_content ) 9194 Hypothetical Queries ​ An LLM can also be used to generate a list of hypothetical questions\nthat could be asked of a particular document. These questions can then\nbe embedded functions = [ { \"name\" : \"hypothetical_questions\" , \"description\" : \"Generate hypothetical questions\" , \"parameters\" : { \"type\" : \"object\" , \"properties\" : { \"questions\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } , } , } , \"required\" : [ \"questions\" ] , } , } ] from langchain . output_parsers . openai_functions import JsonKeyOutputFunctionsParser chain = ( { \"doc\" : lambda x : x . page_content } # Only asking for 3 hypothetical questions, but this could be adjusted | ChatPromptTemplate . from_template ( \"Generate a list of exactly 3 hypothetical questions that the below document could be used to answer:\\n\\n{doc}\" ) | ChatOpenAI ( max_retries = 0 , model = \"gpt-4\" ) . bind ( functions = functions , function_call = { \"name\" : \"hypothetical_questions\" } ) | JsonKeyOutputFunctionsParser ( key_name = \"questions\" ) ) chain . invoke ( docs [ 0 ] ) [\"What was the author's initial career choice before deciding to switch to AI?\", 'Why did the author become disillusioned with AI during his first year of grad school?', 'What realization did the author have when visiting the Carnegie Institute?'] hypothetical_questions = chain . batch ( docs , { \"max_concurrency\" : 5 } ) # The vectorstore to use to index the child chunks vectorstore = Chroma ( collection_name = \"hypo-questions\" , embedding_function = OpenAIEmbeddings ( ) ) # The storage layer for the parent documents store = InMemoryByteStore ( ) id_key = \"doc_id\" # The retriever (empty to start) retriever = MultiVectorRetriever ( vectorstore = vectorstore , byte_store = store , id_key = id_key , ) doc_ids = [ str ( uuid . uuid4 ( ) ) for _ in docs ] question_docs = [ ] for i , question_list in enumerate ( hypothetical_questions ) : question_docs . extend ( [ Document ( page_content = s , metadata = { id_key : doc_ids [ i ] } ) for s in question_list ] ) retriever . vectorstore . add_documents ( question_docs ) retriever . docstore . mset ( list ( zip ( doc_ids , docs ) ) ) sub_docs = vectorstore . similarity_search ( \"justice breyer\" ) sub_docs [Document(page_content='Who is the nominee for the United States Supreme Court, and what is their background?', metadata={'doc_id': 'd4a82bd9-9001-4bd7-bff1-d8ba2dca9692'}), Document(page_content='Why did Robert Morris suggest the narrator to quit Y Combinator?', metadata={'doc_id': 'aba9b00d-860b-4b93-8e80-87dc08fa461d'}), Document(page_content='What events led to the narrator deciding to hand over Y Combinator to someone else?', metadata={'doc_id': 'aba9b00d-860b-4b93-8e80-87dc08fa461d'}), Document(page_content=\"How does the Bipartisan Infrastructure Law aim to improve America's infrastructure?\", metadata={'doc_id': '822c2ba8-0abe-4f28-a72e-7eb8f477cc3d'})] retrieved_docs = retriever . get_relevant_documents ( \"justice breyer\" ) len ( retrieved_docs [ 0 ] . page_content ) 9194 Previous Ensemble Retriever Next Parent Document Retriever"
    },
    {
        "title": "Retrievers | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/retrievers/",
        "text_content": ""
    },
    {
        "title": "LLMs | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/model_io/llms",
        "text_content": "Modules Model I/O LLMs On this page LLMs Head to Integrations for documentation on\nbuilt-in integrations with LLM providers. Large Language Models (LLMs) are a core component of LangChain.\nLangChain does not serve its own LLMs, but rather provides a standard\ninterface for interacting with many different LLMs. There are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) -\nthe LLM class is designed to provide a standard interface for all of\nthem. In this walkthrough we’ll work with an OpenAI LLM wrapper, although the\nfunctionalities highlighted are generic for all LLM types. Setup ​ For this example we’ll need to install the OpenAI Python package: pip install openai Accessing the API requires an API key, which you can get by creating an\naccount and heading here . Once we have a key\nwe’ll want to set it as an environment variable by running: export OPENAI_API_KEY = \"...\" If you’d prefer not to set an environment variable you can pass the key\nin directly via the openai_api_key named parameter when initiating the\nOpenAI LLM class: from langchain . llms import OpenAI llm = OpenAI ( openai_api_key = \"...\" ) otherwise you can initialize without any params: from langchain . llms import OpenAI llm = OpenAI ( ) LCEL ​ LLMs implement the Runnable\ninterface , the basic building\nblock of the LangChain Expression Language\n(LCEL) . This means they support invoke , ainvoke , stream , astream , batch , abatch , astream_log calls. LLMs accept strings as inputs, or objects which can be coerced to\nstring prompts, including List[BaseMessage] and PromptValue . llm . invoke ( \"What are some theories about the relationship between unemployment and inflation?\" ) '\\n\\n1. The Phillips Curve Theory: This suggests that there is an inverse relationship between unemployment and inflation, meaning that when unemployment is low, inflation will be higher, and when unemployment is high, inflation will be lower.\\n\\n2. The Monetarist Theory: This theory suggests that the relationship between unemployment and inflation is weak, and that changes in the money supply are more important in determining inflation.\\n\\n3. The Resource Utilization Theory: This suggests that when unemployment is low, firms are able to raise wages and prices in order to take advantage of the increased demand for their products and services. This leads to higher inflation.' for chunk in llm . stream ( \"What are some theories about the relationship between unemployment and inflation?\" ) : print ( chunk , end = \"\" , flush = True ) 1. The Phillips Curve Theory: This theory states that there is an inverse relationship between unemployment and inflation. As unemployment decreases, inflation increases and vice versa. 2. The Cost-Push Inflation Theory: This theory suggests that an increase in unemployment leads to a decrease in aggregate demand, which causes prices to go up due to a decrease in supply. 3. The Wage-Push Inflation Theory: This theory states that when unemployment is low, wages tend to increase due to competition for labor, which causes prices to rise. 4. The Monetarist Theory: This theory states that there is no direct relationship between unemployment and inflation, but rather, an increase in the money supply leads to inflation, which can be caused by an increase in unemployment. llm . batch ( [ \"What are some theories about the relationship between unemployment and inflation?\" ] ) ['\\n\\n1. The Phillips Curve Theory: This theory suggests that there is an inverse relationship between unemployment and inflation, meaning that when unemployment decreases, inflation rises, and when unemployment increases, inflation decreases. This theory is based on the idea that when the economy is doing well, there is more demand for goods and services, causing prices to increase.\\n\\n2. The Cost-Push Theory: This theory suggests that when the cost of production increases, it leads to higher prices and lower output. This can lead to higher unemployment and eventually higher inflation.\\n\\n3. The Demand-Pull Theory: This theory suggests that when demand for goods and services increases, it leads to higher prices and eventually higher inflation. This can lead to higher unemployment as businesses cannot keep up with the higher demand.\\n\\n4. The Structural Unemployment Theory: This theory suggests that when there is a mismatch between the skills of the unemployed and the skills required in the job market, it leads to higher unemployment and eventually higher inflation.'] await llm . ainvoke ( \"What are some theories about the relationship between unemployment and inflation?\" ) '\\n\\n1. Phillips Curve Theory: This theory states that there is an inverse relationship between inflation and unemployment. As unemployment decreases, inflation increases, and vice versa.\\n\\n2. Cost-Push Theory: This theory suggests that inflation is caused by rising costs, which can be caused by an increase in unemployment. As unemployment rises, businesses are unable to keep up with demand and have to raise prices to compensate.\\n\\n3. Demand-Pull Theory: This theory suggests that inflation occurs when demand exceeds supply. As unemployment increases, demand for goods and services decreases, leading to a decrease in inflation.\\n\\n4. Monetary Theory: This theory suggests that the money supply and inflation are related to unemployment. When the money supply increases, prices increase, leading to an increase in inflation. If unemployment is high, then the money supply increases, leading to an increase in inflation.' async for chunk in llm . astream ( \"What are some theories about the relationship between unemployment and inflation?\" ) : print ( chunk , end = \"\" , flush = True ) 1. Phillips Curve Theory: This theory suggests that there is an inverse relationship between unemployment and inflation, meaning that when unemployment is low, inflation rises and vice versa. 2. Cost-Push Theory: This theory suggests that inflation is caused by rising costs of production, such as wages, raw materials, and energy. It states that when costs increase, firms must pass these costs onto the consumer, thus raising the price of goods and services and leading to inflation. 3. Demand-Pull Theory: This theory suggests that inflation is caused by an increase in demand for goods and services, leading to a rise in prices. It suggests that when unemployment is low, people have more money to spend and this increased demand pushes up prices. 4. Monetarist Theory: This theory states that inflation is caused by an increase in the money supply. It suggests that when the money supply increases, people have more money to spend, leading to higher prices. await llm . abatch ( [ \"What are some theories about the relationship between unemployment and inflation?\" ] ) ['\\n\\n1. The Phillips Curve Theory: This theory states that there is an inverse relationship between unemployment and inflation. When unemployment is low, wages increase, leading to higher prices and overall inflation.\\n\\n2. The Cost-Push Theory: This theory states that inflation is caused by increases in the costs of production, such as wages, goods, and services. When the cost of production increases, the prices of goods and services must also increase, leading to inflation.\\n\\n3. The Demand Pull Theory: This theory states that inflation is caused by an increase in aggregate demand for goods and services. When the demand is high, prices must increase in order to meet the demand. This leads to inflation.\\n\\n4. The Structural Unemployment Theory: This theory states that when unemployment is high, there is an excess supply of labor. This excess supply of labor can result in lower wages, which can cause inflation as people are willing to accept lower wages for the same amount of work.'] async for chunk in llm . astream_log ( \"What are some theories about the relationship between unemployment and inflation?\" ) : print ( chunk ) RunLogPatch({'op': 'replace', 'path': '', 'value': {'final_output': None, 'id': 'baf410ad-618e-44db-93c8-809da4e3ed44', 'logs': {}, 'streamed_output': []}}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '1'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' The'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Phillips'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Curve'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ':'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' This'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' theory'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' suggests'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' that'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' there'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' is'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' an'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' inverse'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' relationship'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' between'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' unemployment and'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' inflation'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' When'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' unemployment'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' is'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' low'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' inflation'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' tends'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' be'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' high'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' when'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' unemployment'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' is'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' high'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' inflation'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' tends'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' be'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' low'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' '}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '2'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' The'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' NA'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'IR'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'U'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Theory'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ':'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' This'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' theory'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' suggests'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' that there is'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' a'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' natural'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' rate'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' of'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' unemployment'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' also'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' known'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' as'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Non'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Ac'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'celer'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ating'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' In'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'flation'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Rate'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' of'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Unemployment'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' ('}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'NA'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'IR'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'U'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ').'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' According'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' this'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' theory'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' when'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' unemployment'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' is'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' below'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' NA'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'IR'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'U'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' then'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' inflation'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' will'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' increase'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' when'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' unemployment'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' is'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' above'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' NA'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'IR'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'U'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' then'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' inflation'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' will'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' decrease'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '3'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' The'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Cost'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Push'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' In'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'flation'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Theory'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ':'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' This'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' theory'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' suggests'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' that high unemployment'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' leads'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' higher'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' wages'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' which'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' in'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' turn'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' leads'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' higher'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' prices'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and higher inflation'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''}) RunLogPatch({'op': 'replace', 'path': '/final_output', 'value': {'generations': [[{'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'text': '\\n' '\\n' '1. The Phillips Curve: This theory ' 'suggests that there is an inverse ' 'relationship between unemployment and ' 'inflation. When unemployment is low, ' 'inflation tends to be high, and when ' 'unemployment is high, inflation tends ' 'to be low. \\n' '\\n' '2. The NAIRU Theory: This theory ' 'suggests that there is a natural rate ' 'of unemployment, also known as the ' 'Non-Accelerating Inflation Rate of ' 'Unemployment (NAIRU). According to this ' 'theory, when unemployment is below the ' 'NAIRU, then inflation will increase, ' 'and when unemployment is above the ' 'NAIRU, then inflation will decrease.\\n' '\\n' '3. The Cost-Push Inflation Theory: This ' 'theory suggests that high unemployment ' 'leads to higher wages, which in turn ' 'leads to higher prices and higher ' 'inflation.'}]], 'llm_output': None, 'run': None}}) LangSmith ​ All LLM s come with built-in LangSmith tracing. Just set the following\nenvironment variables: export LANGCHAIN_TRACING_V2 = \"true\" export LANGCHAIN_API_KEY = < your-api-key > and any LLM invocation (whether it’s nested in a chain or not) will\nautomatically be traced. A trace will include inputs, outputs, latency,\ntoken usage, invocation params, environment params, and more. See an\nexample here: https://smith.langchain.com/public/7924621a-ff58-4b1c-a2a2-035a354ef434/r . In LangSmith you can then provide feedback for any trace, compile\nannotated datasets for evals, debug performance in the playground, and\nmore. [ Legacy ] __call__ : string in - > string out {#legacy- call -string-in---string-out} ​ The simplest way to use an LLM is a callable: pass in a string, get a\nstring completion. llm ( \"Tell me a joke\" ) '\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!' [ Legacy ] generate : batch calls, richer outputs ​ generate lets you call the model with a list of strings, getting back\na more complete response than just the text. This complete response can\ninclude things like multiple top responses and other LLM\nprovider-specific information: llm_result = llm . generate ( [ \"Tell me a joke\" , \"Tell me a poem\" ] * 15 ) len ( llm_result . generations ) 30 llm_result . generations [ 0 ] [Generation(text=\"\\n\\nQ: Why don't scientists trust atoms?\\nA: Because they make up everything!\", generation_info={'finish_reason': 'stop', 'logprobs': None})] llm_result . generations [ - 1 ] [Generation(text='\\n\\nRoses are red,\\nViolets are blue,\\nSugar is sweet,\\nAnd so are you!', generation_info={'finish_reason': 'stop', 'logprobs': None})] You can also access provider specific information that is returned. This\ninformation is not standardized across providers. llm_result . llm_output {'token_usage': {'completion_tokens': 900, 'total_tokens': 1020, 'prompt_tokens': 120}, 'model_name': 'text-davinci-003'} Previous Tracking token usage Next Async API"
    },
    {
        "title": "LLMs | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/llms",
        "text_content": "Components LLMs On this page LLMs Features (natively supported) ​ All LLMs implement the Runnable interface, which comes with default implementations of all methods, ie. ainvoke , batch , abatch , stream , astream . This gives all LLMs basic support for async, streaming and batch, which by default is implemented as below: Async support defaults to calling the respective sync method in asyncio's default thread pool executor. This lets other async functions in your application make progress while the LLM is being executed, by moving this call to a background thread. Streaming support defaults to returning an Iterator (or AsyncIterator in the case of async streaming) of a single value, the final result returned by the underlying LLM provider. This obviously doesn't give you token-by-token streaming, which requires native support from the LLM provider, but ensures your code that expects an iterator of tokens can work for any of our LLM integrations. Batch support defaults to calling the underlying LLM in parallel for each input by making use of a thread pool executor (in the sync batch case) or asyncio.gather (in the async batch case). The concurrency can be controlled with the max_concurrency key in RunnableConfig . Each LLM integration can optionally provide native implementations for async, streaming or batch, which, for providers that support it, can be more efficient. The table shows, for each integration, which features have been implemented with native support. Model Invoke Async invoke Stream Async stream Batch Async batch AI21 ✅ ❌ ❌ ❌ ❌ ❌ AlephAlpha ✅ ❌ ❌ ❌ ❌ ❌ AmazonAPIGateway ✅ ❌ ❌ ❌ ❌ ❌ Anthropic ✅ ✅ ✅ ✅ ❌ ❌ Anyscale ✅ ✅ ✅ ✅ ✅ ✅ Arcee ✅ ❌ ❌ ❌ ❌ ❌ Aviary ✅ ❌ ❌ ❌ ❌ ❌ AzureMLOnlineEndpoint ✅ ❌ ❌ ❌ ❌ ❌ AzureOpenAI ✅ ✅ ✅ ✅ ✅ ✅ Banana ✅ ❌ ❌ ❌ ❌ ❌ Baseten ✅ ❌ ❌ ❌ ❌ ❌ Beam ✅ ❌ ❌ ❌ ❌ ❌ Bedrock ✅ ❌ ✅ ❌ ❌ ❌ CTransformers ✅ ✅ ❌ ❌ ❌ ❌ CTranslate2 ✅ ❌ ❌ ❌ ✅ ❌ CerebriumAI ✅ ❌ ❌ ❌ ❌ ❌ ChatGLM ✅ ❌ ❌ ❌ ❌ ❌ Clarifai ✅ ❌ ❌ ❌ ❌ ❌ Cohere ✅ ✅ ❌ ❌ ❌ ❌ Databricks ✅ ❌ ❌ ❌ ❌ ❌ DeepInfra ✅ ✅ ✅ ✅ ❌ ❌ DeepSparse ✅ ✅ ✅ ✅ ❌ ❌ EdenAI ✅ ✅ ❌ ❌ ❌ ❌ Fireworks ✅ ✅ ✅ ✅ ✅ ✅ ForefrontAI ✅ ❌ ❌ ❌ ❌ ❌ GPT4All ✅ ❌ ❌ ❌ ❌ ❌ GigaChat ✅ ✅ ✅ ✅ ✅ ✅ GooglePalm ✅ ❌ ✅ ❌ ✅ ❌ GooseAI ✅ ❌ ❌ ❌ ❌ ❌ GradientLLM ✅ ✅ ❌ ❌ ✅ ✅ HuggingFaceEndpoint ✅ ❌ ❌ ❌ ❌ ❌ HuggingFaceHub ✅ ❌ ❌ ❌ ❌ ❌ HuggingFacePipeline ✅ ❌ ❌ ❌ ✅ ❌ HuggingFaceTextGenInference ✅ ✅ ✅ ✅ ❌ ❌ HumanInputLLM ✅ ❌ ❌ ❌ ❌ ❌ JavelinAIGateway ✅ ✅ ❌ ❌ ❌ ❌ KoboldApiLLM ✅ ❌ ❌ ❌ ❌ ❌ LlamaCpp ✅ ❌ ✅ ❌ ❌ ❌ ManifestWrapper ✅ ❌ ❌ ❌ ❌ ❌ Minimax ✅ ❌ ❌ ❌ ❌ ❌ MlflowAIGateway ✅ ❌ ❌ ❌ ❌ ❌ Modal ✅ ❌ ❌ ❌ ❌ ❌ MosaicML ✅ ❌ ❌ ❌ ❌ ❌ NIBittensorLLM ✅ ❌ ❌ ❌ ❌ ❌ NLPCloud ✅ ❌ ❌ ❌ ❌ ❌ Nebula ✅ ❌ ❌ ❌ ❌ ❌ OctoAIEndpoint ✅ ❌ ❌ ❌ ❌ ❌ Ollama ✅ ❌ ❌ ❌ ❌ ❌ OpaquePrompts ✅ ❌ ❌ ❌ ❌ ❌ OpenAI ✅ ✅ ✅ ✅ ✅ ✅ OpenLLM ✅ ✅ ❌ ❌ ❌ ❌ OpenLM ✅ ✅ ✅ ✅ ✅ ✅ PaiEasEndpoint ✅ ❌ ✅ ❌ ❌ ❌ Petals ✅ ❌ ❌ ❌ ❌ ❌ PipelineAI ✅ ❌ ❌ ❌ ❌ ❌ Predibase ✅ ❌ ❌ ❌ ❌ ❌ PredictionGuard ✅ ❌ ❌ ❌ ❌ ❌ PromptLayerOpenAI ✅ ❌ ❌ ❌ ❌ ❌ QianfanLLMEndpoint ✅ ✅ ✅ ✅ ❌ ❌ RWKV ✅ ❌ ❌ ❌ ❌ ❌ Replicate ✅ ❌ ✅ ❌ ❌ ❌ SagemakerEndpoint ✅ ❌ ❌ ❌ ❌ ❌ SelfHostedHuggingFaceLLM ✅ ❌ ❌ ❌ ❌ ❌ SelfHostedPipeline ✅ ❌ ❌ ❌ ❌ ❌ StochasticAI ✅ ❌ ❌ ❌ ❌ ❌ TextGen ✅ ❌ ❌ ❌ ❌ ❌ TitanTakeoff ✅ ❌ ✅ ❌ ❌ ❌ TitanTakeoffPro ✅ ❌ ✅ ❌ ❌ ❌ Tongyi ✅ ❌ ❌ ❌ ❌ ❌ VLLM ✅ ❌ ❌ ❌ ✅ ❌ VLLMOpenAI ✅ ✅ ✅ ✅ ✅ ✅ VertexAI ✅ ✅ ✅ ❌ ✅ ✅ VertexAIModelGarden ✅ ✅ ❌ ❌ ✅ ✅ VolcEngineMaasLLM ✅ ❌ ✅ ❌ ❌ ❌ WatsonxLLM ✅ ❌ ✅ ❌ ✅ ❌ Writer ✅ ❌ ❌ ❌ ❌ ❌ Xinference ✅ ❌ ❌ ❌ ❌ ❌ YandexGPT ✅ ✅ ❌ ❌ ❌ ❌ Previous Components Next LLMs"
    },
    {
        "title": "Chat models | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/chat",
        "text_content": "Components Chat models On this page Chat models Features (natively supported) ​ All ChatModels implement the Runnable interface, which comes with default implementations of all methods, ie. ainvoke , batch , abatch , stream , astream . This gives all ChatModels basic support for async, streaming and batch, which by default is implemented as below: Async support defaults to calling the respective sync method in asyncio's default thread pool executor. This lets other async functions in your application make progress while the ChatModel is being executed, by moving this call to a background thread. Streaming support defaults to returning an Iterator (or AsyncIterator in the case of async streaming) of a single value, the final result returned by the underlying ChatModel provider. This obviously doesn't give you token-by-token streaming, which requires native support from the ChatModel provider, but ensures your code that expects an iterator of tokens can work for any of our ChatModel integrations. Batch support defaults to calling the underlying ChatModel in parallel for each input by making use of a thread pool executor (in the sync batch case) or asyncio.gather (in the async batch case). The concurrency can be controlled with the max_concurrency key in RunnableConfig . Each ChatModel integration can optionally provide native implementations to truly enable async or streaming.\nThe table shows, for each integration, which features have been implemented with native support. Model Invoke Async invoke Stream Async stream AzureChatOpenAI ✅ ✅ ✅ ✅ BedrockChat ✅ ❌ ✅ ❌ ChatAnthropic ✅ ✅ ✅ ✅ ChatAnyscale ✅ ✅ ✅ ✅ ChatBaichuan ✅ ❌ ✅ ❌ ChatCohere ✅ ✅ ✅ ✅ ChatDatabricks ✅ ✅ ❌ ❌ ChatEverlyAI ✅ ✅ ✅ ✅ ChatFireworks ✅ ✅ ✅ ✅ ChatGooglePalm ✅ ✅ ❌ ❌ ChatHunyuan ✅ ❌ ✅ ❌ ChatJavelinAIGateway ✅ ✅ ❌ ❌ ChatKonko ✅ ❌ ❌ ❌ ChatLiteLLM ✅ ✅ ✅ ✅ ChatMLflowAIGateway ✅ ❌ ❌ ❌ ChatMlflow ✅ ✅ ❌ ❌ ChatOllama ✅ ❌ ✅ ❌ ChatOpenAI ✅ ✅ ✅ ✅ ChatVertexAI ✅ ✅ ✅ ❌ ChatYandexGPT ✅ ✅ ❌ ❌ ErnieBotChat ✅ ❌ ❌ ❌ GigaChat ✅ ✅ ✅ ✅ JinaChat ✅ ✅ ✅ ✅ MiniMaxChat ✅ ✅ ❌ ❌ PaiEasChatEndpoint ✅ ✅ ❌ ✅ PromptLayerChatOpenAI ✅ ❌ ❌ ❌ QianfanChatEndpoint ✅ ✅ ✅ ✅ VolcEngineMaasChat ✅ ❌ ✅ ❌ Previous YandexGPT Next Chat models"
    },
    {
        "title": "Chat models | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/model_io/chat",
        "text_content": "Modules Model I/O Chat models On this page Chat models Head to Integrations for documentation on\nbuilt-in integrations with chat model providers. Chat models are a variation on language models. While chat models use\nlanguage models under the hood, the interface they use is a bit\ndifferent. Rather than using a “text in, text out” API, they use an\ninterface where “chat messages” are the inputs and outputs. Setup ​ For this example we’ll need to install the OpenAI Python package: pip install openai Accessing the API requires an API key, which you can get by creating an\naccount and heading here . Once we have a key\nwe’ll want to set it as an environment variable by running: export OPENAI_API_KEY = \"...\" If you’d prefer not to set an environment variable you can pass the key\nin directly via the openai_api_key named parameter when initiating the\nOpenAI LLM class: from langchain . chat_models import ChatOpenAI chat = ChatOpenAI ( openai_api_key = \"...\" ) Otherwise you can initialize without any params: from langchain . chat_models import ChatOpenAI chat = ChatOpenAI ( ) Messages ​ The chat model interface is based around messages rather than raw text.\nThe types of messages currently supported in LangChain are AIMessage , HumanMessage , SystemMessage , FunctionMessage and ChatMessage – ChatMessage takes in an arbitrary role parameter. Most of the time,\nyou’ll just be dealing with HumanMessage , AIMessage , and SystemMessage LCEL ​ Chat models implement the Runnable\ninterface , the basic building\nblock of the LangChain Expression Language\n(LCEL) . This means they support invoke , ainvoke , stream , astream , batch , abatch , astream_log calls. Chat models accept List[BaseMessage] as inputs, or objects which can\nbe coerced to messages, including str (converted to HumanMessage )\nand PromptValue . from langchain_core . messages import HumanMessage , SystemMessage messages = [ SystemMessage ( content = \"You're a helpful assistant\" ) , HumanMessage ( content = \"What is the purpose of model regularization?\" ) , ] chat . invoke ( messages ) AIMessage(content=\"The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and starts to fit the noise in the training data, leading to poor generalization on unseen data. Regularization techniques introduce additional constraints or penalties to the model's objective function, discouraging it from becoming overly complex and promoting simpler and more generalizable models. Regularization helps to strike a balance between fitting the training data well and avoiding overfitting, leading to better performance on new, unseen data.\") for chunk in chat . stream ( messages ) : print ( chunk . content , end = \"\" , flush = True ) The purpose of model regularization is to prevent overfitting and improve the generalization of a machine learning model. Overfitting occurs when a model is too complex and learns the noise or random variations in the training data, which leads to poor performance on new, unseen data. Regularization techniques introduce additional constraints or penalties to the model's learning process, discouraging it from fitting the noise and reducing the complexity of the model. This helps to improve the model's ability to generalize well and make accurate predictions on unseen data. chat . batch ( [ messages ] ) [AIMessage(content=\"The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and starts to learn the noise or random fluctuations in the training data, rather than the underlying patterns or relationships. Regularization techniques add a penalty term to the model's objective function, which discourages the model from becoming too complex and helps it generalize better to new, unseen data. This improves the model's ability to make accurate predictions on new data by reducing the variance and increasing the model's overall performance.\")] await chat . ainvoke ( messages ) AIMessage(content='The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and starts to memorize the training data instead of learning general patterns and relationships. This leads to poor performance on new, unseen data.\\n\\nRegularization techniques introduce additional constraints or penalties to the model during training, discouraging it from becoming overly complex. This helps to strike a balance between fitting the training data well and generalizing to new data. Regularization techniques can include adding a penalty term to the loss function, such as L1 or L2 regularization, or using techniques like dropout or early stopping. By regularizing the model, it encourages it to learn the most relevant features and reduce the impact of noise or outliers in the data.') async for chunk in chat . astream ( messages ) : print ( chunk . content , end = \"\" , flush = True ) The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and starts to memorize the training data instead of learning the underlying patterns. Regularization techniques help in reducing the complexity of the model by adding a penalty to the loss function. This penalty encourages the model to have smaller weights or fewer features, making it more generalized and less prone to overfitting. The goal is to find the right balance between fitting the training data well and being able to generalize well to unseen data. async for chunk in chat . astream_log ( messages ) : print ( chunk ) RunLogPatch({'op': 'replace', 'path': '', 'value': {'final_output': None, 'id': '754c4143-2348-46c4-ad2b-3095913084c6', 'logs': {}, 'streamed_output': []}}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='The')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' purpose')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' of')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' model')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' regularization')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' is')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' to')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' prevent')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' a')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' machine')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' learning')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' model')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' from')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' over')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='fit')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='ting')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' the')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' training')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' data')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' and')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' improve')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' its')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' general')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='ization')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' ability')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='.')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' Over')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='fit')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='ting')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' occurs')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' when')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' a')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' model')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' becomes')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' too')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' complex')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' and')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' learns')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' to')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' fit')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' the')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' noise')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' or')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' random')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' fluctuations')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' in')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' the')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' training')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' data')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=',')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' instead')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' of')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' capturing')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' the')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' underlying')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' patterns')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' and')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' relationships')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='.')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' Regular')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='ization')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' techniques')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' introduce')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' a')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' penalty')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' term')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' to')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' the')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' model')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=\"'s\")}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' objective')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' function')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=',')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' which')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' discour')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='ages')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' the')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' model')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' from')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' becoming')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' too')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' complex')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='.')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' This')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' helps')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' to')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' control')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' the')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' model')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=\"'s\")}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' complexity')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' and')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' reduces')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' the')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' risk')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' of')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' over')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='fit')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='ting')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=',')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' leading')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' to')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' better')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' performance')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' on')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' unseen')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content=' data')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='.')}) RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': AIMessageChunk(content='')}) RunLogPatch({'op': 'replace', 'path': '/final_output', 'value': {'generations': [[{'generation_info': {'finish_reason': 'stop'}, 'message': AIMessageChunk(content=\"The purpose of model regularization is to prevent a machine learning model from overfitting the training data and improve its generalization ability. Overfitting occurs when a model becomes too complex and learns to fit the noise or random fluctuations in the training data, instead of capturing the underlying patterns and relationships. Regularization techniques introduce a penalty term to the model's objective function, which discourages the model from becoming too complex. This helps to control the model's complexity and reduces the risk of overfitting, leading to better performance on unseen data.\"), 'text': 'The purpose of model regularization is ' 'to prevent a machine learning model ' 'from overfitting the training data and ' 'improve its generalization ability. ' 'Overfitting occurs when a model becomes ' 'too complex and learns to fit the noise ' 'or random fluctuations in the training ' 'data, instead of capturing the ' 'underlying patterns and relationships. ' 'Regularization techniques introduce a ' \"penalty term to the model's objective \" 'function, which discourages the model ' 'from becoming too complex. This helps ' \"to control the model's complexity and \" 'reduces the risk of overfitting, ' 'leading to better performance on unseen ' 'data.'}]], 'llm_output': None, 'run': None}}) LangSmith ​ All ChatModel s come with built-in LangSmith tracing. Just set the\nfollowing environment variables: export LANGCHAIN_TRACING_V2 = \"true\" export LANGCHAIN_API_KEY = < your-api-key > and any ChatModel invocation (whether it’s nested in a chain or not)\nwill automatically be traced. A trace will include inputs, outputs,\nlatency, token usage, invocation params, environment params, and more.\nSee an example here: https://smith.langchain.com/public/a54192ae-dd5c-4f7a-88d1-daa1eaba1af7/r . In LangSmith you can then provide feedback for any trace, compile\nannotated datasets for evals, debug performance in the playground, and\nmore. [ Legacy ] __call__ {#legacy- call } ​ Messages in - > message out ​ For convenience you can also treat chat models as callables. You can get\nchat completions by passing one or more messages to the chat model. The\nresponse will be a message. from langchain . schema import HumanMessage , SystemMessage chat ( [ HumanMessage ( content = \"Translate this sentence from English to French: I love programming.\" ) ] ) AIMessage(content=\"J'adore la programmation.\") OpenAI’s chat model supports multiple messages as input. See here for more information. Here is an example of sending a system and user\nmessage to the chat model: messages = [ SystemMessage ( content = \"You are a helpful assistant that translates English to French.\" ) , HumanMessage ( content = \"I love programming.\" ) , ] chat ( messages ) AIMessage(content=\"J'adore la programmation.\") [ Legacy ] generate ​ Batch calls, richer outputs ​ You can go one step further and generate completions for multiple sets\nof messages using generate . This returns an LLMResult with an\nadditional message parameter. This will include additional information\nabout each generation beyond the returned message (e.g. the finish\nreason) and additional information about the full API call (e.g. total\ntokens used). batch_messages = [ [ SystemMessage ( content = \"You are a helpful assistant that translates English to French.\" ) , HumanMessage ( content = \"I love programming.\" ) , ] , [ SystemMessage ( content = \"You are a helpful assistant that translates English to French.\" ) , HumanMessage ( content = \"I love artificial intelligence.\" ) , ] , ] result = chat . generate ( batch_messages ) result LLMResult(generations=[[ChatGeneration(text=\"J'adore programmer.\", generation_info={'finish_reason': 'stop'}, message=AIMessage(content=\"J'adore programmer.\"))], [ChatGeneration(text=\"J'adore l'intelligence artificielle.\", generation_info={'finish_reason': 'stop'}, message=AIMessage(content=\"J'adore l'intelligence artificielle.\"))]], llm_output={'token_usage': {'prompt_tokens': 53, 'completion_tokens': 18, 'total_tokens': 71}, 'model_name': 'gpt-3.5-turbo'}, run=[RunInfo(run_id=UUID('077917a9-026c-47c4-b308-77b37c3a3bfa')), RunInfo(run_id=UUID('0a70a0bf-c599-4f51-932a-c7d42202c984'))]) You can recover things like token usage from this LLMResult: result . llm_output {'token_usage': {'prompt_tokens': 53, 'completion_tokens': 18, 'total_tokens': 71}, 'model_name': 'gpt-3.5-turbo'} Previous Select by similarity Next Caching"
    },
    {
        "title": "Retrievers | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/data_connection/retrievers",
        "text_content": "Modules Retrieval Retrievers On this page Retrievers Head to Integrations for documentation\non built-in retriever integrations with 3rd-party tools. A retriever is an interface that returns documents given an unstructured\nquery. It is more general than a vector store. A retriever does not need\nto be able to store documents, only to return (or retrieve) them. Vector\nstores can be used as the backbone of a retriever, but there are other\ntypes of retrievers as well. Retrievers implement the Runnable\ninterface , the basic building\nblock of the LangChain Expression Language\n(LCEL) . This means they support invoke , ainvoke , stream , astream , batch , abatch , astream_log calls. Retrievers accept a string query as input and return a list of Document ’s as output. Get started ​ In this example we’ll use a Chroma vector store-backed retriever. To\nget setup we’ll need to run: pip install chromadb And download the state_of_the_union.txt file here . from langchain . embeddings import OpenAIEmbeddings from langchain . text_splitter import CharacterTextSplitter from langchain . vectorstores import Chroma full_text = open ( \"state_of_the_union.txt\" , \"r\" ) . read ( ) text_splitter = CharacterTextSplitter ( chunk_size = 1000 , chunk_overlap = 100 ) texts = text_splitter . split_text ( full_text ) embeddings = OpenAIEmbeddings ( ) db = Chroma . from_texts ( texts , embeddings ) retriever = db . as_retriever ( ) retrieved_docs = retriever . invoke ( \"What did the president say about Ketanji Brown Jackson?\" ) print ( retrieved_docs [ 0 ] . page_content ) One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence. A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. We can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling. We’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers. LCEL ​ Since retrievers are Runnable ’s, we can easily compose them with other Runnable objects: from langchain . chat_models import ChatOpenAI from langchain . prompts import ChatPromptTemplate from langchain . schema import StrOutputParser from langchain_core . runnables import RunnablePassthrough template = \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\" prompt = ChatPromptTemplate . from_template ( template ) model = ChatOpenAI ( ) def format_docs ( docs ) : return \"\\n\\n\" . join ( [ d . page_content for d in docs ] ) chain = ( { \"context\" : retriever | format_docs , \"question\" : RunnablePassthrough ( ) } | prompt | model | StrOutputParser ( ) ) chain . invoke ( \"What did the president say about technology?\" ) 'The president said that technology plays a crucial role in the future and that passing the Bipartisan Innovation Act will make record investments in emerging technologies and American manufacturing. The president also mentioned Intel\\'s plans to build a semiconductor \"mega site\" and increase their investment from $20 billion to $100 billion, which would be one of the biggest investments in manufacturing in American history.' Previous Vector stores Next MultiQueryRetriever"
    },
    {
        "title": "Retrieval-augmented generation (RAG) | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/question_answering",
        "text_content": "Retrieval-augmented generation (RAG) On this page Retrieval-augmented generation (RAG) Open In Colab Overview ​ What is RAG? ​ RAG is a technique for augmenting LLM knowledge with additional, often\nprivate or real-time, data. LLMs can reason about wide-ranging topics, but their knowledge is\nlimited to the public data up to a specific point in time that they were\ntrained on. If you want to build AI applications that can reason about\nprivate data or data introduced after a model’s cutoff date, you need to\naugment the knowledge of the model with the specific information it\nneeds. The process of bringing the appropriate information and inserting\nit into the model prompt is known as Retrieval Augmented Generation\n(RAG). What’s in this guide? ​ LangChain has a number of components specifically designed to help build\nRAG applications. To familiarize ourselves with these, we’ll build a\nsimple question-answering application over a text data source.\nSpecifically, we’ll build a QA bot over the LLM Powered Autonomous\nAgents blog post\nby Lilian Weng. Along the way we’ll go over a typical QA architecture,\ndiscuss the relevant LangChain components, and highlight additional\nresources for more advanced QA techniques. We’ll also see how LangSmith\ncan help us trace and understand our application. LangSmith will become\nincreasingly helpful as our application grows in complexity. Note Here we focus on RAG for unstructured data. Two RAG use cases\nwhich we cover elsewhere are: - QA over structured\ndata (e.g., SQL) - QA over\ncode (e.g.,\nPython) Architecture ​ A typical RAG application has two main components: Indexing : a pipeline for ingesting data from a source and indexing\nit. This usually happen offline. Retrieval and generation : the actual RAG chain, which takes the user\nquery at run time and retrieves the relevant data from the index, then\npasses that to the model. The most common full sequence from raw data to answer looks like: Indexing ​ Load : First we need to load our data. We’ll use DocumentLoaders for this. Split : Text\nsplitters break large Documents into smaller chunks. This is useful both for\nindexing data and for passing it in to a model, since large chunks\nare harder to search over and won’t fit in a model’s finite context\nwindow. Store : We need somewhere to store and index our splits, so that\nthey can later be searched over. This is often done using a VectorStore and Embeddings model. Retrieval and generation ​ Retrieve : Given a user input, relevant splits are retrieved from\nstorage using a Retriever . Generate : A ChatModel / LLM produces an answer using a\nprompt that includes the question and the retrieved data Setup ​ Dependencies ​ We’ll use an OpenAI chat model and embeddings and a Chroma vector store\nin this walkthrough, but everything shown here works with any ChatModel or LLM , Embeddings , and VectorStore or Retriever . We’ll use the following packages: !pip install - U langchain openai chromadb langchainhub bs4 We need to set environment variable OPENAI_API_KEY , which can be done\ndirectly or loaded from a .env file like so: import getpass import os os . environ [ \"OPENAI_API_KEY\" ] = getpass . getpass ( ) # import dotenv # dotenv.load_dotenv() LangSmith ​ Many of the applications you build with LangChain will contain multiple\nsteps with multiple invocations of LLM calls. As these applications get\nmore and more complex, it becomes crucial to be able to inspect what\nexactly is going on inside your chain or agent. The best way to do this\nis with LangSmith . Note that LangSmith is not needed, but it is helpful. If you do want to\nuse LangSmith, after you sign up at the link above, make sure to set\nyour environment variables to start logging traces: os . environ [ \"LANGCHAIN_TRACING_V2\" ] = \"true\" os . environ [ \"LANGCHAIN_API_KEY\" ] = getpass . getpass ( ) Quickstart ​ Suppose we want to build a QA app over the LLM Powered Autonomous\nAgents blog post\nby Lilian Weng. We can create a simple pipeline for this in ~20 lines of\ncode: import bs4 from langchain import hub from langchain . chat_models import ChatOpenAI from langchain . document_loaders import WebBaseLoader from langchain . embeddings import OpenAIEmbeddings from langchain . schema import StrOutputParser from langchain . text_splitter import RecursiveCharacterTextSplitter from langchain . vectorstores import Chroma from langchain_core . runnables import RunnablePassthrough loader = WebBaseLoader ( web_paths = ( \"https://lilianweng.github.io/posts/2023-06-23-agent/\" , ) , bs_kwargs = dict ( parse_only = bs4 . SoupStrainer ( class_ = ( \"post-content\" , \"post-title\" , \"post-header\" ) ) ) , ) docs = loader . load ( ) text_splitter = RecursiveCharacterTextSplitter ( chunk_size = 1000 , chunk_overlap = 200 ) splits = text_splitter . split_documents ( docs ) vectorstore = Chroma . from_documents ( documents = splits , embedding = OpenAIEmbeddings ( ) ) retriever = vectorstore . as_retriever ( ) prompt = hub . pull ( \"rlm/rag-prompt\" ) llm = ChatOpenAI ( model_name = \"gpt-3.5-turbo\" , temperature = 0 ) def format_docs ( docs ) : return \"\\n\\n\" . join ( doc . page_content for doc in docs ) rag_chain = ( { \"context\" : retriever | format_docs , \"question\" : RunnablePassthrough ( ) } | prompt | llm | StrOutputParser ( ) ) rag_chain . invoke ( \"What is Task Decomposition?\" ) 'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through prompting techniques like Chain of Thought or Tree of Thoughts, or by using task-specific instructions or human inputs. Task decomposition helps agents plan ahead and manage complicated tasks more effectively.' # cleanup vectorstore . delete_collection ( ) Check out the LangSmith\ntrace Detailed walkthrough ​ Let’s go through the above code step-by-step to really understand what’s\ngoing on. Step 1. Load ​ We need to first load the blog post contents. We can use DocumentLoader s for this, which are objects that load in data from a\nsource as Documents . A Document is an object with page_content (str) and metadata (dict) attributes. In this case we’ll use the WebBaseLoader , which uses urllib and BeautifulSoup to load and parse the passed in web urls, returning one Document per url. We can customize the html - > text parsing by\npassing in parameters to the BeautifulSoup parser via bs_kwargs (see BeautifulSoup\ndocs ).\nIn this case only HTML tags with class “post-content”, “post-title”, or\n“post-header” are relevant, so we’ll remove all others. from langchain . document_loaders import WebBaseLoader loader = WebBaseLoader ( web_paths = ( \"https://lilianweng.github.io/posts/2023-06-23-agent/\" , ) , bs_kwargs = { \"parse_only\" : bs4 . SoupStrainer ( class_ = ( \"post-content\" , \"post-title\" , \"post-header\" ) ) } , ) docs = loader . load ( ) len ( docs [ 0 ] . page_content ) 42824 print ( docs [ 0 ] . page_content [ : 500 ] ) LLM Powered Autonomous Agents Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview# In Go deeper ​ DocumentLoader : Object that load data from a source as Documents . - Docs : Further\ndocumentation on how to use DocumentLoader s. - Integrations : Find the relevant DocumentLoader integration (of the > 160 of them) for your use case. Step 2. Split ​ Our loaded document is over 42k characters long. This is too long to fit\nin the context window of many models. And even for those models that\ncould fit the full post in their context window, empirically models\nstruggle to find the relevant context in very long prompts. So we’ll split the Document into chunks for embedding and vector\nstorage. This should help us retrieve only the most relevant bits of the\nblog post at run time. In this case we’ll split our documents into chunks of 1000 characters\nwith 200 characters of overlap between chunks. The overlap helps\nmitigate the possibility of separating a statement from important\ncontext related to it. We use the RecursiveCharacterTextSplitter ,\nwhich will (recursively) split the document using common separators\n(like new lines) until each chunk is the appropriate size. from langchain . text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter ( chunk_size = 1000 , chunk_overlap = 200 , add_start_index = True ) all_splits = text_splitter . split_documents ( docs ) len ( all_splits ) 66 len ( all_splits [ 0 ] . page_content ) 969 all_splits [ 10 ] . metadata {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 7056} Go deeper ​ DocumentSplitter : Object that splits a list of Document s into\nsmaller chunks. Subclass of DocumentTransformer s. - Explore Context-aware splitters , which keep the location (“context”) of each\nsplit in the original Document : - Markdown\nfiles - Code (py or js) - Scientific papers DocumentTransformer : Object that performs a transformation on a list\nof Document s. - Docs : Further\ndocumentation on how to use DocumentTransformer s - Integrations Step 3. Store ​ Now that we’ve got 66 text chunks in memory, we need to store and index\nthem so that we can search them later in our RAG app. The most common\nway to do this is to embed the contents of each document split and\nupload those embeddings to a vector store. Then, when we want to search over our splits, we take the search query,\nembed it as well, and perform some sort of “similarity” search to\nidentify the stored splits with the most similar embeddings to our query\nembedding. The simplest similarity measure is cosine similarity — we\nmeasure the cosine of the angle between each pair of embeddings (which\nare just very high dimensional vectors). We can embed and store all of our document splits in a single command\nusing the Chroma vector store and OpenAIEmbeddings model. from langchain . embeddings import OpenAIEmbeddings from langchain . vectorstores import Chroma vectorstore = Chroma . from_documents ( documents = all_splits , embedding = OpenAIEmbeddings ( ) ) Go deeper ​ Embeddings : Wrapper around a text embedding model, used for converting\ntext to embeddings. - Docs : Further\ndocumentation on the interface. - Integrations : Browse the > 30\ntext embedding integrations VectorStore : Wrapper around a vector database, used for storing and\nquerying embeddings. - Docs : Further\ndocumentation on the interface. - Integrations : Browse the > 40 VectorStore integrations. This completes the Indexing portion of the pipeline. At this point\nwe have an query-able vector store containing the chunked contents of\nour blog post. Given a user question, we should ideally be able to\nreturn the snippets of the blog post that answer the question: Step 4. Retrieve ​ Now let’s write the actual application logic. We want to create a simple\napplication that let’s the user ask a question, searches for documents\nrelevant to that question, passes the retrieved documents and initial\nquestion to a model, and finally returns an answer. LangChain defines a Retriever interface which wraps an index that can\nreturn relevant documents given a string query. All retrievers implement\na common method get_relevant_documents() (and its asynchronous variant aget_relevant_documents() ). The most common type of Retriever is the VectorStoreRetriever , which\nuses the similarity search capabilities of a vector store to facillitate\nretrieval. Any VectorStore can easily be turned into a Retriever : retriever = vectorstore . as_retriever ( search_type = \"similarity\" , search_kwargs = { \"k\" : 6 } ) retrieved_docs = retriever . get_relevant_documents ( \"What are the approaches to Task Decomposition?\" ) len ( retrieved_docs ) 6 print ( retrieved_docs [ 0 ] . page_content ) Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote. Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs. Go deeper ​ Vector stores are commonly used for retrieval, but there are plenty of\nother ways to do retrieval. Retriever : An object that returns Document s given a text query - Docs : Further documentation\non the interface and built-in retrieval techniques. Some of which\ninclude: - MultiQueryRetriever generates variants of the input\nquestion to improve retrieval hit rate. - MultiVectorRetriever (diagram below)\ninstead generates variants of the\nembeddings , also\nin order to improve retrieval hit rate. - Max marginal relevance selects for relevance and\ndiversity among the retrieved documents to avoid passing in duplicate context. -\nDocuments can be filtered during vector store retrieval using metadata filters . - Integrations : Integrations with\nretrieval services. Step 5. Generate ​ Let’s put it all together into a chain that takes a question, retrieves\nrelevant documents, constructs a prompt, passes that to a model, and\nparses the output. We’ll use the gpt-3.5-turbo OpenAI chat model, but any LangChain LLM or ChatModel could be substituted in. from langchain . chat_models import ChatOpenAI llm = ChatOpenAI ( model_name = \"gpt-3.5-turbo\" , temperature = 0 ) We’ll use a prompt for RAG that is checked into the LangChain prompt hub\n( here ). from langchain import hub prompt = hub . pull ( \"rlm/rag-prompt\" ) print ( prompt . invoke ( { \"context\" : \"filler context\" , \"question\" : \"filler question\" } ) . to_string ( ) ) Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Question: filler question Context: filler context Answer: We’ll use the LCEL\nRunnable protocol to define the chain, allowing us to - pipe together components\nand functions in a transparent way - automatically trace our chain in\nLangSmith - get streaming, async, and batched calling out of the box from langchain . schema import StrOutputParser from langchain_core . runnables import RunnablePassthrough def format_docs ( docs ) : return \"\\n\\n\" . join ( doc . page_content for doc in docs ) rag_chain = ( { \"context\" : retriever | format_docs , \"question\" : RunnablePassthrough ( ) } | prompt | llm | StrOutputParser ( ) ) for chunk in rag_chain . stream ( \"What is Task Decomposition?\" ) : print ( chunk , end = \"\" , flush = True ) Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through methods like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the task into manageable subtasks and exploring multiple reasoning possibilities at each step. Task decomposition can be performed by AI models with prompting, task-specific instructions, or human inputs. Check out the LangSmith\ntrace Go deeper ​ Choosing LLMs ​ ChatModel : An LLM-backed chat model wrapper. Takes in a sequence of\nmessages and returns a message. - Docs - Integrations : Explore over 25 ChatModel integrations. LLM : A text-in-text-out LLM. Takes in a string and returns a string. - Docs - Integrations : Explore over 75 LLM integrations. See a guide on RAG with locally-running models here . Customizing the prompt ​ As shown above, we can load prompts (e.g., this RAG\nprompt ) from the prompt\nhub. The prompt can also be easily customized: from langchain . prompts import PromptTemplate template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum and keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. {context} Question: {question} Helpful Answer:\"\"\" rag_prompt_custom = PromptTemplate . from_template ( template ) rag_chain = ( { \"context\" : retriever | format_docs , \"question\" : RunnablePassthrough ( ) } | rag_prompt_custom | llm | StrOutputParser ( ) ) rag_chain . invoke ( \"What is Task Decomposition?\" ) 'Task decomposition is the process of breaking down a complex task into smaller and simpler steps. It can be done through techniques like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the problem into multiple thought steps and generating multiple thoughts per step. Task decomposition helps in enhancing model performance and understanding the thinking process of the model. Thanks for asking!' Check out the LangSmith\ntrace Adding sources ​ With LCEL it’s easy to return the retrieved documents or certain source\nmetadata from the documents: from operator import itemgetter from langchain_core . runnables import RunnableParallel rag_chain_from_docs = ( { \"context\" : lambda input : format_docs ( input [ \"documents\" ] ) , \"question\" : itemgetter ( \"question\" ) , } | rag_prompt_custom | llm | StrOutputParser ( ) ) rag_chain_with_source = RunnableParallel ( { \"documents\" : retriever , \"question\" : RunnablePassthrough ( ) } ) | { \"documents\" : lambda input : [ doc . metadata for doc in input [ \"documents\" ] ] , \"answer\" : rag_chain_from_docs , } rag_chain_with_source . invoke ( \"What is Task Decomposition\" ) {'documents': [{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17804}, {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17414}, {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}, {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}], 'answer': 'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!'} Check out the LangSmith\ntrace Adding memory ​ Suppose we want to create a stateful application that remembers past\nuser inputs. There are two main things we need to do to support this. 1.\nAdd a messages placeholder to our chain which allows us to pass in\nhistorical messages 2. Add a chain that takes the latest user query and\nreformulates it in the context of the chat history into a standalone\nquestion that can be passed to our retriever. Let’s start with 2. We can build a “condense question” chain that looks\nsomething like this: from langchain . prompts import ChatPromptTemplate , MessagesPlaceholder condense_q_system_prompt = \"\"\"Given a chat history and the latest user question \\ which might reference the chat history, formulate a standalone question \\ which can be understood without the chat history. Do NOT answer the question, \\ just reformulate it if needed and otherwise return it as is.\"\"\" condense_q_prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , condense_q_system_prompt ) , MessagesPlaceholder ( variable_name = \"chat_history\" ) , ( \"human\" , \"{question}\" ) , ] ) condense_q_chain = condense_q_prompt | llm | StrOutputParser ( ) from langchain_core . messages import AIMessage , HumanMessage condense_q_chain . invoke ( { \"chat_history\" : [ HumanMessage ( content = \"What does LLM stand for?\" ) , AIMessage ( content = \"Large language model\" ) , ] , \"question\" : \"What is meant by large\" , } ) 'What is the definition of \"large\" in the context of a language model?' condense_q_chain . invoke ( { \"chat_history\" : [ HumanMessage ( content = \"What does LLM stand for?\" ) , AIMessage ( content = \"Large language model\" ) , ] , \"question\" : \"How do transformers work\" , } ) 'How do transformer models function?' And now we can build our full QA chain. Notice we add some routing\nfunctionality to only run the “condense question chain” when our chat\nhistory isn’t empty. qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\ Use the following pieces of retrieved context to answer the question. \\ If you don't know the answer, just say that you don't know. \\ Use three sentences maximum and keep the answer concise.\\ {context}\"\"\" qa_prompt = ChatPromptTemplate . from_messages ( [ ( \"system\" , qa_system_prompt ) , MessagesPlaceholder ( variable_name = \"chat_history\" ) , ( \"human\" , \"{question}\" ) , ] ) def condense_question ( input : dict ) : if input . get ( \"chat_history\" ) : return condense_q_chain else : return input [ \"question\" ] rag_chain = ( RunnablePassthrough . assign ( context = condense_question | retriever | format_docs ) | qa_prompt | llm ) chat_history = [ ] question = \"What is Task Decomposition?\" ai_msg = rag_chain . invoke ( { \"question\" : question , \"chat_history\" : chat_history } ) chat_history . extend ( [ HumanMessage ( content = question ) , ai_msg ] ) second_question = \"What are common ways of doing it?\" rag_chain . invoke ( { \"question\" : second_question , \"chat_history\" : chat_history } ) AIMessage(content='Common ways of task decomposition include:\\n\\n1. Using Chain of Thought (CoT): CoT is a prompting technique that instructs the model to \"think step by step\" and decompose complex tasks into smaller and simpler steps. It utilizes more test-time computation and sheds light on the model\\'s thinking process.\\n\\n2. Prompting with LLM: Language Model (LLM) can be used to prompt the model with simple instructions like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" This allows the model to generate a sequence of subtasks or thought steps.\\n\\n3. Task-specific instructions: For certain tasks, task-specific instructions can be provided to guide the model in decomposing the task. For example, for writing a novel, the instruction \"Write a story outline\" can be given to break down the task into manageable steps.\\n\\n4. Human inputs: In some cases, human inputs can be used to assist in task decomposition. Humans can provide their expertise and knowledge to identify and break down complex tasks into smaller subtasks.') Check out the LangSmith\ntrace Here we’ve gone over how to add chain logic for incorporating historical\noutputs. But how do we actually store and retrieve historical outputs\nfor different sessions? For that check out the LCEL How to add message\nhistory (memory) page. Next steps ​ That’s a lot of content we’ve covered in a short amount of time. There’s\nplenty of nuances, features, integrations, etc to explore in each of the\nabove sections. Aside from the sources mentioned above, good next steps\ninclude: Reading up on more advanced retrieval techniques in the Retrievers section. Learning about the LangChain Indexing\nAPI , which helps repeatedly\nsync data sources and vector stores without redundant computation or\nstorage. Exploring RAG LangChain\nTemplates , which are\nreference applications that can easily be deployed with LangServe . Learning about evaluating RAG applications with\nLangSmith . Previous SQL Next RAG over code"
    },
    {
        "title": "Anthropic Functions | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/chat/anthropic_functions",
        "text_content": "Components Chat models Anthropic Functions On this page Anthropic Functions This notebook shows how to use an experimental wrapper around Anthropic\nthat gives it the same API as OpenAI Functions. from langchain_experimental . llms . anthropic_functions import AnthropicFunctions /Users/harrisonchase/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.14) is available. It's recommended that you update to the latest version using `pip install -U deeplake`. warnings.warn( Initialize Model ​ You can initialize this wrapper the same way you’d initialize\nChatAnthropic model = AnthropicFunctions ( model = \"claude-2\" ) Passing in functions ​ You can now pass in functions in a similar way functions = [ { \"name\" : \"get_current_weather\" , \"description\" : \"Get the current weather in a given location\" , \"parameters\" : { \"type\" : \"object\" , \"properties\" : { \"location\" : { \"type\" : \"string\" , \"description\" : \"The city and state, e.g. San Francisco, CA\" , } , \"unit\" : { \"type\" : \"string\" , \"enum\" : [ \"celsius\" , \"fahrenheit\" ] } , } , \"required\" : [ \"location\" ] , } , } ] from langchain . schema import HumanMessage response = model . predict_messages ( [ HumanMessage ( content = \"whats the weater in boston?\" ) ] , functions = functions ) response AIMessage(content=' ', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"Boston, MA\", \"unit\": \"fahrenheit\"}'}}, example=False) Using for extraction ​ You can now use this for extraction. from langchain . chains import create_extraction_chain schema = { \"properties\" : { \"name\" : { \"type\" : \"string\" } , \"height\" : { \"type\" : \"integer\" } , \"hair_color\" : { \"type\" : \"string\" } , } , \"required\" : [ \"name\" , \"height\" ] , } inp = \"\"\" Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde. \"\"\" chain = create_extraction_chain ( schema , model ) chain . run ( inp ) [{'name': 'Alex', 'height': '5', 'hair_color': 'blonde'}, {'name': 'Claudia', 'height': '6', 'hair_color': 'brunette'}] Using for tagging ​ You can now use this for tagging from langchain . chains import create_tagging_chain schema = { \"properties\" : { \"sentiment\" : { \"type\" : \"string\" } , \"aggressiveness\" : { \"type\" : \"integer\" } , \"language\" : { \"type\" : \"string\" } , } } chain = create_tagging_chain ( schema , model ) chain . run ( \"this is really cool\" ) {'sentiment': 'positive', 'aggressiveness': '0', 'language': 'english'} Previous Anthropic Next Anyscale"
    },
    {
        "title": "Llama.cpp | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/llms/llamacpp",
        "text_content": "Components LLMs Llama.cpp On this page Llama.cpp llama-cpp-python is a\nPython binding for llama.cpp . It supports inference for many\nLLMs models, which\ncan be accessed on Hugging Face . This notebook goes over how to run llama-cpp-python within LangChain. Note: new versions of llama-cpp-python use GGUF model files (see here ). This is a breaking change. To convert existing GGML models to GGUF you can run the following in llama.cpp : python ./convert-llama-ggmlv3-to-gguf.py --eps 1e-5 --input models/openorca-platypus2-13b.ggmlv3.q4_0.bin --output models/openorca-platypus2-13b.gguf.q4_0.bin Installation ​ There are different options on how to install the llama-cpp package: -\nCPU usage - CPU + GPU (using one of many BLAS backends) - Metal GPU\n(MacOS with Apple Silicon Chip) CPU only installation ​ !pip install llama - cpp - python Installation with OpenBLAS / cuBLAS / CLBlast ​ llama.cpp supports multiple BLAS backends for faster processing. Use\nthe FORCE_CMAKE=1 environment variable to force the use of cmake and\ninstall the pip package for the desired BLAS backend\n( source ). Example installation with cuBLAS backend: !CMAKE_ARGS = \"-DLLAMA_CUBLAS=on\" FORCE_CMAKE = 1 pip install llama - cpp - python IMPORTANT : If you have already installed the CPU only version of the\npackage, you need to reinstall it from scratch. Consider the following\ncommand: !CMAKE_ARGS = \"-DLLAMA_CUBLAS=on\" FORCE_CMAKE = 1 pip install - - upgrade - - force - reinstall llama - cpp - python - - no - cache - dir Installation with Metal ​ llama.cpp supports Apple silicon first-class citizen - optimized via\nARM NEON, Accelerate and Metal frameworks. Use the FORCE_CMAKE=1 environment variable to force the use of cmake and install the pip\npackage for the Metal support\n( source ). Example installation with Metal Support: !CMAKE_ARGS = \"-DLLAMA_METAL=on\" FORCE_CMAKE = 1 pip install llama - cpp - python IMPORTANT : If you have already installed a cpu only version of the\npackage, you need to reinstall it from scratch: consider the following\ncommand: !CMAKE_ARGS = \"-DLLAMA_METAL=on\" FORCE_CMAKE = 1 pip install - - upgrade - - force - reinstall llama - cpp - python - - no - cache - dir Installation with Windows ​ It is stable to install the llama-cpp-python library by compiling from\nthe source. You can follow most of the instructions in the repository\nitself but there are some windows specific instructions which might be\nuseful. Requirements to install the llama-cpp-python , git python cmake Visual Studio Community (make sure you install this with the\nfollowing settings) Desktop development with C++ Python development Linux embedded development with C++ Clone git repository recursively to get llama.cpp submodule as\nwell git clone --recursive -j8 https://github.com/abetlen/llama-cpp-python.git Open up command Prompt (or anaconda prompt if you have it\ninstalled), set up environment variables to install. Follow this if\nyou do not have a GPU, you must set both of the following variables. set FORCE_CMAKE=1 set CMAKE_ARGS=-DLLAMA_CUBLAS=OFF You can ignore the second environment variable if you have an NVIDIA\nGPU. Compiling and installing ​ In the same command prompt (anaconda prompt) you set the variables, you\ncan cd into llama-cpp-python directory and run the following\ncommands. python setup.py clean python setup.py install Usage ​ Make sure you are following all instructions to install all necessary\nmodel files . You don’t need an API_TOKEN as you will run the LLM locally. It is worth understanding which models are suitable to be used on the\ndesired machine. TheBloke’s Hugging Face models have a Provided files section that exposes the RAM required to run models of\ndifferent quantisation sizes and methods (eg: Llama2-7B-Chat-GGUF ). This github\nissue is also\nrelevant to find the right model for your machine. from langchain . callbacks . manager import CallbackManager from langchain . callbacks . streaming_stdout import StreamingStdOutCallbackHandler from langchain . chains import LLMChain from langchain . llms import LlamaCpp from langchain . prompts import PromptTemplate Consider using a template that suits your model! Check the models page\non Hugging Face etc. to get a correct prompting template. template = \"\"\"Question: {question} Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\" prompt = PromptTemplate ( template = template , input_variables = [ \"question\" ] ) # Callbacks support token-wise streaming callback_manager = CallbackManager ( [ StreamingStdOutCallbackHandler ( ) ] ) CPU ​ Example using a LLaMA 2 7B model # Make sure the model path is correct for your system! llm = LlamaCpp ( model_path = \"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\" , temperature = 0.75 , max_tokens = 2000 , top_p = 1 , callback_manager = callback_manager , verbose = True , # Verbose is required to pass to the callback manager ) prompt = \"\"\" Question: A rap battle between Stephen Colbert and John Oliver \"\"\" llm ( prompt ) Stephen Colbert: Yo, John, I heard you've been talkin' smack about me on your show. Let me tell you somethin', pal, I'm the king of late-night TV My satire is sharp as a razor, it cuts deeper than a knife While you're just a british bloke tryin' to be funny with your accent and your wit. John Oliver: Oh Stephen, don't be ridiculous, you may have the ratings but I got the real talk. My show is the one that people actually watch and listen to, not just for the laughs but for the facts. While you're busy talkin' trash, I'm out here bringing the truth to light. Stephen Colbert: Truth? Ha! You think your show is about truth? Please, it's all just a joke to you. You're just a fancy-pants british guy tryin' to be funny with your news and your jokes. While I'm the one who's really makin' a difference, with my sat llama_print_timings:        load time =   358.60 ms llama_print_timings:      sample time =   172.55 ms /   256 runs   (    0.67 ms per token,  1483.59 tokens per second) llama_print_timings: prompt eval time =   613.36 ms /    16 tokens (   38.33 ms per token,    26.09 tokens per second) llama_print_timings:        eval time = 10151.17 ms /   255 runs   (   39.81 ms per token,    25.12 tokens per second) llama_print_timings:       total time = 11332.41 ms \"\\nStephen Colbert:\\nYo, John, I heard you've been talkin' smack about me on your show.\\nLet me tell you somethin', pal, I'm the king of late-night TV\\nMy satire is sharp as a razor, it cuts deeper than a knife\\nWhile you're just a british bloke tryin' to be funny with your accent and your wit.\\nJohn Oliver:\\nOh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.\\nMy show is the one that people actually watch and listen to, not just for the laughs but for the facts.\\nWhile you're busy talkin' trash, I'm out here bringing the truth to light.\\nStephen Colbert:\\nTruth? Ha! You think your show is about truth? Please, it's all just a joke to you.\\nYou're just a fancy-pants british guy tryin' to be funny with your news and your jokes.\\nWhile I'm the one who's really makin' a difference, with my sat\" Example using a LLaMA v1 model # Make sure the model path is correct for your system! llm = LlamaCpp ( model_path = \"./ggml-model-q4_0.bin\" , callback_manager = callback_manager , verbose = True ) llm_chain = LLMChain ( prompt = prompt , llm = llm ) question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\" llm_chain . run ( question ) 1. First, find out when Justin Bieber was born. 2. We know that Justin Bieber was born on March 1, 1994. 3. Next, we need to look up when the Super Bowl was played in that year. 4. The Super Bowl was played on January 28, 1995. 5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers. llama_print_timings:        load time =   434.15 ms llama_print_timings:      sample time =    41.81 ms /   121 runs   (    0.35 ms per token) llama_print_timings: prompt eval time =  2523.78 ms /    48 tokens (   52.58 ms per token) llama_print_timings:        eval time = 23971.57 ms /   121 runs   (  198.11 ms per token) llama_print_timings:       total time = 28945.95 ms '\\n\\n1. First, find out when Justin Bieber was born.\\n2. We know that Justin Bieber was born on March 1, 1994.\\n3. Next, we need to look up when the Super Bowl was played in that year.\\n4. The Super Bowl was played on January 28, 1995.\\n5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.' GPU ​ If the installation with BLAS backend was correct, you will see a BLAS = 1 indicator in model properties. Two of the most important parameters for use with GPU are: n_gpu_layers - determines how many layers of the model are\noffloaded to your GPU. n_batch - how many tokens are processed in parallel. Setting these parameters correctly will dramatically improve the\nevaluation speed (see wrapper\ncode for more details). n_gpu_layers = 40 # Change this value based on your model and your GPU VRAM pool. n_batch = 512 # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU. # Make sure the model path is correct for your system! llm = LlamaCpp ( model_path = \"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\" , n_gpu_layers = n_gpu_layers , n_batch = n_batch , callback_manager = callback_manager , verbose = True , # Verbose is required to pass to the callback manager ) llm_chain = LLMChain ( prompt = prompt , llm = llm ) question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\" llm_chain . run ( question ) 1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994. 2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994. 3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup. So, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl. llama_print_timings:        load time =   427.63 ms llama_print_timings:      sample time =   115.85 ms /   164 runs   (    0.71 ms per token,  1415.67 tokens per second) llama_print_timings: prompt eval time =   427.53 ms /    45 tokens (    9.50 ms per token,   105.26 tokens per second) llama_print_timings:        eval time =  4526.53 ms /   163 runs   (   27.77 ms per token,    36.01 tokens per second) llama_print_timings:       total time =  5293.77 ms \"\\n\\n1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.\\n\\n2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.\\n\\n3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.\\n\\nSo, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl.\" Metal ​ If the installation with Metal was correct, you will see a NEON = 1 indicator in model properties. Two of the most important GPU parameters are: n_gpu_layers - determines how many layers of the model are\noffloaded to your Metal GPU, in the most case, set it to 1 is\nenough for Metal n_batch - how many tokens are processed in parallel, default is 8,\nset to bigger number. f16_kv - for some reason, Metal only support True , otherwise you\nwill get error such as Asserting on type 0 GGML_ASSERT: .../ggml-metal.m:706: false && \"not implemented\" Setting these parameters correctly will dramatically improve the\nevaluation speed (see wrapper\ncode for more details). n_gpu_layers = 1 # Metal set to 1 is enough. n_batch = 512 # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip. # Make sure the model path is correct for your system! llm = LlamaCpp ( model_path = \"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\" , n_gpu_layers = n_gpu_layers , n_batch = n_batch , f16_kv = True , # MUST set to True, otherwise you will run into problem after a couple of calls callback_manager = callback_manager , verbose = True , # Verbose is required to pass to the callback manager ) The console log will show the following log to indicate Metal was enable\nproperly. ggml_metal_init: allocating ggml_metal_init: using MPS ... You also could check Activity Monitor by watching the GPU usage of the\nprocess, the CPU usage will drop dramatically after turn on n_gpu_layers=1 . For the first call to the LLM, the performance may be slow due to the\nmodel compilation in Metal GPU. Grammars ​ We can use grammars to constrain model outputs and sample tokens based on the rules defined\nin them. To demonstrate this concept, we’ve included sample grammar\nfiles ,\nthat will be used in the examples below. Creating gbnf grammar files can be time-consuming, but if you have a\nuse-case where output schemas are important, there are two tools that\ncan help: - Online grammar generator\napp that converts TypeScript\ninterface definitions to gbnf file. - Python\nscript for converting json schema to gbnf file. You can for example create pydantic object, generate its JSON schema using .schema_json() method, and then use this script to convert it to gbnf file. In the first example, supply the path to the specified json.gbnf file\nin order to produce JSON: n_gpu_layers = 1 # Metal set to 1 is enough. n_batch = 512 # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip. # Make sure the model path is correct for your system! llm = LlamaCpp ( model_path = \"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\" , n_gpu_layers = n_gpu_layers , n_batch = n_batch , f16_kv = True , # MUST set to True, otherwise you will run into problem after a couple of calls callback_manager = callback_manager , verbose = True , # Verbose is required to pass to the callback manager grammar_path = \"/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/json.gbnf\" , ) % % capture captured - - no - stdout result = llm ( \"Describe a person in JSON format:\" ) { \"name\": \"John Doe\", \"age\": 34, \"\": { \"title\": \"Software Developer\", \"company\": \"Google\" }, \"interests\": [ \"Sports\", \"Music\", \"Cooking\" ], \"address\": { \"street_number\": 123, \"street_name\": \"Oak Street\", \"city\": \"Mountain View\", \"state\": \"California\", \"postal_code\": 94040 }} llama_print_timings:        load time =   357.51 ms llama_print_timings:      sample time =  1213.30 ms /   144 runs   (    8.43 ms per token,   118.68 tokens per second) llama_print_timings: prompt eval time =   356.78 ms /     9 tokens (   39.64 ms per token,    25.23 tokens per second) llama_print_timings:        eval time =  3947.16 ms /   143 runs   (   27.60 ms per token,    36.23 tokens per second) llama_print_timings:       total time =  5846.21 ms We can also supply list.gbnf to return a list: n_gpu_layers = 1 n_batch = 512 llm = LlamaCpp ( model_path = \"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\" , n_gpu_layers = n_gpu_layers , n_batch = n_batch , f16_kv = True , # MUST set to True, otherwise you will run into problem after a couple of calls callback_manager = callback_manager , verbose = True , grammar_path = \"/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/list.gbnf\" , ) % % capture captured - - no - stdout result = llm ( \"List of top-3 my favourite books:\" ) [\"The Catcher in the Rye\", \"Wuthering Heights\", \"Anna Karenina\"] llama_print_timings:        load time =   322.34 ms llama_print_timings:      sample time =   232.60 ms /    26 runs   (    8.95 ms per token,   111.78 tokens per second) llama_print_timings: prompt eval time =   321.90 ms /    11 tokens (   29.26 ms per token,    34.17 tokens per second) llama_print_timings:        eval time =   680.82 ms /    25 runs   (   27.23 ms per token,    36.72 tokens per second) llama_print_timings:       total time =  1295.27 ms Previous KoboldAI API Next LLM Caching integrations"
    },
    {
        "title": "JSONFormer | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/llms/jsonformer_experimental",
        "text_content": "Components LLMs JSONFormer On this page JSONFormer JSONFormer is a library that wraps\nlocal Hugging Face pipeline models for structured decoding of a subset\nof the JSON Schema. It works by filling in the structure tokens and then sampling the\ncontent tokens from the model. Warning - this module is still experimental !pip install - - upgrade jsonformer > / dev / null Hugging Face Baseline ​ First, let’s establish a qualitative baseline by checking the output of\nthe model without structured decoding. import logging logging . basicConfig ( level = logging . ERROR ) import json import os import requests from langchain . tools import tool HF_TOKEN = os . environ . get ( \"HUGGINGFACE_API_KEY\" ) @tool def ask_star_coder ( query : str , temperature : float = 1.0 , max_new_tokens : float = 250 ) : \"\"\"Query the BigCode StarCoder model about coding questions.\"\"\" url = \"https://api-inference.huggingface.co/models/bigcode/starcoder\" headers = { \"Authorization\" : f\"Bearer { HF_TOKEN } \" , \"content-type\" : \"application/json\" , } payload = { \"inputs\" : f\" { query } \\n\\nAnswer:\" , \"temperature\" : temperature , \"max_new_tokens\" : int ( max_new_tokens ) , } response = requests . post ( url , headers = headers , data = json . dumps ( payload ) ) response . raise_for_status ( ) return json . loads ( response . content . decode ( \"utf-8\" ) ) prompt = \"\"\"You must respond using JSON format, with a single action and single action input. You may 'ask_star_coder' for help on coding problems. {arg_schema} EXAMPLES ---- Human: \"So what's all this about a GIL?\" AI Assistant:{{ \"action\": \"ask_star_coder\", \"action_input\": {{\"query\": \"What is a GIL?\", \"temperature\": 0.0, \"max_new_tokens\": 100}}\" }} Observation: \"The GIL is python's Global Interpreter Lock\" Human: \"Could you please write a calculator program in LISP?\" AI Assistant:{{ \"action\": \"ask_star_coder\", \"action_input\": {{\"query\": \"Write a calculator program in LISP\", \"temperature\": 0.0, \"max_new_tokens\": 250}} }} Observation: \"(defun add (x y) (+ x y))\\n(defun sub (x y) (- x y ))\" Human: \"What's the difference between an SVM and an LLM?\" AI Assistant:{{ \"action\": \"ask_star_coder\", \"action_input\": {{\"query\": \"What's the difference between SGD and an SVM?\", \"temperature\": 1.0, \"max_new_tokens\": 250}} }} Observation: \"SGD stands for stochastic gradient descent, while an SVM is a Support Vector Machine.\" BEGIN! Answer the Human's question as best as you are able. ------ Human: 'What's the difference between an iterator and an iterable?' AI Assistant:\"\"\" . format ( arg_schema = ask_star_coder . args ) from langchain . llms import HuggingFacePipeline from transformers import pipeline hf_model = pipeline ( \"text-generation\" , model = \"cerebras/Cerebras-GPT-590M\" , max_new_tokens = 200 ) original_model = HuggingFacePipeline ( pipeline = hf_model ) generated = original_model . predict ( prompt , stop = [ \"Observation:\" , \"Human:\" ] ) print ( generated ) Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation. 'What's the difference between an iterator and an iterable?' That’s not so impressive, is it? It didn’t follow the JSON format at\nall! Let’s try with the structured decoder. JSONFormer LLM Wrapper ​ Let’s try that again, now providing a the Action input’s JSON Schema to\nthe model. decoder_schema = { \"title\" : \"Decoding Schema\" , \"type\" : \"object\" , \"properties\" : { \"action\" : { \"type\" : \"string\" , \"default\" : ask_star_coder . name } , \"action_input\" : { \"type\" : \"object\" , \"properties\" : ask_star_coder . args , } , } , } from langchain_experimental . llms import JsonFormer json_former = JsonFormer ( json_schema = decoder_schema , pipeline = hf_model ) results = json_former . predict ( prompt , stop = [ \"Observation:\" , \"Human:\" ] ) print ( results ) {\"action\": \"ask_star_coder\", \"action_input\": {\"query\": \"What's the difference between an iterator and an iter\", \"temperature\": 0.0, \"max_new_tokens\": 50.0}} Voila! Free of parsing errors. Previous Javelin AI Gateway Tutorial Next KoboldAI API"
    },
    {
        "title": "Stuff | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/chains/document/stuff",
        "text_content": "Modules More Chains Documents Stuff On this page Stuff The stuff documents chain (“stuff” as in “to stuff” or “to fill”) is the\nmost straightforward of the document chains. It takes a list of\ndocuments, inserts them all into a prompt and passes that prompt to an\nLLM. This chain is well-suited for applications where documents are small and\nonly a few are passed in for most calls. Recreating with LCEL ​ With LangChain Expression Language we can\neasily recreate the StuffDocumentsChain functionality, with the\nadditional benefit of getting all the built-in LCEL features (batch,\nasync, etc.) and with much more ability to customize specific parts of\nthe chain. from langchain . chat_models import ChatAnthropic from langchain . prompts import PromptTemplate from langchain . schema import StrOutputParser from langchain_core . prompts import format_document doc_prompt = PromptTemplate . from_template ( \"{page_content}\" ) chain = ( { \"content\" : lambda docs : \"\\n\\n\" . join ( format_document ( doc , doc_prompt ) for doc in docs ) } | PromptTemplate . from_template ( \"Summarize the following content:\\n\\n{content}\" ) | ChatAnthropic ( ) | StrOutputParser ( ) ) Example run ​ Lets run this summarization chain on some sample data. from langchain . schema import Document text = \"\"\"Nuclear power in space is the use of nuclear power in outer space, typically either small fission systems or radioactive decay for electricity or heat. Another use is for scientific observation, as in a Mössbauer spectrometer. The most common type is a radioisotope thermoelectric generator, which has been used on many space probes and on crewed lunar missions. Small fission reactors for Earth observation satellites, such as the TOPAZ nuclear reactor, have also been flown.[1] A radioisotope heater unit is powered by radioactive decay and can keep components from becoming too cold to function, potentially over a span of decades.[2] The United States tested the SNAP-10A nuclear reactor in space for 43 days in 1965,[3] with the next test of a nuclear reactor power system intended for space use occurring on 13 September 2012 with the Demonstration Using Flattop Fission (DUFF) test of the Kilopower reactor.[4] After a ground-based test of the experimental 1965 Romashka reactor, which used uranium and direct thermoelectric conversion to electricity,[5] the USSR sent about 40 nuclear-electric satellites into space, mostly powered by the BES-5 reactor. The more powerful TOPAZ-II reactor produced 10 kilowatts of electricity.[3] Examples of concepts that use nuclear power for space propulsion systems include the nuclear electric rocket (nuclear powered ion thruster(s)), the radioisotope rocket, and radioisotope electric propulsion (REP).[6] One of the more explored concepts is the nuclear thermal rocket, which was ground tested in the NERVA program. Nuclear pulse propulsion was the subject of Project Orion.[7] Regulation and hazard prevention[edit] After the ban of nuclear weapons in space by the Outer Space Treaty in 1967, nuclear power has been discussed at least since 1972 as a sensitive issue by states.[8] Particularly its potential hazards to Earth's environment and thus also humans has prompted states to adopt in the U.N. General Assembly the Principles Relevant to the Use of Nuclear Power Sources in Outer Space (1992), particularly introducing safety principles for launches and to manage their traffic.[8] Benefits Both the Viking 1 and Viking 2 landers used RTGs for power on the surface of Mars. (Viking launch vehicle pictured) While solar power is much more commonly used, nuclear power can offer advantages in some areas. Solar cells, although efficient, can only supply energy to spacecraft in orbits where the solar flux is sufficiently high, such as low Earth orbit and interplanetary destinations close enough to the Sun. Unlike solar cells, nuclear power systems function independently of sunlight, which is necessary for deep space exploration. Nuclear-based systems can have less mass than solar cells of equivalent power, allowing more compact spacecraft that are easier to orient and direct in space. In the case of crewed spaceflight, nuclear power concepts that can power both life support and propulsion systems may reduce both cost and flight time.[9] Selected applications and/or technologies for space include: Radioisotope thermoelectric generator Radioisotope heater unit Radioisotope piezoelectric generator Radioisotope rocket Nuclear thermal rocket Nuclear pulse propulsion Nuclear electric rocket \"\"\" docs = [ Document ( page_content = split , metadata = { \"source\" : \"https://en.wikipedia.org/wiki/Nuclear_power_in_space\" } , ) for split in text . split ( ) ] print ( chain . invoke ( docs ) ) Here is a summary of the key points: - Nuclear power has been used in space for electricity, heat, and scientific observation. The most common type is a radioisotope thermoelectric generator, used on many probes and lunar missions. - Small fission reactors have been used for Earth observation satellites. Radioisotope heater units use radioactive decay to keep components warm for decades. - The US tested a nuclear reactor in space in 1965. The Soviet Union launched around 40 nuclear-powered satellites, mostly with BES-5 reactors. - Concepts for nuclear propulsion include nuclear thermal rockets, nuclear electric rockets, and nuclear pulse propulsion. The NERVA program ground tested nuclear thermal rockets. - After the 1967 Outer Space Treaty banned nuclear weapons in space, safety principles were introduced for nuclear power launch and traffic management. - Benefits of nuclear power in space include functioning independently of sunlight needed for deep space exploration, less mass than equivalent solar power, and ability to power both life support and propulsion. Previous Documents Next Refine"
    },
    {
        "title": "Map reduce | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/chains/document/map_reduce",
        "text_content": "Modules More Chains Documents Map reduce On this page Map reduce The map reduce documents chain first applies an LLM chain to each\ndocument individually (the Map step), treating the chain output as a new\ndocument. It then passes all the new documents to a separate combine\ndocuments chain to get a single output (the Reduce step). It can\noptionally first compress, or collapse, the mapped documents to make\nsure that they fit in the combine documents chain (which will often pass\nthem to an LLM). This compression step is performed recursively if\nnecessary. Recreating with LCEL ​ With LangChain Expression Language we can\nrecreate the MapReduceDocumentsChain functionality, with the\nadditional benefit of getting all the built-in LCEL features (batch,\nasync, etc.) and with much more ability to customize specific parts of\nthe chain. from functools import partial from langchain . chains . combine_documents import collapse_docs , split_list_of_docs from langchain . chat_models import ChatAnthropic from langchain . prompts import PromptTemplate from langchain . schema import StrOutputParser from langchain_core . prompts import format_document from langchain_core . runnables import RunnableParallel , RunnablePassthrough llm = ChatAnthropic ( ) # Prompt and method for converting Document -> str. document_prompt = PromptTemplate . from_template ( \"{page_content}\" ) partial_format_document = partial ( format_document , prompt = document_prompt ) # The chain we'll apply to each individual document. # Returns a summary of the document. map_chain = ( { \"context\" : partial_format_document } | PromptTemplate . from_template ( \"Summarize this content:\\n\\n{context}\" ) | llm | StrOutputParser ( ) ) # A wrapper chain to keep the original Document metadata map_as_doc_chain = ( RunnableParallel ( { \"doc\" : RunnablePassthrough ( ) , \"content\" : map_chain } ) | ( lambda x : Document ( page_content = x [ \"content\" ] , metadata = x [ \"doc\" ] . metadata ) ) ) . with_config ( run_name = \"Summarize (return doc)\" ) # The chain we'll repeatedly apply to collapse subsets of the documents # into a consolidate document until the total token size of our # documents is below some max size. def format_docs ( docs ) : return \"\\n\\n\" . join ( partial_format_document ( doc ) for doc in docs ) collapse_chain = ( { \"context\" : format_docs } | PromptTemplate . from_template ( \"Collapse this content:\\n\\n{context}\" ) | llm | StrOutputParser ( ) ) def get_num_tokens ( docs ) : return llm . get_num_tokens ( format_docs ( docs ) ) def collapse ( docs , config , token_max = 4000 , ) : collapse_ct = 1 while get_num_tokens ( docs ) > token_max : config [ \"run_name\" ] = f\"Collapse { collapse_ct } \" invoke = partial ( collapse_chain . invoke , config = config ) split_docs = split_list_of_docs ( docs , get_num_tokens , token_max ) docs = [ collapse_docs ( _docs , invoke ) for _docs in split_docs ] collapse_ct += 1 return docs # The chain we'll use to combine our individual document summaries # (or summaries over subset of documents if we had to collapse the map results) # into a final summary. reduce_chain = ( { \"context\" : format_docs } | PromptTemplate . from_template ( \"Combine these summaries:\\n\\n{context}\" ) | llm | StrOutputParser ( ) ) . with_config ( run_name = \"Reduce\" ) # The final full chain map_reduce = ( map_as_doc_chain . map ( ) | collapse | reduce_chain ) . with_config ( run_name = \"Map reduce\" ) Example run ​ from langchain . schema import Document text = \"\"\"Nuclear power in space is the use of nuclear power in outer space, typically either small fission systems or radioactive decay for electricity or heat. Another use is for scientific observation, as in a Mössbauer spectrometer. The most common type is a radioisotope thermoelectric generator, which has been used on many space probes and on crewed lunar missions. Small fission reactors for Earth observation satellites, such as the TOPAZ nuclear reactor, have also been flown.[1] A radioisotope heater unit is powered by radioactive decay and can keep components from becoming too cold to function, potentially over a span of decades.[2] The United States tested the SNAP-10A nuclear reactor in space for 43 days in 1965,[3] with the next test of a nuclear reactor power system intended for space use occurring on 13 September 2012 with the Demonstration Using Flattop Fission (DUFF) test of the Kilopower reactor.[4] After a ground-based test of the experimental 1965 Romashka reactor, which used uranium and direct thermoelectric conversion to electricity,[5] the USSR sent about 40 nuclear-electric satellites into space, mostly powered by the BES-5 reactor. The more powerful TOPAZ-II reactor produced 10 kilowatts of electricity.[3] Examples of concepts that use nuclear power for space propulsion systems include the nuclear electric rocket (nuclear powered ion thruster(s)), the radioisotope rocket, and radioisotope electric propulsion (REP).[6] One of the more explored concepts is the nuclear thermal rocket, which was ground tested in the NERVA program. Nuclear pulse propulsion was the subject of Project Orion.[7] Regulation and hazard prevention[edit] After the ban of nuclear weapons in space by the Outer Space Treaty in 1967, nuclear power has been discussed at least since 1972 as a sensitive issue by states.[8] Particularly its potential hazards to Earth's environment and thus also humans has prompted states to adopt in the U.N. General Assembly the Principles Relevant to the Use of Nuclear Power Sources in Outer Space (1992), particularly introducing safety principles for launches and to manage their traffic.[8] Benefits Both the Viking 1 and Viking 2 landers used RTGs for power on the surface of Mars. (Viking launch vehicle pictured) While solar power is much more commonly used, nuclear power can offer advantages in some areas. Solar cells, although efficient, can only supply energy to spacecraft in orbits where the solar flux is sufficiently high, such as low Earth orbit and interplanetary destinations close enough to the Sun. Unlike solar cells, nuclear power systems function independently of sunlight, which is necessary for deep space exploration. Nuclear-based systems can have less mass than solar cells of equivalent power, allowing more compact spacecraft that are easier to orient and direct in space. In the case of crewed spaceflight, nuclear power concepts that can power both life support and propulsion systems may reduce both cost and flight time.[9] Selected applications and/or technologies for space include: Radioisotope thermoelectric generator Radioisotope heater unit Radioisotope piezoelectric generator Radioisotope rocket Nuclear thermal rocket Nuclear pulse propulsion Nuclear electric rocket \"\"\" docs = [ Document ( page_content = split , metadata = { \"source\" : \"https://en.wikipedia.org/wiki/Nuclear_power_in_space\" } , ) for split in text . split ( \"\\n\\n\" ) ] print ( map_reduce . invoke ( docs [ 0 : 1 ] , config = { \"max_concurrency\" : 5 } ) ) Here is a summary that combines the key points about nuclear power in space: Nuclear power is used in space for electricity, heat, and scientific observation. The most common type is a radioisotope thermoelectric generator, which has powered space probes and lunar missions using the heat from radioactive decay. Small nuclear fission reactors have also been used to generate electricity for Earth observation satellites like the TOPAZ reactor. In addition, radioisotope heater units use radioactive decay to provide reliable heat that can keep components functioning properly over decades in the harsh space environment. Overall, nuclear power has proven useful for providing long-lasting power for space applications where solar power is not practical. Technologies like radioisotope decay heat and small fission reactors allow probes, satellites, and missions to operate far from the Sun and for extended periods by generating electricity and heat without reliance on solar energy. Previous Refine Next Map re-rank"
    },
    {
        "title": "Refine | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/chains/document/refine",
        "text_content": "Modules More Chains Documents Refine On this page Refine The Refine documents chain constructs a response by looping over the\ninput documents and iteratively updating its answer. For each document,\nit passes all non-document inputs, the current document, and the latest\nintermediate answer to an LLM chain to get a new answer. Since the Refine chain only passes a single document to the LLM at a\ntime, it is well-suited for tasks that require analyzing more documents\nthan can fit in the model’s context. The obvious tradeoff is that this\nchain will make far more LLM calls than, for example, the Stuff\ndocuments chain. There are also certain tasks which are difficult to\naccomplish iteratively. For example, the Refine chain can perform poorly\nwhen documents frequently cross-reference one another or when a task\nrequires detailed information from many documents. Recreating with LCEL ​ With LangChain Expression Language we can\neasily recreate the RefineDocumentsChain , with the additional benefit\nof getting all the built-in LCEL features (batch, async, etc.) and with\nmuch more ability to customize specific parts of the chain. from functools import partial from operator import itemgetter from langchain . callbacks . manager import trace_as_chain_group from langchain . chat_models import ChatAnthropic from langchain . prompts import PromptTemplate from langchain . schema import StrOutputParser from langchain_core . prompts import format_document # Chain for generating initial summary based on the first document llm = ChatAnthropic ( ) first_prompt = PromptTemplate . from_template ( \"Summarize this content:\\n\\n{context}\" ) document_prompt = PromptTemplate . from_template ( \"{page_content}\" ) partial_format_doc = partial ( format_document , prompt = document_prompt ) summary_chain = { \"context\" : partial_format_doc } | first_prompt | llm | StrOutputParser ( ) # Chain for refining an existing summary based on # an additional document refine_prompt = PromptTemplate . from_template ( \"Here's your first summary: {prev_response}. \" \"Now add to it based on the following context: {context}\" ) refine_chain = ( { \"prev_response\" : itemgetter ( \"prev_response\" ) , \"context\" : lambda x : partial_format_doc ( x [ \"doc\" ] ) , } | refine_prompt | llm | StrOutputParser ( ) ) # The final refine loop, which generates an initial summary # then iteratively refines it based on each of the rest of the documents def refine_loop ( docs ) : with trace_as_chain_group ( \"refine loop\" , inputs = { \"input\" : docs } ) as manager : summary = summary_chain . invoke ( docs [ 0 ] , config = { \"callbacks\" : manager , \"run_name\" : \"initial summary\" } ) for i , doc in enumerate ( docs [ 1 : ] ) : summary = refine_chain . invoke ( { \"prev_response\" : summary , \"doc\" : doc } , config = { \"callbacks\" : manager , \"run_name\" : f\"refine { i } \" } , ) manager . on_chain_end ( { \"output\" : summary } ) return summary Example run ​ from langchain . schema import Document text = \"\"\"Nuclear power in space is the use of nuclear power in outer space, typically either small fission systems or radioactive decay for electricity or heat. Another use is for scientific observation, as in a Mössbauer spectrometer. The most common type is a radioisotope thermoelectric generator, which has been used on many space probes and on crewed lunar missions. Small fission reactors for Earth observation satellites, such as the TOPAZ nuclear reactor, have also been flown.[1] A radioisotope heater unit is powered by radioactive decay and can keep components from becoming too cold to function, potentially over a span of decades.[2] The United States tested the SNAP-10A nuclear reactor in space for 43 days in 1965,[3] with the next test of a nuclear reactor power system intended for space use occurring on 13 September 2012 with the Demonstration Using Flattop Fission (DUFF) test of the Kilopower reactor.[4] After a ground-based test of the experimental 1965 Romashka reactor, which used uranium and direct thermoelectric conversion to electricity,[5] the USSR sent about 40 nuclear-electric satellites into space, mostly powered by the BES-5 reactor. The more powerful TOPAZ-II reactor produced 10 kilowatts of electricity.[3] Examples of concepts that use nuclear power for space propulsion systems include the nuclear electric rocket (nuclear powered ion thruster(s)), the radioisotope rocket, and radioisotope electric propulsion (REP).[6] One of the more explored concepts is the nuclear thermal rocket, which was ground tested in the NERVA program. Nuclear pulse propulsion was the subject of Project Orion.[7] Regulation and hazard prevention[edit] After the ban of nuclear weapons in space by the Outer Space Treaty in 1967, nuclear power has been discussed at least since 1972 as a sensitive issue by states.[8] Particularly its potential hazards to Earth's environment and thus also humans has prompted states to adopt in the U.N. General Assembly the Principles Relevant to the Use of Nuclear Power Sources in Outer Space (1992), particularly introducing safety principles for launches and to manage their traffic.[8] Benefits Both the Viking 1 and Viking 2 landers used RTGs for power on the surface of Mars. (Viking launch vehicle pictured) While solar power is much more commonly used, nuclear power can offer advantages in some areas. Solar cells, although efficient, can only supply energy to spacecraft in orbits where the solar flux is sufficiently high, such as low Earth orbit and interplanetary destinations close enough to the Sun. Unlike solar cells, nuclear power systems function independently of sunlight, which is necessary for deep space exploration. Nuclear-based systems can have less mass than solar cells of equivalent power, allowing more compact spacecraft that are easier to orient and direct in space. In the case of crewed spaceflight, nuclear power concepts that can power both life support and propulsion systems may reduce both cost and flight time.[9] Selected applications and/or technologies for space include: Radioisotope thermoelectric generator Radioisotope heater unit Radioisotope piezoelectric generator Radioisotope rocket Nuclear thermal rocket Nuclear pulse propulsion Nuclear electric rocket \"\"\" docs = [ Document ( page_content = split , metadata = { \"source\" : \"https://en.wikipedia.org/wiki/Nuclear_power_in_space\" } , ) for split in text . split ( \"\\n\\n\" ) ] print ( refine_loop ( docs ) ) Here is the updated summary with the additional context: Here is a summary of the key points about nuclear power in space: - Nuclear power is used in space for electricity, heat, and scientific observation. The most common type is a radioisotope thermoelectric generator (RTG), which uses radioactive decay to generate electricity. RTGs have powered space probes and crewed lunar missions. - Small nuclear fission reactors have also been used to power Earth observation satellites, like the TOPAZ reactor. The United States tested the SNAP-10A nuclear reactor in space for 43 days in 1965. - After a ground-based test of the experimental 1965 Romashka reactor, which used uranium and direct thermoelectric conversion to electricity, the USSR sent about 40 nuclear-electric satellites into space, mostly powered by the BES-5 reactor. The more powerful TOPAZ-II reactor produced 10 kilowatts of electricity. - Radioisotope heater units use radioactive decay for heat. They can keep components warm enough to function over decades. - Nuclear power concepts have also been proposed and tested for space propulsion. Examples include the nuclear electric rocket (nuclear powered ion thruster(s)), the radioisotope Previous Stuff Next Map reduce"
    },
    {
        "title": "ChatAnthropic | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/chat/anthropic",
        "text_content": "Components Chat models Anthropic On this page ChatAnthropic This notebook covers how to get started with Anthropic chat models. from langchain . chat_models import ChatAnthropic from langchain . schema import HumanMessage chat = ChatAnthropic ( ) messages = [ HumanMessage ( content = \"Translate this sentence from English to French. I love programming.\" ) ] chat . invoke ( messages ) ChatAnthropic also supports async and streaming functionality: ​ from langchain . callbacks . manager import CallbackManager from langchain . callbacks . streaming_stdout import StreamingStdOutCallbackHandler await chat . ainvoke ( [ messages ] ) chat = ChatAnthropic ( streaming = True , verbose = True , callback_manager = CallbackManager ( [ StreamingStdOutCallbackHandler ( ) ] ) , ) chat . stream ( messages ) ChatAnthropicMessages LangChain also offers the beta Anthropic Messages endpoint through the\nnew langchain-anthropic package. !pip install langchain - anthropic from langchain_anthropic import ChatAnthropicMessages chat = ChatAnthropicMessages ( model_name = \"claude-instant-1.2\" ) chat . invoke ( messages ) Previous Alibaba Cloud PAI EAS Next Anthropic Functions"
    },
    {
        "title": "OpenAI metadata tagger | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_transformers/openai_metadata_tagger",
        "text_content": "Components Document transformers OpenAI metadata tagger On this page OpenAI metadata tagger It can often be useful to tag ingested documents with structured\nmetadata, such as the title, tone, or length of a document, to allow for\na more targeted similarity search later. However, for large numbers of\ndocuments, performing this labelling process manually can be tedious. The OpenAIMetadataTagger document transformer automates this process\nby extracting metadata from each provided document according to a\nprovided schema. It uses a configurable OpenAI Functions -powered chain\nunder the hood, so if you pass a custom LLM instance, it must be an OpenAI model with functions support. Note: This document transformer works best with complete documents,\nso it’s best to run it first with whole documents before doing any other\nsplitting or processing! For example, let’s say you wanted to index a set of movie reviews. You\ncould initialize the document transformer with a valid JSON Schema object as follows: from langchain . chat_models import ChatOpenAI from langchain . document_transformers . openai_functions import create_metadata_tagger from langchain . schema import Document schema = { \"properties\" : { \"movie_title\" : { \"type\" : \"string\" } , \"critic\" : { \"type\" : \"string\" } , \"tone\" : { \"type\" : \"string\" , \"enum\" : [ \"positive\" , \"negative\" ] } , \"rating\" : { \"type\" : \"integer\" , \"description\" : \"The number of stars the critic rated the movie\" , } , } , \"required\" : [ \"movie_title\" , \"critic\" , \"tone\" ] , } # Must be an OpenAI model that supports functions llm = ChatOpenAI ( temperature = 0 , model = \"gpt-3.5-turbo-0613\" ) document_transformer = create_metadata_tagger ( metadata_schema = schema , llm = llm ) You can then simply pass the document transformer a list of documents,\nand it will extract metadata from the contents: original_documents = [ Document ( page_content = \"Review of The Bee Movie\\nBy Roger Ebert\\n\\nThis is the greatest movie ever made. 4 out of 5 stars.\" ) , Document ( page_content = \"Review of The Godfather\\nBy Anonymous\\n\\nThis movie was super boring. 1 out of 5 stars.\" , metadata = { \"reliable\" : False } , ) , ] enhanced_documents = document_transformer . transform_documents ( original_documents ) import json print ( * [ d . page_content + \"\\n\\n\" + json . dumps ( d . metadata ) for d in enhanced_documents ] , sep = \"\\n\\n---------------\\n\\n\" , ) Review of The Bee Movie By Roger Ebert This is the greatest movie ever made. 4 out of 5 stars. {\"movie_title\": \"The Bee Movie\", \"critic\": \"Roger Ebert\", \"tone\": \"positive\", \"rating\": 4} --------------- Review of The Godfather By Anonymous This movie was super boring. 1 out of 5 stars. {\"movie_title\": \"The Godfather\", \"critic\": \"Anonymous\", \"tone\": \"negative\", \"rating\": 1, \"reliable\": false} The new documents can then be further processed by a text splitter\nbefore being loaded into a vector store. Extracted fields will not\noverwrite existing metadata. You can also initialize the document transformer with a Pydantic schema: from typing import Literal from pydantic import BaseModel , Field class Properties ( BaseModel ) : movie_title : str critic : str tone : Literal [ \"positive\" , \"negative\" ] rating : int = Field ( description = \"Rating out of 5 stars\" ) document_transformer = create_metadata_tagger ( Properties , llm ) enhanced_documents = document_transformer . transform_documents ( original_documents ) print ( * [ d . page_content + \"\\n\\n\" + json . dumps ( d . metadata ) for d in enhanced_documents ] , sep = \"\\n\\n---------------\\n\\n\" , ) Review of The Bee Movie By Roger Ebert This is the greatest movie ever made. 4 out of 5 stars. {\"movie_title\": \"The Bee Movie\", \"critic\": \"Roger Ebert\", \"tone\": \"positive\", \"rating\": 4} --------------- Review of The Godfather By Anonymous This movie was super boring. 1 out of 5 stars. {\"movie_title\": \"The Godfather\", \"critic\": \"Anonymous\", \"tone\": \"negative\", \"rating\": 1, \"reliable\": false} Customization ​ You can pass the underlying tagging chain the standard LLMChain\narguments in the document transformer constructor. For example, if you\nwanted to ask the LLM to focus specific details in the input documents,\nor extract metadata in a certain style, you could pass in a custom\nprompt: from langchain . prompts import ChatPromptTemplate prompt = ChatPromptTemplate . from_template ( \"\"\"Extract relevant information from the following text. Anonymous critics are actually Roger Ebert. {input} \"\"\" ) document_transformer = create_metadata_tagger ( schema , llm , prompt = prompt ) enhanced_documents = document_transformer . transform_documents ( original_documents ) print ( * [ d . page_content + \"\\n\\n\" + json . dumps ( d . metadata ) for d in enhanced_documents ] , sep = \"\\n\\n---------------\\n\\n\" , ) Review of The Bee Movie By Roger Ebert This is the greatest movie ever made. 4 out of 5 stars. {\"movie_title\": \"The Bee Movie\", \"critic\": \"Roger Ebert\", \"tone\": \"positive\", \"rating\": 4} --------------- Review of The Godfather By Anonymous This movie was super boring. 1 out of 5 stars. {\"movie_title\": \"The Godfather\", \"critic\": \"Roger Ebert\", \"tone\": \"negative\", \"rating\": 1, \"reliable\": false} Previous Nuclia Next Text embedding models"
    },
    {
        "title": "WebResearchRetriever | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/modules/data_connection/retrievers/web_research",
        "text_content": "Modules Retrieval Retrievers WebResearchRetriever On this page WebResearchRetriever Given a query, this retriever will: Formulate a set of relate Google searches Search for each Load all the resulting URLs Then embed and perform similarity search with the query on the\nconsolidate page content from langchain . retrievers . web_research import WebResearchRetriever Simple usage ​ Specify the LLM to use for Google search query generation. import os from langchain . chat_models . openai import ChatOpenAI from langchain . embeddings import OpenAIEmbeddings from langchain . utilities import GoogleSearchAPIWrapper from langchain . vectorstores import Chroma # Vectorstore vectorstore = Chroma ( embedding_function = OpenAIEmbeddings ( ) , persist_directory = \"./chroma_db_oai\" ) # LLM llm = ChatOpenAI ( temperature = 0 ) # Search os . environ [ \"GOOGLE_CSE_ID\" ] = \"xxx\" os . environ [ \"GOOGLE_API_KEY\" ] = \"xxx\" search = GoogleSearchAPIWrapper ( ) # Initialize web_research_retriever = WebResearchRetriever . from_llm ( vectorstore = vectorstore , llm = llm , search = search , ) Run with citations ​ We can use RetrievalQAWithSourcesChain to retrieve docs and provide\ncitations. from langchain . chains import RetrievalQAWithSourcesChain user_input = \"How do LLM Powered Autonomous Agents work?\" qa_chain = RetrievalQAWithSourcesChain . from_chain_type ( llm , retriever = web_research_retriever ) result = qa_chain ( { \"question\" : user_input } ) result Fetching pages: 100%|###################################################################################################################################| 1/1 [00:00<00:00,  3.33it/s] {'question': 'How do LLM Powered Autonomous Agents work?', 'answer': \"LLM Powered Autonomous Agents work by using LLM (large language model) as the core controller of the agent's brain. It is complemented by several key components, including planning, memory, and tool use. The agent system is designed to be a powerful general problem solver. \\n\", 'sources': 'https://lilianweng.github.io/posts/2023-06-23-agent/'} Run with logging ​ Here, we use get_relevant_documents method to return docs. # Run import logging logging . basicConfig ( ) logging . getLogger ( \"langchain.retrievers.web_research\" ) . setLevel ( logging . INFO ) user_input = \"What is Task Decomposition in LLM Powered Autonomous Agents?\" docs = web_research_retriever . get_relevant_documents ( user_input ) INFO:langchain.retrievers.web_research:Generating questions for Google Search ... INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'What is Task Decomposition in LLM Powered Autonomous Agents?', 'text': LineList(lines=['1. How do LLM powered autonomous agents utilize task decomposition?\\n', '2. Can you explain the concept of task decomposition in LLM powered autonomous agents?\\n', '3. What role does task decomposition play in the functioning of LLM powered autonomous agents?\\n', '4. Why is task decomposition important for LLM powered autonomous agents?\\n'])} INFO:langchain.retrievers.web_research:Questions for Google Search: ['1. How do LLM powered autonomous agents utilize task decomposition?\\n', '2. Can you explain the concept of task decomposition in LLM powered autonomous agents?\\n', '3. What role does task decomposition play in the functioning of LLM powered autonomous agents?\\n', '4. Why is task decomposition important for LLM powered autonomous agents?\\n'] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\" , \"What are the subgoals for achieving XYZ?'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\" , \"What are the subgoals for achieving XYZ?\" , (2)\\xa0...'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... In a LLM-powered autonomous agent system, LLM functions as the ... Task decomposition can be done (1) by LLM with simple prompting like\\xa0...'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Agent System Overview In a LLM-powered autonomous agent system, ... Task decomposition can be done (1) by LLM with simple prompting like\\xa0...'}] INFO:langchain.retrievers.web_research:New URLs to load: [] Generate answer using retrieved docs ​ We can use load_qa_chain for QA using the retrieved docs. from langchain . chains . question_answering import load_qa_chain chain = load_qa_chain ( llm , chain_type = \"stuff\" ) output = chain ( { \"input_documents\" : docs , \"question\" : user_input } , return_only_outputs = True ) output [ \"output_text\" ] 'Task decomposition in LLM-powered autonomous agents refers to the process of breaking down a complex task into smaller, more manageable subgoals. This allows the agent to efficiently handle and execute the individual steps required to complete the overall task. By decomposing the task, the agent can prioritize and organize its actions, making it easier to plan and execute the necessary steps towards achieving the desired outcome.' More flexibility ​ Pass an LLM chain with custom prompt and output parsing. import os import re from typing import List from langchain . chains import LLMChain from langchain . output_parsers . pydantic import PydanticOutputParser from langchain . prompts import PromptTemplate from pydantic import BaseModel , Field # LLMChain search_prompt = PromptTemplate ( input_variables = [ \"question\" ] , template = \"\"\"You are an assistant tasked with improving Google search results. Generate FIVE Google search queries that are similar to this question. The output should be a numbered list of questions and each should have a question mark at the end: {question}\"\"\" , ) class LineList ( BaseModel ) : \"\"\"List of questions.\"\"\" lines : List [ str ] = Field ( description = \"Questions\" ) class QuestionListOutputParser ( PydanticOutputParser ) : \"\"\"Output parser for a list of numbered questions.\"\"\" def __init__ ( self ) - > None : super ( ) . __init__ ( pydantic_object = LineList ) def parse ( self , text : str ) - > LineList : lines = re . findall ( r\"\\d+\\..*?\\n\" , text ) return LineList ( lines = lines ) llm_chain = LLMChain ( llm = llm , prompt = search_prompt , output_parser = QuestionListOutputParser ( ) , ) # Initialize web_research_retriever_llm_chain = WebResearchRetriever ( vectorstore = vectorstore , llm_chain = llm_chain , search = search , ) # Run docs = web_research_retriever_llm_chain . get_relevant_documents ( user_input ) INFO:langchain.retrievers.web_research:Generating questions for Google Search ... INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'What is Task Decomposition in LLM Powered Autonomous Agents?', 'text': LineList(lines=['1. How do LLM powered autonomous agents use task decomposition?\\n', '2. Why is task decomposition important for LLM powered autonomous agents?\\n', '3. Can you explain the concept of task decomposition in LLM powered autonomous agents?\\n', '4. What are the benefits of task decomposition in LLM powered autonomous agents?\\n'])} INFO:langchain.retrievers.web_research:Questions for Google Search: ['1. How do LLM powered autonomous agents use task decomposition?\\n', '2. Why is task decomposition important for LLM powered autonomous agents?\\n', '3. Can you explain the concept of task decomposition in LLM powered autonomous agents?\\n', '4. What are the benefits of task decomposition in LLM powered autonomous agents?\\n'] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\" , \"What are the subgoals for achieving XYZ?'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\" , \"What are the subgoals for achieving XYZ?\" , (2)\\xa0...'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\" , \"What are the subgoals for achieving XYZ?'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\" , \"What are the subgoals for achieving XYZ?'}] INFO:langchain.retrievers.web_research:New URLs to load: ['https://lilianweng.github.io/posts/2023-06-23-agent/'] INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls ... Fetching pages: 100%|###################################################################################################################################| 1/1 [00:00<00:00,  6.32it/s] len ( docs ) 1 Run locally ​ Specify LLM and embeddings that will run locally (e.g., on your laptop). from langchain . callbacks . manager import CallbackManager from langchain . callbacks . streaming_stdout import StreamingStdOutCallbackHandler from langchain . embeddings import GPT4AllEmbeddings from langchain . llms import LlamaCpp n_gpu_layers = 1 # Metal set to 1 is enough. n_batch = 512 # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip. callback_manager = CallbackManager ( [ StreamingStdOutCallbackHandler ( ) ] ) llama = LlamaCpp ( model_path = \"/Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\" , n_gpu_layers = n_gpu_layers , n_batch = n_batch , n_ctx = 4096 , # Context window max_tokens = 1000 , # Max tokens to generate f16_kv = True , # MUST set to True, otherwise you will run into problem after a couple of calls callback_manager = callback_manager , verbose = True , ) vectorstore_llama = Chroma ( embedding_function = GPT4AllEmbeddings ( ) , persist_directory = \"./chroma_db_llama\" ) llama.cpp: loading model from /Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin llama_model_load_internal: format     = ggjt v3 (latest) llama_model_load_internal: n_vocab    = 32000 llama_model_load_internal: n_ctx      = 4096 llama_model_load_internal: n_embd     = 5120 llama_model_load_internal: n_mult     = 256 llama_model_load_internal: n_head     = 40 llama_model_load_internal: n_layer    = 40 llama_model_load_internal: n_rot      = 128 llama_model_load_internal: freq_base  = 10000.0 llama_model_load_internal: freq_scale = 1 llama_model_load_internal: ftype      = 2 (mostly Q4_0) llama_model_load_internal: n_ff       = 13824 llama_model_load_internal: model size = 13B llama_model_load_internal: ggml ctx size =    0.09 MB llama_model_load_internal: mem required  = 9132.71 MB (+ 1608.00 MB per state) llama_new_context_with_model: kv self size  = 3200.00 MB ggml_metal_init: allocating ggml_metal_init: using MPS ggml_metal_init: loading '/Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal' ggml_metal_init: loaded kernel_add                            0x110fbd600 ggml_metal_init: loaded kernel_mul                            0x110fbeb30 ggml_metal_init: loaded kernel_mul_row                        0x110fbf350 ggml_metal_init: loaded kernel_scale                          0x110fbf9e0 ggml_metal_init: loaded kernel_silu                           0x110fc0150 ggml_metal_init: loaded kernel_relu                           0x110fbd950 ggml_metal_init: loaded kernel_gelu                           0x110fbdbb0 ggml_metal_init: loaded kernel_soft_max                       0x110fc14d0 ggml_metal_init: loaded kernel_diag_mask_inf                  0x110fc1980 ggml_metal_init: loaded kernel_get_rows_f16                   0x110fc22a0 ggml_metal_init: loaded kernel_get_rows_q4_0                  0x110fc2ad0 ggml_metal_init: loaded kernel_get_rows_q4_1                  0x110fc3260 ggml_metal_init: loaded kernel_get_rows_q2_K                  0x110fc3ad0 ggml_metal_init: loaded kernel_get_rows_q3_K                  0x110fc41c0 ggml_metal_init: loaded kernel_get_rows_q4_K                  0x110fc48c0 ggml_metal_init: loaded kernel_get_rows_q5_K                  0x110fc4fa0 ggml_metal_init: loaded kernel_get_rows_q6_K                  0x110fc56a0 ggml_metal_init: loaded kernel_rms_norm                       0x110fc5da0 ggml_metal_init: loaded kernel_norm                           0x110fc64d0 ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x2a5c19990 ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x2a5c1d4a0 ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x2a5c19fc0 ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x2a5c1dcc0 ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x2a5c1e420 ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x2a5c1edc0 ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x2a5c1fd90 ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x2a5c20540 ggml_metal_init: loaded kernel_rope                           0x2a5c20d40 ggml_metal_init: loaded kernel_alibi_f32                      0x2a5c21730 ggml_metal_init: loaded kernel_cpy_f32_f16                    0x2a5c21ab0 ggml_metal_init: loaded kernel_cpy_f32_f32                    0x2a5c22080 ggml_metal_init: loaded kernel_cpy_f16_f16                    0x2a5c231d0 ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB ggml_metal_init: hasUnifiedMemory             = true ggml_metal_init: maxTransferRate              = built-in GPU ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, ( 6984.52 / 21845.34) ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1040.00 MB, ( 8024.52 / 21845.34) ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  3202.00 MB, (11226.52 / 21845.34) ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   597.00 MB, (11823.52 / 21845.34) AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB, (12335.52 / 21845.34) objc[33471]: Class GGMLMetalClass is implemented in both /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/libllama.dylib (0x2c7368208) and /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x5ebf48208). One of the two will be used. Which one is undefined. objc[33471]: Class GGMLMetalClass is implemented in both /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/libllama.dylib (0x2c7368208) and /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x5ec374208). One of the two will be used. Which one is undefined. Found model file at  /Users/rlm/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin llama_new_context_with_model: max tensor size =    87.89 MB We supplied StreamingStdOutCallbackHandler() , so model outputs (e.g.,\ngenerated questions) are streamed. We also have logging on, so we seem them there too. from langchain . chains import RetrievalQAWithSourcesChain # Initialize web_research_retriever = WebResearchRetriever . from_llm ( vectorstore = vectorstore_llama , llm = llama , search = search , ) # Run user_input = \"What is Task Decomposition in LLM Powered Autonomous Agents?\" qa_chain = RetrievalQAWithSourcesChain . from_chain_type ( llama , retriever = web_research_retriever ) result = qa_chain ( { \"question\" : user_input } ) result INFO:langchain.retrievers.web_research:Generating questions for Google Search ... llama_print_timings:        load time =  8585.01 ms llama_print_timings:      sample time =   124.24 ms /   164 runs   (    0.76 ms per token,  1320.04 tokens per second) llama_print_timings: prompt eval time =  8584.83 ms /   101 tokens (   85.00 ms per token,    11.76 tokens per second) llama_print_timings:        eval time =  7268.55 ms /   163 runs   (   44.59 ms per token,    22.43 tokens per second) llama_print_timings:       total time = 16236.13 ms INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'What is Task Decomposition in LLM Powered Autonomous Agents?', 'text': LineList(lines=['1. How does Task Decomposition work in LLM Powered Autonomous Agents? \\n', '2. What are the benefits of using Task Decomposition in LLM Powered Autonomous Agents? \\n', '3. Can you provide examples of Task Decomposition in LLM Powered Autonomous Agents? \\n', '4. How does Task Decomposition improve the performance of LLM Powered Autonomous Agents? \\n'])} INFO:langchain.retrievers.web_research:Questions for Google Search: ['1. How does Task Decomposition work in LLM Powered Autonomous Agents? \\n', '2. What are the benefits of using Task Decomposition in LLM Powered Autonomous Agents? \\n', '3. Can you provide examples of Task Decomposition in LLM Powered Autonomous Agents? \\n', '4. How does Task Decomposition improve the performance of LLM Powered Autonomous Agents? \\n'] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\" , \"What are the subgoals for achieving XYZ?'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\" , \"What are the subgoals for achieving XYZ?\" , (2)\\xa0...'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... A complicated task usually involves many steps. An agent needs to know what they are and plan ahead. Task Decomposition#. Chain of thought (CoT;\\xa0...'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Agent System Overview In a LLM-powered autonomous agent system, ... Task decomposition can be done (1) by LLM with simple prompting like\\xa0...'}] INFO:langchain.retrievers.web_research:New URLs to load: ['https://lilianweng.github.io/posts/2023-06-23-agent/'] INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls ... Fetching pages: 100%|###################################################################################################################################| 1/1 [00:00<00:00, 10.49it/s] Llama.generate: prefix-match hit llama_print_timings:        load time =  8585.01 ms llama_print_timings:      sample time =    52.88 ms /    72 runs   (    0.73 ms per token,  1361.55 tokens per second) llama_print_timings: prompt eval time = 125925.13 ms /  2358 tokens (   53.40 ms per token,    18.73 tokens per second) llama_print_timings:        eval time =  3504.16 ms /    71 runs   (   49.35 ms per token,    20.26 tokens per second) llama_print_timings:       total time = 129584.60 ms Sure, here are five Google search queries that are similar to \"What is Task Decomposition in LLM Powered Autonomous Agents?\": 1. How does Task Decomposition work in LLM Powered Autonomous Agents? 2. What are the benefits of using Task Decomposition in LLM Powered Autonomous Agents? 3. Can you provide examples of Task Decomposition in LLM Powered Autonomous Agents? 4. How does Task Decomposition improve the performance of LLM Powered Autonomous Agents? 5. What are some common challenges or limitations of using Task Decomposition in LLM Powered Autonomous Agents, and how can they be addressed? The content discusses Task Decomposition in LLM Powered Autonomous Agents, which involves breaking down large tasks into smaller, manageable subgoals for efficient handling of complex tasks. SOURCES: https://lilianweng.github.io/posts/2023-06-23-agent/ {'question': 'What is Task Decomposition in LLM Powered Autonomous Agents?', 'answer': ' The content discusses Task Decomposition in LLM Powered Autonomous Agents, which involves breaking down large tasks into smaller, manageable subgoals for efficient handling of complex tasks.\\n', 'sources': 'https://lilianweng.github.io/posts/2023-06-23-agent/'} Previous Vector store-backed retriever Next Indexing"
    },
    {
        "title": "Diffbot Graph Transformer | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/graph/diffbot_graphtransformer",
        "text_content": "Graph querying Diffbot Graph Transformer On this page Diffbot Graph Transformer Open In Colab Use case ​ Text data often contain rich relationships and insights that can be\nuseful for various analytics, recommendation engines, or knowledge\nmanagement applications. Diffbot’s NLP API allows for the extraction of entities, relationships,\nand semantic meaning from unstructured text data. By coupling Diffbot’s NLP API with Neo4j, a graph database, you can\ncreate powerful, dynamic graph structures based on the information\nextracted from text. These graph structures are fully queryable and can\nbe integrated into various applications. This combination allows for use cases such as: Building knowledge graphs from textual documents, websites, or\nsocial media feeds. Generating recommendations based on semantic relationships in the\ndata. Creating advanced search features that understand the relationships\nbetween entities. Building analytics dashboards that allow users to explore the hidden\nrelationships in data. Overview ​ LangChain provides tools to interact with Graph Databases: Construct knowledge graphs from text using graph transformer and\nstore integrations Query a graph database using chains for query creation and\nexecution Interact with a graph database using agents for robust and\nflexible querying Quickstart ​ First, get required packages and set environment variables: !pip install langchain langchain - experimental openai neo4j wikipedia Diffbot NLP Service ​ Diffbot’s NLP service is a tool for extracting entities, relationships,\nand semantic context from unstructured text data. This extracted\ninformation can be used to construct a knowledge graph. To use their\nservice, you’ll need to obtain an API key from Diffbot . from langchain_experimental . graph_transformers . diffbot import DiffbotGraphTransformer diffbot_api_key = \"DIFFBOT_API_KEY\" diffbot_nlp = DiffbotGraphTransformer ( diffbot_api_key = diffbot_api_key ) This code fetches Wikipedia articles about “Warren Buffett” and then\nuses DiffbotGraphTransformer to extract entities and relationships.\nThe DiffbotGraphTransformer outputs a structured data GraphDocument ,\nwhich can be used to populate a graph database. Note that text chunking\nis avoided due to Diffbot’s character limit per API\nrequest . from langchain . document_loaders import WikipediaLoader query = \"Warren Buffett\" raw_documents = WikipediaLoader ( query = query ) . load ( ) graph_documents = diffbot_nlp . convert_to_graph_documents ( raw_documents ) Loading the data into a knowledge graph ​ You will need to have a running Neo4j instance. One option is to create\na free Neo4j database instance in their Aura cloud\nservice . You can\nalso run the database locally using the Neo4j Desktop\napplication , or running a docker\ncontainer. You can run a local docker container by running the executing\nthe following script: docker run \\ --name neo4j \\ -p 7474:7474 -p 7687:7687 \\ -d \\ -e NEO4J_AUTH=neo4j/pleaseletmein \\ -e NEO4J_PLUGINS=\\[\\\"apoc\\\"\\]  \\ neo4j:latest If you are using the docker container, you need to wait a couple of\nsecond for the database to start. from langchain . graphs import Neo4jGraph url = \"bolt://localhost:7687\" username = \"neo4j\" password = \"pleaseletmein\" graph = Neo4jGraph ( url = url , username = username , password = password ) The GraphDocuments can be loaded into a knowledge graph using the add_graph_documents method. graph . add_graph_documents ( graph_documents ) Refresh graph schema information ​ If the schema of database changes, you can refresh the schema\ninformation needed to generate Cypher statements graph . refresh_schema ( ) Querying the graph ​ We can now use the graph cypher QA chain to ask question of the graph.\nIt is advisable to use gpt-4 to construct Cypher queries to get the\nbest experience. from langchain . chains import GraphCypherQAChain from langchain . chat_models import ChatOpenAI chain = GraphCypherQAChain . from_llm ( cypher_llm = ChatOpenAI ( temperature = 0 , model_name = \"gpt-4\" ) , qa_llm = ChatOpenAI ( temperature = 0 , model_name = \"gpt-3.5-turbo\" ) , graph = graph , verbose = True , ) chain . run ( \"Which university did Warren Buffett attend?\" ) > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (p:Person {name: \"Warren Buffett\"})-[:EDUCATED_AT]->(o:Organization) RETURN o.name Full Context: [{'o.name': 'New York Institute of Finance'}, {'o.name': 'Alice Deal Junior High School'}, {'o.name': 'Woodrow Wilson High School'}, {'o.name': 'University of Nebraska'}] > Finished chain. 'Warren Buffett attended the University of Nebraska.' chain . run ( \"Who is or was working at Berkshire Hathaway?\" ) > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (p:Person)-[r:EMPLOYEE_OR_MEMBER_OF]->(o:Organization) WHERE o.name = 'Berkshire Hathaway' RETURN p.name Full Context: [{'p.name': 'Charlie Munger'}, {'p.name': 'Oliver Chace'}, {'p.name': 'Howard Buffett'}, {'p.name': 'Howard'}, {'p.name': 'Susan Buffett'}, {'p.name': 'Warren Buffett'}] > Finished chain. 'Charlie Munger, Oliver Chace, Howard Buffett, Susan Buffett, and Warren Buffett are or were working at Berkshire Hathaway.' Previous Graph querying Next ArangoDB QA chain"
    },
    {
        "title": "ArangoDB QA chain | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/graph/graph_arangodb_qa",
        "text_content": "Graph querying ArangoDB QA chain On this page ArangoDB QA chain Open In Colab This notebook shows how to use LLMs to provide a natural language\ninterface to an ArangoDB database. You can get a local ArangoDB instance running via the ArangoDB Docker\nimage : docker run -p 8529:8529 -e ARANGO_ROOT_PASSWORD= arangodb/arangodb An alternative is to use the ArangoDB Cloud Connector\npackage to get\na temporary cloud instance running: % % capture !pip install python - arango # The ArangoDB Python Driver !pip install adb - cloud - connector # The ArangoDB Cloud Instance provisioner !pip install openai !pip install langchain # Instantiate ArangoDB Database import json from adb_cloud_connector import get_temp_credentials from arango import ArangoClient con = get_temp_credentials ( ) db = ArangoClient ( hosts = con [ \"url\" ] ) . db ( con [ \"dbName\" ] , con [ \"username\" ] , con [ \"password\" ] , verify = True ) print ( json . dumps ( con , indent = 2 ) ) Log: requesting new credentials... Succcess: new credentials acquired { \"dbName\": \"TUT3sp29s3pjf1io0h4cfdsq\", \"username\": \"TUTo6nkwgzkizej3kysgdyeo8\", \"password\": \"TUT9vx0qjqt42i9bq8uik4v9\", \"hostname\": \"tutorials.arangodb.cloud\", \"port\": 8529, \"url\": \"https://tutorials.arangodb.cloud:8529\" } # Instantiate the ArangoDB-LangChain Graph from langchain . graphs import ArangoGraph graph = ArangoGraph ( db ) Populating the Database ​ We will rely on the Python Driver to import our GameOfThrones data into our database. if db . has_graph ( \"GameOfThrones\" ) : db . delete_graph ( \"GameOfThrones\" , drop_collections = True ) db . create_graph ( \"GameOfThrones\" , edge_definitions = [ { \"edge_collection\" : \"ChildOf\" , \"from_vertex_collections\" : [ \"Characters\" ] , \"to_vertex_collections\" : [ \"Characters\" ] , } , ] , ) documents = [ { \"_key\" : \"NedStark\" , \"name\" : \"Ned\" , \"surname\" : \"Stark\" , \"alive\" : True , \"age\" : 41 , \"gender\" : \"male\" , } , { \"_key\" : \"CatelynStark\" , \"name\" : \"Catelyn\" , \"surname\" : \"Stark\" , \"alive\" : False , \"age\" : 40 , \"gender\" : \"female\" , } , { \"_key\" : \"AryaStark\" , \"name\" : \"Arya\" , \"surname\" : \"Stark\" , \"alive\" : True , \"age\" : 11 , \"gender\" : \"female\" , } , { \"_key\" : \"BranStark\" , \"name\" : \"Bran\" , \"surname\" : \"Stark\" , \"alive\" : True , \"age\" : 10 , \"gender\" : \"male\" , } , ] edges = [ { \"_to\" : \"Characters/NedStark\" , \"_from\" : \"Characters/AryaStark\" } , { \"_to\" : \"Characters/NedStark\" , \"_from\" : \"Characters/BranStark\" } , { \"_to\" : \"Characters/CatelynStark\" , \"_from\" : \"Characters/AryaStark\" } , { \"_to\" : \"Characters/CatelynStark\" , \"_from\" : \"Characters/BranStark\" } , ] db . collection ( \"Characters\" ) . import_bulk ( documents ) db . collection ( \"ChildOf\" ) . import_bulk ( edges ) {'error': False, 'created': 4, 'errors': 0, 'empty': 0, 'updated': 0, 'ignored': 0, 'details': []} Getting & Setting the ArangoDB Schema ​ An initial ArangoDB Schema is generated upon instantiating the ArangoDBGraph object. Below are the schema’s getter & setter methods\nshould you be interested in viewing or modifying the schema: # The schema should be empty here, # since `graph` was initialized prior to ArangoDB Data ingestion (see above). import json print ( json . dumps ( graph . schema , indent = 4 ) ) { \"Graph Schema\": [], \"Collection Schema\": [] } graph . set_schema ( ) # We can now view the generated schema import json print ( json . dumps ( graph . schema , indent = 4 ) ) { \"Graph Schema\": [ { \"graph_name\": \"GameOfThrones\", \"edge_definitions\": [ { \"edge_collection\": \"ChildOf\", \"from_vertex_collections\": [ \"Characters\" ], \"to_vertex_collections\": [ \"Characters\" ] } ] } ], \"Collection Schema\": [ { \"collection_name\": \"ChildOf\", \"collection_type\": \"edge\", \"edge_properties\": [ { \"name\": \"_key\", \"type\": \"str\" }, { \"name\": \"_id\", \"type\": \"str\" }, { \"name\": \"_from\", \"type\": \"str\" }, { \"name\": \"_to\", \"type\": \"str\" }, { \"name\": \"_rev\", \"type\": \"str\" } ], \"example_edge\": { \"_key\": \"266218884025\", \"_id\": \"ChildOf/266218884025\", \"_from\": \"Characters/AryaStark\", \"_to\": \"Characters/NedStark\", \"_rev\": \"_gVPKGSq---\" } }, { \"collection_name\": \"Characters\", \"collection_type\": \"document\", \"document_properties\": [ { \"name\": \"_key\", \"type\": \"str\" }, { \"name\": \"_id\", \"type\": \"str\" }, { \"name\": \"_rev\", \"type\": \"str\" }, { \"name\": \"name\", \"type\": \"str\" }, { \"name\": \"surname\", \"type\": \"str\" }, { \"name\": \"alive\", \"type\": \"bool\" }, { \"name\": \"age\", \"type\": \"int\" }, { \"name\": \"gender\", \"type\": \"str\" } ], \"example_document\": { \"_key\": \"NedStark\", \"_id\": \"Characters/NedStark\", \"_rev\": \"_gVPKGPi---\", \"name\": \"Ned\", \"surname\": \"Stark\", \"alive\": true, \"age\": 41, \"gender\": \"male\" } } ] } Querying the ArangoDB Database ​ We can now use the ArangoDB Graph QA Chain to inquire about our data import os os . environ [ \"OPENAI_API_KEY\" ] = \"your-key-here\" from langchain . chains import ArangoGraphQAChain from langchain . chat_models import ChatOpenAI chain = ArangoGraphQAChain . from_llm ( ChatOpenAI ( temperature = 0 ) , graph = graph , verbose = True ) chain . run ( \"Is Ned Stark alive?\" ) > Entering new ArangoGraphQAChain chain... AQL Query (1): WITH Characters FOR character IN Characters FILTER character.name == \"Ned\" AND character.surname == \"Stark\" RETURN character.alive AQL Result: [True] > Finished chain. 'Yes, Ned Stark is alive.' chain . run ( \"How old is Arya Stark?\" ) > Entering new ArangoGraphQAChain chain... AQL Query (1): WITH Characters FOR character IN Characters FILTER character.name == \"Arya\" && character.surname == \"Stark\" RETURN character.age AQL Result: [11] > Finished chain. 'Arya Stark is 11 years old.' chain . run ( \"Are Arya Stark and Ned Stark related?\" ) > Entering new ArangoGraphQAChain chain... AQL Query (1): WITH Characters, ChildOf FOR v, e, p IN 1..1 OUTBOUND 'Characters/AryaStark' ChildOf FILTER p.vertices[-1]._key == 'NedStark' RETURN p AQL Result: [{'vertices': [{'_key': 'AryaStark', '_id': 'Characters/AryaStark', '_rev': '_gVPKGPi--B', 'name': 'Arya', 'surname': 'Stark', 'alive': True, 'age': 11, 'gender': 'female'}, {'_key': 'NedStark', '_id': 'Characters/NedStark', '_rev': '_gVPKGPi---', 'name': 'Ned', 'surname': 'Stark', 'alive': True, 'age': 41, 'gender': 'male'}], 'edges': [{'_key': '266218884025', '_id': 'ChildOf/266218884025', '_from': 'Characters/AryaStark', '_to': 'Characters/NedStark', '_rev': '_gVPKGSq---'}], 'weights': [0, 1]}] > Finished chain. 'Yes, Arya Stark and Ned Stark are related. According to the information retrieved from the database, there is a relationship between them. Arya Stark is the child of Ned Stark.' chain . run ( \"Does Arya Stark have a dead parent?\" ) > Entering new ArangoGraphQAChain chain... AQL Query (1): WITH Characters, ChildOf FOR v, e IN 1..1 OUTBOUND 'Characters/AryaStark' ChildOf FILTER v.alive == false RETURN e AQL Result: [{'_key': '266218884027', '_id': 'ChildOf/266218884027', '_from': 'Characters/AryaStark', '_to': 'Characters/CatelynStark', '_rev': '_gVPKGSu---'}] > Finished chain. 'Yes, Arya Stark has a dead parent. The parent is Catelyn Stark.' Chain Modifiers ​ You can alter the values of the following ArangoDBGraphQAChain class\nvariables to modify the behaviour of your chain results # Specify the maximum number of AQL Query Results to return chain . top_k = 10 # Specify whether or not to return the AQL Query in the output dictionary chain . return_aql_query = True # Specify whether or not to return the AQL JSON Result in the output dictionary chain . return_aql_result = True # Specify the maximum amount of AQL Generation attempts that should be made chain . max_aql_generation_attempts = 5 # Specify a set of AQL Query Examples, which are passed to # the AQL Generation Prompt Template to promote few-shot-learning. # Defaults to an empty string. chain . aql_examples = \"\"\" # Is Ned Stark alive? RETURN DOCUMENT('Characters/NedStark').alive # Is Arya Stark the child of Ned Stark? FOR e IN ChildOf FILTER e._from == \"Characters/AryaStark\" AND e._to == \"Characters/NedStark\" RETURN e \"\"\" chain . run ( \"Is Ned Stark alive?\" ) # chain(\"Is Ned Stark alive?\") # Returns a dictionary with the AQL Query & AQL Result > Entering new ArangoGraphQAChain chain... AQL Query (1): RETURN DOCUMENT('Characters/NedStark').alive AQL Result: [True] > Finished chain. 'Yes, according to the information in the database, Ned Stark is alive.' chain . run ( \"Is Bran Stark the child of Ned Stark?\" ) > Entering new ArangoGraphQAChain chain... AQL Query (1): FOR e IN ChildOf FILTER e._from == \"Characters/BranStark\" AND e._to == \"Characters/NedStark\" RETURN e AQL Result: [{'_key': '266218884026', '_id': 'ChildOf/266218884026', '_from': 'Characters/BranStark', '_to': 'Characters/NedStark', '_rev': '_gVPKGSq--_'}] > Finished chain. 'Yes, according to the information in the ArangoDB database, Bran Stark is indeed the child of Ned Stark.' Previous Diffbot Graph Transformer Next Neo4j DB QA chain"
    },
    {
        "title": "Neo4j DB QA chain | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/graph/graph_cypher_qa",
        "text_content": "Graph querying Neo4j DB QA chain On this page Neo4j DB QA chain This notebook shows how to use LLMs to provide a natural language\ninterface to a graph database you can query with the Cypher query\nlanguage. You will need to have a running Neo4j instance. One option is to create\na free Neo4j database instance in their Aura cloud\nservice . You can\nalso run the database locally using the Neo4j Desktop\napplication , or running a docker\ncontainer. You can run a local docker container by running the executing\nthe following script: docker run \\ --name neo4j \\ -p 7474:7474 -p 7687:7687 \\ -d \\ -e NEO4J_AUTH=neo4j/pleaseletmein \\ -e NEO4J_PLUGINS=\\[\\\"apoc\\\"\\]  \\ neo4j:latest If you are using the docker container, you need to wait a couple of\nsecond for the database to start. from langchain . chains import GraphCypherQAChain from langchain . chat_models import ChatOpenAI from langchain . graphs import Neo4jGraph graph = Neo4jGraph ( url = \"bolt://localhost:7687\" , username = \"neo4j\" , password = \"pleaseletmein\" ) Seeding the database ​ Assuming your database is empty, you can populate it using Cypher query\nlanguage. The following Cypher statement is idempotent, which means the\ndatabase information will be the same if you run it one or multiple\ntimes. graph . query ( \"\"\" MERGE (m:Movie {name:\"Top Gun\"}) WITH m UNWIND [\"Tom Cruise\", \"Val Kilmer\", \"Anthony Edwards\", \"Meg Ryan\"] AS actor MERGE (a:Actor {name:actor}) MERGE (a)-[:ACTED_IN]->(m) \"\"\" ) [] Refresh graph schema information ​ If the schema of database changes, you can refresh the schema\ninformation needed to generate Cypher statements. graph . refresh_schema ( ) print ( graph . schema ) Node properties are the following: Movie {name: STRING},Actor {name: STRING} Relationship properties are the following: The relationships are the following: (:Actor)-[:ACTED_IN]->(:Movie) Querying the graph ​ We can now use the graph cypher QA chain to ask question of the graph chain = GraphCypherQAChain . from_llm ( ChatOpenAI ( temperature = 0 ) , graph = graph , verbose = True ) chain . run ( \"Who played in Top Gun?\" ) > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (a:Actor)-[:ACTED_IN]->(m:Movie {name: 'Top Gun'}) RETURN a.name Full Context: [{'a.name': 'Tom Cruise'}, {'a.name': 'Val Kilmer'}, {'a.name': 'Anthony Edwards'}, {'a.name': 'Meg Ryan'}] > Finished chain. 'Tom Cruise, Val Kilmer, Anthony Edwards, and Meg Ryan played in Top Gun.' Limit the number of results ​ You can limit the number of results from the Cypher QA Chain using the top_k parameter. The default is 10. chain = GraphCypherQAChain . from_llm ( ChatOpenAI ( temperature = 0 ) , graph = graph , verbose = True , top_k = 2 ) chain . run ( \"Who played in Top Gun?\" ) > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (a:Actor)-[:ACTED_IN]->(m:Movie {name: 'Top Gun'}) RETURN a.name Full Context: [{'a.name': 'Tom Cruise'}, {'a.name': 'Val Kilmer'}] > Finished chain. 'Tom Cruise and Val Kilmer played in Top Gun.' Return intermediate results ​ You can return intermediate steps from the Cypher QA Chain using the return_intermediate_steps parameter chain = GraphCypherQAChain . from_llm ( ChatOpenAI ( temperature = 0 ) , graph = graph , verbose = True , return_intermediate_steps = True ) result = chain ( \"Who played in Top Gun?\" ) print ( f\"Intermediate steps: { result [ 'intermediate_steps' ] } \" ) print ( f\"Final answer: { result [ 'result' ] } \" ) > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (a:Actor)-[:ACTED_IN]->(m:Movie {name: 'Top Gun'}) RETURN a.name Full Context: [{'a.name': 'Tom Cruise'}, {'a.name': 'Val Kilmer'}, {'a.name': 'Anthony Edwards'}, {'a.name': 'Meg Ryan'}] > Finished chain. Intermediate steps: [{'query': \"MATCH (a:Actor)-[:ACTED_IN]->(m:Movie {name: 'Top Gun'})\\nRETURN a.name\"}, {'context': [{'a.name': 'Tom Cruise'}, {'a.name': 'Val Kilmer'}, {'a.name': 'Anthony Edwards'}, {'a.name': 'Meg Ryan'}]}] Final answer: Tom Cruise, Val Kilmer, Anthony Edwards, and Meg Ryan played in Top Gun. Return direct results ​ You can return direct results from the Cypher QA Chain using the return_direct parameter chain = GraphCypherQAChain . from_llm ( ChatOpenAI ( temperature = 0 ) , graph = graph , verbose = True , return_direct = True ) chain . run ( \"Who played in Top Gun?\" ) > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (a:Actor)-[:ACTED_IN]->(m:Movie {name: 'Top Gun'}) RETURN a.name > Finished chain. [{'a.name': 'Tom Cruise'}, {'a.name': 'Val Kilmer'}, {'a.name': 'Anthony Edwards'}, {'a.name': 'Meg Ryan'}] Add examples in the Cypher generation prompt ​ You can define the Cypher statement you want the LLM to generate for\nparticular questions from langchain . prompts . prompt import PromptTemplate CYPHER_GENERATION_TEMPLATE = \"\"\"Task:Generate Cypher statement to query a graph database. Instructions: Use only the provided relationship types and properties in the schema. Do not use any other relationship types or properties that are not provided. Schema: {schema} Note: Do not include any explanations or apologies in your responses. Do not respond to any questions that might ask anything else than for you to construct a Cypher statement. Do not include any text except the generated Cypher statement. Examples: Here are a few examples of generated Cypher statements for particular questions: # How many people played in Top Gun? MATCH (m:Movie {{title:\"Top Gun\"}})<-[:ACTED_IN]-() RETURN count(*) AS numberOfActors The question is: {question}\"\"\" CYPHER_GENERATION_PROMPT = PromptTemplate ( input_variables = [ \"schema\" , \"question\" ] , template = CYPHER_GENERATION_TEMPLATE ) chain = GraphCypherQAChain . from_llm ( ChatOpenAI ( temperature = 0 ) , graph = graph , verbose = True , cypher_prompt = CYPHER_GENERATION_PROMPT , ) chain . run ( \"How many people played in Top Gun?\" ) > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (m:Movie {name:\"Top Gun\"})<-[:ACTED_IN]-(:Actor) RETURN count(*) AS numberOfActors Full Context: [{'numberOfActors': 4}] > Finished chain. 'Four people played in Top Gun.' Use separate LLMs for Cypher and answer generation ​ You can use the cypher_llm and qa_llm parameters to define different\nllms chain = GraphCypherQAChain . from_llm ( graph = graph , cypher_llm = ChatOpenAI ( temperature = 0 , model = \"gpt-3.5-turbo\" ) , qa_llm = ChatOpenAI ( temperature = 0 , model = \"gpt-3.5-turbo-16k\" ) , verbose = True , ) chain . run ( \"Who played in Top Gun?\" ) > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (a:Actor)-[:ACTED_IN]->(m:Movie {name: 'Top Gun'}) RETURN a.name Full Context: [{'a.name': 'Tom Cruise'}, {'a.name': 'Val Kilmer'}, {'a.name': 'Anthony Edwards'}, {'a.name': 'Meg Ryan'}] > Finished chain. 'Tom Cruise, Val Kilmer, Anthony Edwards, and Meg Ryan played in Top Gun.' Ignore specified node and relationship types You can use include_types or exclude_types to ignore parts of the\ngraph schema when generating Cypher statements. chain = GraphCypherQAChain . from_llm ( graph = graph , cypher_llm = ChatOpenAI ( temperature = 0 , model = \"gpt-3.5-turbo\" ) , qa_llm = ChatOpenAI ( temperature = 0 , model = \"gpt-3.5-turbo-16k\" ) , verbose = True , exclude_types = [ \"Movie\" ] , ) # Inspect graph schema print ( chain . graph_schema ) Node properties are the following: Actor {name: STRING} Relationship properties are the following: The relationships are the following: Validate generated Cypher statements You can use the validate_cypher parameter to validate and correct\nrelationship directions in generated Cypher statements chain = GraphCypherQAChain . from_llm ( llm = ChatOpenAI ( temperature = 0 , model = \"gpt-3.5-turbo\" ) , graph = graph , verbose = True , validate_cypher = True , ) chain . run ( \"Who played in Top Gun?\" ) > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (a:Actor)-[:ACTED_IN]->(m:Movie {name: 'Top Gun'}) RETURN a.name Full Context: [{'a.name': 'Tom Cruise'}, {'a.name': 'Val Kilmer'}, {'a.name': 'Anthony Edwards'}, {'a.name': 'Meg Ryan'}] > Finished chain. 'Tom Cruise, Val Kilmer, Anthony Edwards, and Meg Ryan played in Top Gun.' Previous ArangoDB QA chain Next FalkorDBQAChain"
    },
    {
        "title": "FalkorDBQAChain | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/graph/graph_falkordb_qa",
        "text_content": "Graph querying FalkorDBQAChain On this page FalkorDBQAChain This notebook shows how to use LLMs to provide a natural language\ninterface to FalkorDB database. FalkorDB is a low latency property graph database management system. You\ncan simply run its docker locally: docker run -p 6379 :6379 -it --rm falkordb/falkordb:edge Once launched, you can simply start creating a database on the local\nmachine and connect to it. from langchain . chains import FalkorDBQAChain from langchain . chat_models import ChatOpenAI from langchain . graphs import FalkorDBGraph Create a graph connection and insert some demo data. ​ graph = FalkorDBGraph ( database = \"movies\" ) graph . query ( \"\"\" CREATE (al:Person {name: 'Al Pacino', birthDate: '1940-04-25'}), (robert:Person {name: 'Robert De Niro', birthDate: '1943-08-17'}), (tom:Person {name: 'Tom Cruise', birthDate: '1962-07-3'}), (val:Person {name: 'Val Kilmer', birthDate: '1959-12-31'}), (anthony:Person {name: 'Anthony Edwards', birthDate: '1962-7-19'}), (meg:Person {name: 'Meg Ryan', birthDate: '1961-11-19'}), (god1:Movie {title: 'The Godfather'}), (god2:Movie {title: 'The Godfather: Part II'}), (god3:Movie {title: 'The Godfather Coda: The Death of Michael Corleone'}), (top:Movie {title: 'Top Gun'}), (al)-[:ACTED_IN]->(god1), (al)-[:ACTED_IN]->(god2), (al)-[:ACTED_IN]->(god3), (robert)-[:ACTED_IN]->(god2), (tom)-[:ACTED_IN]->(top), (val)-[:ACTED_IN]->(top), (anthony)-[:ACTED_IN]->(top), (meg)-[:ACTED_IN]->(top) \"\"\" ) [] Creating FalkorDBQAChain ​ graph . refresh_schema ( ) print ( graph . schema ) import os os . environ [ \"OPENAI_API_KEY\" ] = \"API_KEY_HERE\" Node properties: [[OrderedDict([('label', None), ('properties', ['name', 'birthDate', 'title'])])]] Relationships properties: [[OrderedDict([('type', None), ('properties', [])])]] Relationships: [['(:Person)-[:ACTED_IN]->(:Movie)']] chain = FalkorDBQAChain . from_llm ( ChatOpenAI ( temperature = 0 ) , graph = graph , verbose = True ) Querying the graph ​ chain . run ( \"Who played in Top Gun?\" ) > Entering new FalkorDBQAChain chain... Generated Cypher: MATCH (p:Person)-[:ACTED_IN]->(m:Movie) WHERE m.title = 'Top Gun' RETURN p.name Full Context: [['Tom Cruise'], ['Val Kilmer'], ['Anthony Edwards'], ['Meg Ryan'], ['Tom Cruise'], ['Val Kilmer'], ['Anthony Edwards'], ['Meg Ryan']] > Finished chain. 'Tom Cruise, Val Kilmer, Anthony Edwards, and Meg Ryan played in Top Gun.' chain . run ( \"Who is the oldest actor who played in The Godfather: Part II?\" ) > Entering new FalkorDBQAChain chain... Generated Cypher: MATCH (p:Person)-[r:ACTED_IN]->(m:Movie) WHERE m.title = 'The Godfather: Part II' RETURN p.name ORDER BY p.birthDate ASC LIMIT 1 Full Context: [['Al Pacino']] > Finished chain. 'The oldest actor who played in The Godfather: Part II is Al Pacino.' chain . run ( \"Robert De Niro played in which movies?\" ) > Entering new FalkorDBQAChain chain... Generated Cypher: MATCH (p:Person {name: 'Robert De Niro'})-[:ACTED_IN]->(m:Movie) RETURN m.title Full Context: [['The Godfather: Part II'], ['The Godfather: Part II']] > Finished chain. 'Robert De Niro played in \"The Godfather: Part II\".' Previous Neo4j DB QA chain Next HugeGraph QA Chain"
    },
    {
        "title": "HugeGraph QA Chain | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/graph/graph_hugegraph_qa",
        "text_content": "Graph querying HugeGraph QA Chain On this page HugeGraph QA Chain This notebook shows how to use LLMs to provide a natural language\ninterface to HugeGraph database. You will need to have a running HugeGraph instance. You can run a local\ndocker container by running the executing the following script: docker run \\ --name=graph \\ -itd \\ -p 8080:8080 \\ hugegraph/hugegraph If we want to connect HugeGraph in the application, we need to install\npython sdk: pip3 install hugegraph-python If you are using the docker container, you need to wait a couple of\nsecond for the database to start, and then we need create schema and\nwrite graph data for the database. from hugegraph . connection import PyHugeGraph client = PyHugeGraph ( \"localhost\" , \"8080\" , user = \"admin\" , pwd = \"admin\" , graph = \"hugegraph\" ) First, we create the schema for a simple movie database: \"\"\"schema\"\"\" schema = client . schema ( ) schema . propertyKey ( \"name\" ) . asText ( ) . ifNotExist ( ) . create ( ) schema . propertyKey ( \"birthDate\" ) . asText ( ) . ifNotExist ( ) . create ( ) schema . vertexLabel ( \"Person\" ) . properties ( \"name\" , \"birthDate\" ) . usePrimaryKeyId ( ) . primaryKeys ( \"name\" ) . ifNotExist ( ) . create ( ) schema . vertexLabel ( \"Movie\" ) . properties ( \"name\" ) . usePrimaryKeyId ( ) . primaryKeys ( \"name\" ) . ifNotExist ( ) . create ( ) schema . edgeLabel ( \"ActedIn\" ) . sourceLabel ( \"Person\" ) . targetLabel ( \"Movie\" ) . ifNotExist ( ) . create ( ) 'create EdgeLabel success, Detail: \"b\\'{\"id\":1,\"name\":\"ActedIn\",\"source_label\":\"Person\",\"target_label\":\"Movie\",\"frequency\":\"SINGLE\",\"sort_keys\":[],\"nullable_keys\":[],\"index_labels\":[],\"properties\":[],\"status\":\"CREATED\",\"ttl\":0,\"enable_label_index\":true,\"user_data\":{\"~create_time\":\"2023-07-04 10:48:47.908\"}}\\'\"' Then we can insert some data. \"\"\"graph\"\"\" g = client . graph ( ) g . addVertex ( \"Person\" , { \"name\" : \"Al Pacino\" , \"birthDate\" : \"1940-04-25\" } ) g . addVertex ( \"Person\" , { \"name\" : \"Robert De Niro\" , \"birthDate\" : \"1943-08-17\" } ) g . addVertex ( \"Movie\" , { \"name\" : \"The Godfather\" } ) g . addVertex ( \"Movie\" , { \"name\" : \"The Godfather Part II\" } ) g . addVertex ( \"Movie\" , { \"name\" : \"The Godfather Coda The Death of Michael Corleone\" } ) g . addEdge ( \"ActedIn\" , \"1:Al Pacino\" , \"2:The Godfather\" , { } ) g . addEdge ( \"ActedIn\" , \"1:Al Pacino\" , \"2:The Godfather Part II\" , { } ) g . addEdge ( \"ActedIn\" , \"1:Al Pacino\" , \"2:The Godfather Coda The Death of Michael Corleone\" , { } ) g . addEdge ( \"ActedIn\" , \"1:Robert De Niro\" , \"2:The Godfather Part II\" , { } ) 1:Robert De Niro--ActedIn-->2:The Godfather Part II Creating HugeGraphQAChain ​ We can now create the HugeGraph and HugeGraphQAChain . To create the HugeGraph we simply need to pass the database object to the HugeGraph constructor. from langchain . chains import HugeGraphQAChain from langchain . chat_models import ChatOpenAI from langchain . graphs import HugeGraph graph = HugeGraph ( username = \"admin\" , password = \"admin\" , address = \"localhost\" , port = 8080 , graph = \"hugegraph\" , ) Refresh graph schema information ​ If the schema of database changes, you can refresh the schema\ninformation needed to generate Gremlin statements. # graph.refresh_schema() print ( graph . get_schema ) Node properties: [name: Person, primary_keys: ['name'], properties: ['name', 'birthDate'], name: Movie, primary_keys: ['name'], properties: ['name']] Edge properties: [name: ActedIn, properties: []] Relationships: ['Person--ActedIn-->Movie'] Querying the graph ​ We can now use the graph Gremlin QA chain to ask question of the graph chain = HugeGraphQAChain . from_llm ( ChatOpenAI ( temperature = 0 ) , graph = graph , verbose = True ) chain . run ( \"Who played in The Godfather?\" ) > Entering new  chain... Generated gremlin: g.V().has('Movie', 'name', 'The Godfather').in('ActedIn').valueMap(true) Full Context: [{'id': '1:Al Pacino', 'label': 'Person', 'name': ['Al Pacino'], 'birthDate': ['1940-04-25']}] > Finished chain. 'Al Pacino played in The Godfather.' Previous FalkorDBQAChain Next KuzuQAChain"
    },
    {
        "title": "KuzuQAChain | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/graph/graph_kuzu_qa",
        "text_content": "Graph querying KuzuQAChain On this page KuzuQAChain This notebook shows how to use LLMs to provide a natural language\ninterface to Kùzu database. Kùzu is an in-process property graph database\nmanagement system. You can simply install it with pip : pip install kuzu Once installed, you can simply import it and start creating a database\non the local machine and connect to it: import kuzu db = kuzu . Database ( \"test_db\" ) conn = kuzu . Connection ( db ) First, we create the schema for a simple movie database: conn . execute ( \"CREATE NODE TABLE Movie (name STRING, PRIMARY KEY(name))\" ) conn . execute ( \"CREATE NODE TABLE Person (name STRING, birthDate STRING, PRIMARY KEY(name))\" ) conn . execute ( \"CREATE REL TABLE ActedIn (FROM Person TO Movie)\" ) <kuzu.query_result.QueryResult at 0x1066ff410> Then we can insert some data. conn . execute ( \"CREATE (:Person {name: 'Al Pacino', birthDate: '1940-04-25'})\" ) conn . execute ( \"CREATE (:Person {name: 'Robert De Niro', birthDate: '1943-08-17'})\" ) conn . execute ( \"CREATE (:Movie {name: 'The Godfather'})\" ) conn . execute ( \"CREATE (:Movie {name: 'The Godfather: Part II'})\" ) conn . execute ( \"CREATE (:Movie {name: 'The Godfather Coda: The Death of Michael Corleone'})\" ) conn . execute ( \"MATCH (p:Person), (m:Movie) WHERE p.name = 'Al Pacino' AND m.name = 'The Godfather' CREATE (p)-[:ActedIn]->(m)\" ) conn . execute ( \"MATCH (p:Person), (m:Movie) WHERE p.name = 'Al Pacino' AND m.name = 'The Godfather: Part II' CREATE (p)-[:ActedIn]->(m)\" ) conn . execute ( \"MATCH (p:Person), (m:Movie) WHERE p.name = 'Al Pacino' AND m.name = 'The Godfather Coda: The Death of Michael Corleone' CREATE (p)-[:ActedIn]->(m)\" ) conn . execute ( \"MATCH (p:Person), (m:Movie) WHERE p.name = 'Robert De Niro' AND m.name = 'The Godfather: Part II' CREATE (p)-[:ActedIn]->(m)\" ) <kuzu.query_result.QueryResult at 0x107016210> Creating KuzuQAChain ​ We can now create the KuzuGraph and KuzuQAChain . To create the KuzuGraph we simply need to pass the database object to the KuzuGraph constructor. from langchain . chains import KuzuQAChain from langchain . chat_models import ChatOpenAI from langchain . graphs import KuzuGraph graph = KuzuGraph ( db ) chain = KuzuQAChain . from_llm ( ChatOpenAI ( temperature = 0 ) , graph = graph , verbose = True ) Refresh graph schema information ​ If the schema of database changes, you can refresh the schema\ninformation needed to generate Cypher statements. # graph.refresh_schema() print ( graph . get_schema ) Node properties: [{'properties': [('name', 'STRING')], 'label': 'Movie'}, {'properties': [('name', 'STRING'), ('birthDate', 'STRING')], 'label': 'Person'}] Relationships properties: [{'properties': [], 'label': 'ActedIn'}] Relationships: ['(:Person)-[:ActedIn]->(:Movie)'] Querying the graph ​ We can now use the KuzuQAChain to ask question of the graph chain . run ( \"Who played in The Godfather: Part II?\" ) > Entering new  chain... Generated Cypher: MATCH (p:Person)-[:ActedIn]->(m:Movie {name: 'The Godfather: Part II'}) RETURN p.name Full Context: [{'p.name': 'Al Pacino'}, {'p.name': 'Robert De Niro'}] > Finished chain. 'Al Pacino and Robert De Niro both played in The Godfather: Part II.' chain . run ( \"Robert De Niro played in which movies?\" ) > Entering new  chain... Generated Cypher: MATCH (p:Person {name: 'Robert De Niro'})-[:ActedIn]->(m:Movie) RETURN m.name Full Context: [{'m.name': 'The Godfather: Part II'}] > Finished chain. 'Robert De Niro played in The Godfather: Part II.' chain . run ( \"Robert De Niro is born in which year?\" ) > Entering new  chain... Generated Cypher: MATCH (p:Person {name: 'Robert De Niro'})-[:ActedIn]->(m:Movie) RETURN p.birthDate Full Context: [{'p.birthDate': '1943-08-17'}] > Finished chain. 'Robert De Niro was born on August 17, 1943.' chain . run ( \"Who is the oldest actor who played in The Godfather: Part II?\" ) > Entering new  chain... Generated Cypher: MATCH (p:Person)-[:ActedIn]->(m:Movie{name:'The Godfather: Part II'}) WITH p, m, p.birthDate AS birthDate ORDER BY birthDate ASC LIMIT 1 RETURN p.name Full Context: [{'p.name': 'Al Pacino'}] > Finished chain. 'The oldest actor who played in The Godfather: Part II is Al Pacino.' Previous HugeGraph QA Chain Next Memgraph QA chain"
    },
    {
        "title": "Memgraph QA chain | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/graph/graph_memgraph_qa",
        "text_content": "Graph querying Memgraph QA chain On this page Memgraph QA chain This notebook shows how to use LLMs to provide a natural language\ninterface to a Memgraph database. To complete this tutorial, you will need Docker and Python\n3.x installed. To follow along with this tutorial, ensure you have a running Memgraph\ninstance. You can download and run it in a local Docker container by\nexecuting the following script: docker run \\ -it \\ -p 7687:7687 \\ -p 7444:7444 \\ -p 3000:3000 \\ -e MEMGRAPH=\"--bolt-server-name-for-init=Neo4j/\" \\ -v mg_lib:/var/lib/memgraph memgraph/memgraph-platform You will need to wait a few seconds for the database to start. If the\nprocess completes successfully, you should see something like this: mgconsole X.X Connected to 'memgraph://127.0.0.1:7687' Type :help for shell usage Quit the shell by typing Ctrl-D(eof) or :quit memgraph> Now you can start playing with Memgraph! Begin by installing and importing all the necessary packages. We’ll use\nthe package manager called pip , along with the --user flag, to ensure proper permissions. If you’ve installed Python\n3.4 or a later version, pip is included by default. You can install all\nthe required packages using the following command: pip install langchain openai neo4j gqlalchemy - - user You can either run the provided code blocks in this notebook or use a\nseparate Python file to experiment with Memgraph and LangChain. import os from gqlalchemy import Memgraph from langchain . chains import GraphCypherQAChain from langchain . chat_models import ChatOpenAI from langchain . graphs import MemgraphGraph from langchain . prompts import PromptTemplate We’re utilizing the Python library GQLAlchemy to establish a\nconnection between our Memgraph database and Python script. To execute\nqueries, we can set up a Memgraph instance as follows: memgraph = Memgraph ( host = \"127.0.0.1\" , port = 7687 ) Populating the database ​ You can effortlessly populate your new, empty database using the Cypher\nquery language. Don’t worry if you don’t grasp every line just yet, you\ncan learn Cypher from the documentation here . Running the following\nscript will execute a seeding query on the database, giving us data\nabout a video game, including details like the publisher, available\nplatforms, and genres. This data will serve as a basis for our work. # Creating and executing the seeding query query = \"\"\" MERGE (g:Game {name: \"Baldur's Gate 3\"}) WITH g, [\"PlayStation 5\", \"Mac OS\", \"Windows\", \"Xbox Series X/S\"] AS platforms, [\"Adventure\", \"Role-Playing Game\", \"Strategy\"] AS genres FOREACH (platform IN platforms | MERGE (p:Platform {name: platform}) MERGE (g)-[:AVAILABLE_ON]->(p) ) FOREACH (genre IN genres | MERGE (gn:Genre {name: genre}) MERGE (g)-[:HAS_GENRE]->(gn) ) MERGE (p:Publisher {name: \"Larian Studios\"}) MERGE (g)-[:PUBLISHED_BY]->(p); \"\"\" memgraph . execute ( query ) Refresh graph schema ​ You’re all set to instantiate the Memgraph-LangChain graph using the\nfollowing script. This interface will allow us to query our database\nusing LangChain, automatically creating the required graph schema for\ngenerating Cypher queries through LLM. graph = MemgraphGraph ( url = \"bolt://localhost:7687\" , username = \"\" , password = \"\" ) If necessary, you can manually refresh the graph schema as follows. graph . refresh_schema ( ) To familiarize yourself with the data and verify the updated graph\nschema, you can print it using the following statement. print ( graph . schema ) Node properties are the following: Node name: 'Game', Node properties: [{'property': 'name', 'type': 'str'}] Node name: 'Platform', Node properties: [{'property': 'name', 'type': 'str'}] Node name: 'Genre', Node properties: [{'property': 'name', 'type': 'str'}] Node name: 'Publisher', Node properties: [{'property': 'name', 'type': 'str'}] Relationship properties are the following: The relationships are the following: ['(:Game)-[:AVAILABLE_ON]->(:Platform)'] ['(:Game)-[:HAS_GENRE]->(:Genre)'] ['(:Game)-[:PUBLISHED_BY]->(:Publisher)'] Querying the database ​ To interact with the OpenAI API, you must configure your API key as an\nenvironment variable using the Python os package. This ensures\nproper authorization for your requests. You can find more information on\nobtaining your API key here . os . environ [ \"OPENAI_API_KEY\" ] = \"your-key-here\" You should create the graph chain using the following script, which will\nbe utilized in the question-answering process based on your graph data.\nWhile it defaults to GPT-3.5-turbo, you might also consider\nexperimenting with other models like GPT-4 for notably improved Cypher queries and outcomes. We’ll utilize the\nOpenAI chat, utilizing the key you previously configured. We’ll set the\ntemperature to zero, ensuring predictable and consistent answers.\nAdditionally, we’ll use our Memgraph-LangChain graph and set the verbose\nparameter, which defaults to False, to True to receive more detailed\nmessages regarding query generation. chain = GraphCypherQAChain . from_llm ( ChatOpenAI ( temperature = 0 ) , graph = graph , verbose = True , model_name = \"gpt-3.5-turbo\" ) Now you can start asking questions! response = chain . run ( \"Which platforms is Baldur's Gate 3 available on?\" ) print ( response ) > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (g:Game {name: 'Baldur\\'s Gate 3'})-[:AVAILABLE_ON]->(p:Platform) RETURN p.name Full Context: [{'p.name': 'PlayStation 5'}, {'p.name': 'Mac OS'}, {'p.name': 'Windows'}, {'p.name': 'Xbox Series X/S'}] > Finished chain. Baldur's Gate 3 is available on PlayStation 5, Mac OS, Windows, and Xbox Series X/S. response = chain . run ( \"Is Baldur's Gate 3 available on Windows?\" ) print ( response ) > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (:Game {name: 'Baldur\\'s Gate 3'})-[:AVAILABLE_ON]->(:Platform {name: 'Windows'}) RETURN true Full Context: [{'true': True}] > Finished chain. Yes, Baldur's Gate 3 is available on Windows. Chain modifiers ​ To modify the behavior of your chain and obtain more context or\nadditional information, you can modify the chain’s parameters. Return direct query results ​ The return_direct modifier specifies whether to return the direct\nresults of the executed Cypher query or the processed natural language\nresponse. # Return the result of querying the graph directly chain = GraphCypherQAChain . from_llm ( ChatOpenAI ( temperature = 0 ) , graph = graph , verbose = True , return_direct = True ) response = chain . run ( \"Which studio published Baldur's Gate 3?\" ) print ( response ) > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (:Game {name: 'Baldur\\'s Gate 3'})-[:PUBLISHED_BY]->(p:Publisher) RETURN p.name > Finished chain. [{'p.name': 'Larian Studios'}] Return query intermediate steps ​ The return_intermediate_steps chain modifier enhances the returned\nresponse by including the intermediate steps of the query in addition to\nthe initial query result. # Return all the intermediate steps of query execution chain = GraphCypherQAChain . from_llm ( ChatOpenAI ( temperature = 0 ) , graph = graph , verbose = True , return_intermediate_steps = True ) response = chain ( \"Is Baldur's Gate 3 an Adventure game?\" ) print ( f\"Intermediate steps: { response [ 'intermediate_steps' ] } \" ) print ( f\"Final response: { response [ 'result' ] } \" ) > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (g:Game {name: 'Baldur\\'s Gate 3'})-[:HAS_GENRE]->(genre:Genre {name: 'Adventure'}) RETURN g, genre Full Context: [{'g': {'name': \"Baldur's Gate 3\"}, 'genre': {'name': 'Adventure'}}] > Finished chain. Intermediate steps: [{'query': \"MATCH (g:Game {name: 'Baldur\\\\'s Gate 3'})-[:HAS_GENRE]->(genre:Genre {name: 'Adventure'})\\nRETURN g, genre\"}, {'context': [{'g': {'name': \"Baldur's Gate 3\"}, 'genre': {'name': 'Adventure'}}]}] Final response: Yes, Baldur's Gate 3 is an Adventure game. Limit the number of query results ​ The top_k modifier can be used when you want to restrict the maximum\nnumber of query results. # Limit the maximum number of results returned by query chain = GraphCypherQAChain . from_llm ( ChatOpenAI ( temperature = 0 ) , graph = graph , verbose = True , top_k = 2 ) response = chain . run ( \"What genres are associated with Baldur's Gate 3?\" ) print ( response ) > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (:Game {name: 'Baldur\\'s Gate 3'})-[:HAS_GENRE]->(g:Genre) RETURN g.name Full Context: [{'g.name': 'Adventure'}, {'g.name': 'Role-Playing Game'}] > Finished chain. Baldur's Gate 3 is associated with the genres Adventure and Role-Playing Game. Advanced querying As the complexity of your solution grows, you might encounter different\nuse-cases that require careful handling. Ensuring your application’s\nscalability is essential to maintain a smooth user flow without any\nhitches. Let’s instantiate our chain once again and attempt to ask some questions\nthat users might potentially ask. chain = GraphCypherQAChain . from_llm ( ChatOpenAI ( temperature = 0 ) , graph = graph , verbose = True , model_name = \"gpt-3.5-turbo\" ) response = chain . run ( \"Is Baldur's Gate 3 available on PS5?\" ) print ( response ) > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (g:Game {name: 'Baldur\\'s Gate 3'})-[:AVAILABLE_ON]->(p:Platform {name: 'PS5'}) RETURN g.name, p.name Full Context: [] > Finished chain. I'm sorry, but I don't have the information to answer your question. The generated Cypher query looks fine, but we didn’t receive any\ninformation in response. This illustrates a common challenge when\nworking with LLMs - the misalignment between how users phrase queries\nand how data is stored. In this case, the difference between user\nperception and the actual data storage can cause mismatches. Prompt\nrefinement, the process of honing the model’s prompts to better grasp\nthese distinctions, is an efficient solution that tackles this issue.\nThrough prompt refinement, the model gains increased proficiency in\ngenerating precise and pertinent queries, leading to the successful\nretrieval of the desired data. Prompt refinement ​ To address this, we can adjust the initial Cypher prompt of the QA\nchain. This involves adding guidance to the LLM on how users can refer\nto specific platforms, such as PS5 in our case. We achieve this using\nthe LangChain PromptTemplate ,\ncreating a modified initial prompt. This modified prompt is then\nsupplied as an argument to our refined Memgraph-LangChain instance. CYPHER_GENERATION_TEMPLATE = \"\"\" Task:Generate Cypher statement to query a graph database. Instructions: Use only the provided relationship types and properties in the schema. Do not use any other relationship types or properties that are not provided. Schema: {schema} Note: Do not include any explanations or apologies in your responses. Do not respond to any questions that might ask anything else than for you to construct a Cypher statement. Do not include any text except the generated Cypher statement. If the user asks about PS5, Play Station 5 or PS 5, that is the platform called PlayStation 5. The question is: {question} \"\"\" CYPHER_GENERATION_PROMPT = PromptTemplate ( input_variables = [ \"schema\" , \"question\" ] , template = CYPHER_GENERATION_TEMPLATE ) chain = GraphCypherQAChain . from_llm ( ChatOpenAI ( temperature = 0 ) , cypher_prompt = CYPHER_GENERATION_PROMPT , graph = graph , verbose = True , model_name = \"gpt-3.5-turbo\" , ) response = chain . run ( \"Is Baldur's Gate 3 available on PS5?\" ) print ( response ) > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (g:Game {name: 'Baldur\\'s Gate 3'})-[:AVAILABLE_ON]->(p:Platform {name: 'PlayStation 5'}) RETURN g.name, p.name Full Context: [{'g.name': \"Baldur's Gate 3\", 'p.name': 'PlayStation 5'}] > Finished chain. Yes, Baldur's Gate 3 is available on PlayStation 5. Now, with the revised initial Cypher prompt that includes guidance on\nplatform naming, we are obtaining accurate and relevant results that\nalign more closely with user queries. This approach allows for further improvement of your QA chain. You can\neffortlessly integrate extra prompt refinement data into your chain,\nthereby enhancing the overall user experience of your app. Previous KuzuQAChain Next NebulaGraphQAChain"
    },
    {
        "title": "NebulaGraphQAChain | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/graph/graph_nebula_qa",
        "text_content": "Graph querying NebulaGraphQAChain On this page NebulaGraphQAChain This notebook shows how to use LLMs to provide a natural language\ninterface to NebulaGraph database. You will need to have a running NebulaGraph cluster, for which you can\nrun a containerized cluster by running the following script: curl -fsSL nebula-up.siwei.io/install.sh | bash Other options are: - Install as a Docker Desktop\nExtension .\nSee here -\nNebulaGraph Cloud Service. See here - Deploy from package, source\ncode, or via Kubernetes. See here Once the cluster is running, we could create the SPACE and SCHEMA for\nthe database. % pip install ipython - ngql % load_ext ngql # connect ngql jupyter extension to nebulagraph % ngql - - address 127.0 .0 .1 - - port 9669 - - user root - - password nebula # create a new space % ngql CREATE SPACE IF NOT EXISTS langchain ( partition_num = 1 , replica_factor = 1 , vid_type = fixed_string ( 128 ) ) ; # Wait for a few seconds for the space to be created. % ngql USE langchain ; Create the schema, for full dataset, refer here . % % ngql CREATE TAG IF NOT EXISTS movie ( name string ) ; CREATE TAG IF NOT EXISTS person ( name string , birthdate string ) ; CREATE EDGE IF NOT EXISTS acted_in ( ) ; CREATE TAG INDEX IF NOT EXISTS person_index ON person ( name ( 128 ) ) ; CREATE TAG INDEX IF NOT EXISTS movie_index ON movie ( name ( 128 ) ) ; Wait for schema creation to complete, then we can insert some data. % % ngql INSERT VERTEX person ( name , birthdate ) VALUES \"Al Pacino\" : ( \"Al Pacino\" , \"1940-04-25\" ) ; INSERT VERTEX movie ( name ) VALUES \"The Godfather II\" : ( \"The Godfather II\" ) ; INSERT VERTEX movie ( name ) VALUES \"The Godfather Coda: The Death of Michael Corleone\" : ( \"The Godfather Coda: The Death of Michael Corleone\" ) ; INSERT EDGE acted_in ( ) VALUES \"Al Pacino\" - > \"The Godfather II\" : ( ) ; INSERT EDGE acted_in ( ) VALUES \"Al Pacino\" - > \"The Godfather Coda: The Death of Michael Corleone\" : ( ) ; UsageError: Cell magic `%%ngql` not found. from langchain . chains import NebulaGraphQAChain from langchain . chat_models import ChatOpenAI from langchain . graphs import NebulaGraph graph = NebulaGraph ( space = \"langchain\" , username = \"root\" , password = \"nebula\" , address = \"127.0.0.1\" , port = 9669 , session_pool_size = 30 , ) Refresh graph schema information ​ If the schema of database changes, you can refresh the schema\ninformation needed to generate nGQL statements. # graph.refresh_schema() print ( graph . get_schema ) Node properties: [{'tag': 'movie', 'properties': [('name', 'string')]}, {'tag': 'person', 'properties': [('name', 'string'), ('birthdate', 'string')]}] Edge properties: [{'edge': 'acted_in', 'properties': []}] Relationships: ['(:person)-[:acted_in]->(:movie)'] Querying the graph ​ We can now use the graph cypher QA chain to ask question of the graph chain = NebulaGraphQAChain . from_llm ( ChatOpenAI ( temperature = 0 ) , graph = graph , verbose = True ) chain . run ( \"Who played in The Godfather II?\" ) > Entering new NebulaGraphQAChain chain... Generated nGQL: MATCH (p:`person`)-[:acted_in]->(m:`movie`) WHERE m.`movie`.`name` == 'The Godfather II' RETURN p.`person`.`name` Full Context: {'p.person.name': ['Al Pacino']} > Finished chain. 'Al Pacino played in The Godfather II.' Previous Memgraph QA chain Next NetworkX"
    },
    {
        "title": "NetworkX | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/graph/graph_networkx_qa",
        "text_content": "Graph querying NetworkX On this page NetworkX NetworkX is a Python package for the\ncreation, manipulation, and study of the structure, dynamics, and\nfunctions of complex networks. This notebook goes over how to do question answering over a graph data\nstructure. Setting up ​ We have to install a Python package. !pip install networkx Create the graph ​ In this section, we construct an example graph. At the moment, this\nworks best for small pieces of text. from langchain . indexes import GraphIndexCreator from langchain . llms import OpenAI index_creator = GraphIndexCreator ( llm = OpenAI ( temperature = 0 ) ) with open ( \"../../../modules/state_of_the_union.txt\" ) as f : all_text = f . read ( ) We will use just a small snippet, because extracting the knowledge\ntriplets is a bit intensive at the moment. text = \"\\n\" . join ( all_text . split ( \"\\n\\n\" ) [ 105 : 108 ] ) text 'It won’t look like much, but if you stop and look closely, you’ll see a “Field of dreams,” the ground on which America’s future will be built. \\nThis is where Intel, the American company that helped build Silicon Valley, is going to build its $20 billion semiconductor “mega site”. \\nUp to eight state-of-the-art factories in one place. 10,000 new good-paying jobs. ' graph = index_creator . from_text ( text ) We can inspect the created graph. graph . get_triples ( ) [('Intel', '$20 billion semiconductor \"mega site\"', 'is going to build'), ('Intel', 'state-of-the-art factories', 'is building'), ('Intel', '10,000 new good-paying jobs', 'is creating'), ('Intel', 'Silicon Valley', 'is helping build'), ('Field of dreams', \"America's future will be built\", 'is the ground on which')] Querying the graph ​ We can now use the graph QA chain to ask question of the graph from langchain . chains import GraphQAChain chain = GraphQAChain . from_llm ( OpenAI ( temperature = 0 ) , graph = graph , verbose = True ) chain . run ( \"what is Intel going to build?\" ) > Entering new GraphQAChain chain... Entities Extracted: Intel Full Context: Intel is going to build $20 billion semiconductor \"mega site\" Intel is building state-of-the-art factories Intel is creating 10,000 new good-paying jobs Intel is helping build Silicon Valley > Finished chain. ' Intel is going to build a $20 billion semiconductor \"mega site\" with state-of-the-art factories, creating 10,000 new good-paying jobs and helping to build Silicon Valley.' Save the graph ​ We can also save and load the graph. graph . write_to_gml ( \"graph.gml\" ) from langchain . indexes . graph import NetworkxEntityGraph loaded_graph = NetworkxEntityGraph . from_gml ( \"graph.gml\" ) loaded_graph . get_triples ( ) [('Intel', '$20 billion semiconductor \"mega site\"', 'is going to build'), ('Intel', 'state-of-the-art factories', 'is building'), ('Intel', '10,000 new good-paying jobs', 'is creating'), ('Intel', 'Silicon Valley', 'is helping build'), ('Field of dreams', \"America's future will be built\", 'is the ground on which')] Previous NebulaGraphQAChain Next GraphSparqlQAChain"
    },
    {
        "title": "GraphSparqlQAChain | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/graph/graph_sparql_qa",
        "text_content": "Graph querying GraphSparqlQAChain On this page GraphSparqlQAChain Graph databases are an excellent choice for applications based on\nnetwork-like models. To standardize the syntax and semantics of such\ngraphs, the W3C recommends Semantic Web Technologies, cp. Semantic\nWeb . SPARQL serves as a query\nlanguage analogously to SQL or Cypher for these graphs. This notebook\ndemonstrates the application of LLMs as a natural language interface to\na graph database by generating SPARQL. Disclaimer: To date, SPARQL query generation via LLMs is still a bit\nunstable. Be especially careful with UPDATE queries, which alter the\ngraph. There are several sources you can run queries against, including files\non the web, files you have available locally, SPARQL endpoints, e.g., Wikidata , and triple stores . from langchain . chains import GraphSparqlQAChain from langchain . chat_models import ChatOpenAI from langchain . graphs import RdfGraph graph = RdfGraph ( source_file = \"http://www.w3.org/People/Berners-Lee/card\" , standard = \"rdf\" , local_copy = \"test.ttl\" , ) Note that providing a local_file is necessary for storing changes\nlocally if the source is read-only. Refresh graph schema information ​ If the schema of the database changes, you can refresh the schema\ninformation needed to generate SPARQL queries. graph . load_schema ( ) graph . get_schema In the following, each IRI is followed by the local name and optionally its description in parentheses. The RDF graph supports the following node types: <http://xmlns.com/foaf/0.1/PersonalProfileDocument> (PersonalProfileDocument, None), <http://www.w3.org/ns/auth/cert#RSAPublicKey> (RSAPublicKey, None), <http://www.w3.org/2000/10/swap/pim/contact#Male> (Male, None), <http://xmlns.com/foaf/0.1/Person> (Person, None), <http://www.w3.org/2006/vcard/ns#Work> (Work, None) The RDF graph supports the following relationships: <http://www.w3.org/2000/01/rdf-schema#seeAlso> (seeAlso, None), <http://purl.org/dc/elements/1.1/title> (title, None), <http://xmlns.com/foaf/0.1/mbox_sha1sum> (mbox_sha1sum, None), <http://xmlns.com/foaf/0.1/maker> (maker, None), <http://www.w3.org/ns/solid/terms#oidcIssuer> (oidcIssuer, None), <http://www.w3.org/2000/10/swap/pim/contact#publicHomePage> (publicHomePage, None), <http://xmlns.com/foaf/0.1/openid> (openid, None), <http://www.w3.org/ns/pim/space#storage> (storage, None), <http://xmlns.com/foaf/0.1/name> (name, None), <http://www.w3.org/2000/10/swap/pim/contact#country> (country, None), <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> (type, None), <http://www.w3.org/ns/solid/terms#profileHighlightColor> (profileHighlightColor, None), <http://www.w3.org/ns/pim/space#preferencesFile> (preferencesFile, None), <http://www.w3.org/2000/01/rdf-schema#label> (label, None), <http://www.w3.org/ns/auth/cert#modulus> (modulus, None), <http://www.w3.org/2000/10/swap/pim/contact#participant> (participant, None), <http://www.w3.org/2000/10/swap/pim/contact#street2> (street2, None), <http://www.w3.org/2006/vcard/ns#locality> (locality, None), <http://xmlns.com/foaf/0.1/nick> (nick, None), <http://xmlns.com/foaf/0.1/homepage> (homepage, None), <http://creativecommons.org/ns#license> (license, None), <http://xmlns.com/foaf/0.1/givenname> (givenname, None), <http://www.w3.org/2006/vcard/ns#street-address> (street-address, None), <http://www.w3.org/2006/vcard/ns#postal-code> (postal-code, None), <http://www.w3.org/2000/10/swap/pim/contact#street> (street, None), <http://www.w3.org/2003/01/geo/wgs84_pos#lat> (lat, None), <http://xmlns.com/foaf/0.1/primaryTopic> (primaryTopic, None), <http://www.w3.org/2006/vcard/ns#fn> (fn, None), <http://www.w3.org/2003/01/geo/wgs84_pos#location> (location, None), <http://usefulinc.com/ns/doap#developer> (developer, None), <http://www.w3.org/2000/10/swap/pim/contact#city> (city, None), <http://www.w3.org/2006/vcard/ns#region> (region, None), <http://xmlns.com/foaf/0.1/member> (member, None), <http://www.w3.org/2003/01/geo/wgs84_pos#long> (long, None), <http://www.w3.org/2000/10/swap/pim/contact#address> (address, None), <http://xmlns.com/foaf/0.1/family_name> (family_name, None), <http://xmlns.com/foaf/0.1/account> (account, None), <http://xmlns.com/foaf/0.1/workplaceHomepage> (workplaceHomepage, None), <http://purl.org/dc/terms/title> (title, None), <http://www.w3.org/ns/solid/terms#publicTypeIndex> (publicTypeIndex, None), <http://www.w3.org/2000/10/swap/pim/contact#office> (office, None), <http://www.w3.org/2000/10/swap/pim/contact#homePage> (homePage, None), <http://xmlns.com/foaf/0.1/mbox> (mbox, None), <http://www.w3.org/2000/10/swap/pim/contact#preferredURI> (preferredURI, None), <http://www.w3.org/ns/solid/terms#profileBackgroundColor> (profileBackgroundColor, None), <http://schema.org/owns> (owns, None), <http://xmlns.com/foaf/0.1/based_near> (based_near, None), <http://www.w3.org/2006/vcard/ns#hasAddress> (hasAddress, None), <http://xmlns.com/foaf/0.1/img> (img, None), <http://www.w3.org/2000/10/swap/pim/contact#assistant> (assistant, None), <http://xmlns.com/foaf/0.1/title> (title, None), <http://www.w3.org/ns/auth/cert#key> (key, None), <http://www.w3.org/ns/ldp#inbox> (inbox, None), <http://www.w3.org/ns/solid/terms#editableProfile> (editableProfile, None), <http://www.w3.org/2000/10/swap/pim/contact#postalCode> (postalCode, None), <http://xmlns.com/foaf/0.1/weblog> (weblog, None), <http://www.w3.org/ns/auth/cert#exponent> (exponent, None), <http://rdfs.org/sioc/ns#avatar> (avatar, None) Querying the graph ​ Now, you can use the graph SPARQL QA chain to ask questions about the\ngraph. chain = GraphSparqlQAChain . from_llm ( ChatOpenAI ( temperature = 0 ) , graph = graph , verbose = True ) chain . run ( \"What is Tim Berners-Lee's work homepage?\" ) > Entering new GraphSparqlQAChain chain... Identified intent: SELECT Generated SPARQL: PREFIX foaf: <http://xmlns.com/foaf/0.1/> SELECT ?homepage WHERE { ?person foaf:name \"Tim Berners-Lee\" . ?person foaf:workplaceHomepage ?homepage . } Full Context: [] > Finished chain. \"Tim Berners-Lee's work homepage is http://www.w3.org/People/Berners-Lee/.\" Updating the graph ​ Analogously, you can update the graph, i.e., insert triples, using\nnatural language. chain . run ( \"Save that the person with the name 'Timothy Berners-Lee' has a work homepage at 'http://www.w3.org/foo/bar/'\" ) > Entering new GraphSparqlQAChain chain... Identified intent: UPDATE Generated SPARQL: PREFIX foaf: <http://xmlns.com/foaf/0.1/> INSERT { ?person foaf:workplaceHomepage <http://www.w3.org/foo/bar/> . } WHERE { ?person foaf:name \"Timothy Berners-Lee\" . } > Finished chain. 'Successfully inserted triples into the graph.' Let’s verify the results: query = ( \"\"\"PREFIX foaf: <http://xmlns.com/foaf/0.1/>\\n\"\"\" \"\"\"SELECT ?hp\\n\"\"\" \"\"\"WHERE {\\n\"\"\" \"\"\"    ?person foaf:name \"Timothy Berners-Lee\" . \\n\"\"\" \"\"\"    ?person foaf:workplaceHomepage ?hp .\\n\"\"\" \"\"\"}\"\"\" ) graph . query ( query ) [(rdflib.term.URIRef('https://www.w3.org/'),), (rdflib.term.URIRef('http://www.w3.org/foo/bar/'),)] Previous NetworkX Next Neptune Open Cypher QA Chain"
    },
    {
        "title": "Neptune Open Cypher QA Chain | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/use_cases/graph/neptune_cypher_qa",
        "text_content": "Graph querying Neptune Open Cypher QA Chain Neptune Open Cypher QA Chain This QA chain queries Neptune graph database using openCypher and\nreturns human readable response from langchain . graphs import NeptuneGraph host = \"<neptune-host>\" port = 8182 use_https = True graph = NeptuneGraph ( host = host , port = port , use_https = use_https ) from langchain . chains import NeptuneOpenCypherQAChain from langchain . chat_models import ChatOpenAI llm = ChatOpenAI ( temperature = 0 , model = \"gpt-4\" ) chain = NeptuneOpenCypherQAChain . from_llm ( llm = llm , graph = graph ) chain . run ( \"how many outgoing routes does the Austin airport have?\" ) 'The Austin airport has 98 outgoing routes.' Previous GraphSparqlQAChain"
    },
    {
        "title": "acreom | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/acreom",
        "text_content": "Components Document loaders acreom acreom acreom is a dev-first knowledge base with tasks\nrunning on local markdown files. Below is an example on how to load a local acreom vault into Langchain.\nAs the local vault in acreom is a folder of plain text .md files, the\nloader requires the path to the directory. Vault files may contain some metadata which is stored as a YAML header.\nThese values will be added to the document’s metadata if collect_metadata is set to true. from langchain . document_loaders import AcreomLoader loader = AcreomLoader ( \"<path-to-acreom-vault>\" , collect_metadata = False ) docs = loader . load ( ) Previous Document loaders Next Airbyte CDK"
    },
    {
        "title": "Airbyte CDK | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_cdk",
        "text_content": "Components Document loaders Airbyte CDK On this page Airbyte CDK Airbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases. A lot of source connectors are implemented using the Airbyte\nCDK . This\nloader allows to run any of these connectors and return the data as\ndocuments. Installation ​ First, you need to install the airbyte-cdk python package. #!pip install airbyte-cdk Then, either install an existing connector from the Airbyte Github\nrepository or create your own connector using the Airbyte\nCDK . For example, to install the Github connector, run #!pip install \"source_github@git+https://github.com/airbytehq/airbyte.git@master#subdirectory=airbyte-integrations/connectors/source-github\" Some sources are also published as regular packages on PyPI Example ​ Now you can create an AirbyteCDKLoader based on the imported source.\nIt takes a config object that’s passed to the connector. You also have\nto pick the stream you want to retrieve records from by name\n( stream_name ). Check the connectors documentation page and spec\ndefinition for more information on the config object and available\nstreams. For the Github connectors these are: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-github/source_github/spec.json . https://docs.airbyte.com/integrations/sources/github/ from langchain . document_loaders . airbyte import AirbyteCDKLoader from source_github . source import SourceGithub # plug in your own source here config = { # your github configuration \"credentials\" : { \"api_url\" : \"api.github.com\" , \"personal_access_token\" : \"<token>\" } , \"repository\" : \"<repo>\" , \"start_date\" : \"<date from which to start retrieving records from in ISO format, e.g. 2020-10-20T00:00:00Z>\" , } issues_loader = AirbyteCDKLoader ( source_class = SourceGithub , config = config , stream_name = \"issues\" ) Now you can load documents the usual way docs = issues_loader . load ( ) As load returns a list, it will block until all documents are loaded.\nTo have better control over this process, you can also you the lazy_load method which returns an iterator instead: docs_iterator = issues_loader . lazy_load ( ) Keep in mind that by default the page content is empty and the metadata\nobject contains all the information from the record. To create documents\nin a different, pass in a record_handler function when creating the\nloader: from langchain . docstore . document import Document def handle_record ( record , id ) : return Document ( page_content = record . data [ \"title\" ] + \"\\n\" + ( record . data [ \"body\" ] or \"\" ) , metadata = record . data , ) issues_loader = AirbyteCDKLoader ( source_class = SourceGithub , config = config , stream_name = \"issues\" , record_handler = handle_record , ) docs = issues_loader . load ( ) Incremental loads ​ Some streams allow incremental loading, this means the source keeps\ntrack of synced records and won’t load them again. This is useful for\nsources that have a high volume of data and are updated frequently. To take advantage of this, store the last_state property of the loader\nand pass it in when creating the loader again. This will ensure that\nonly new records are loaded. last_state = issues_loader . last_state # store safely incremental_issue_loader = AirbyteCDKLoader ( source_class = SourceGithub , config = config , stream_name = \"issues\" , state = last_state ) new_docs = incremental_issue_loader . load ( ) Previous acreom Next Airbyte Gong"
    },
    {
        "title": "Airbyte Gong | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong",
        "text_content": "Components Document loaders Airbyte Gong On this page Airbyte Gong Airbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases. This loader exposes the Gong connector as a document loader, allowing\nyou to load various Gong objects as documents. Installation ​ First, you need to install the airbyte-source-gong python package. #!pip install airbyte-source-gong Example ​ Check out the Airbyte documentation\npage for details\nabout how to configure the reader. The JSON schema the config object\nshould adhere to can be found on Github: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gong/source_gong/spec.yaml . The general shape looks like this: { \"access_key\" : \"<access key name>\" , \"access_key_secret\" : \"<access key secret>\" , \"start_date\" : \"<date from which to start retrieving records from in ISO format, e.g. 2020-10-20T00:00:00Z>\" , } By default all fields are stored as metadata in the documents and the\ntext is set to an empty string. Construct the text of the document by\ntransforming the documents returned by the reader. from langchain . document_loaders . airbyte import AirbyteGongLoader config = { # your gong configuration } loader = AirbyteGongLoader ( config = config , stream_name = \"calls\" ) # check the documentation linked above for a list of all streams Now you can load documents the usual way docs = loader . load ( ) As load returns a list, it will block until all documents are loaded.\nTo have better control over this process, you can also you the lazy_load method which returns an iterator instead: docs_iterator = loader . lazy_load ( ) Keep in mind that by default the page content is empty and the metadata\nobject contains all the information from the record. To process\ndocuments, create a class inheriting from the base loader and implement\nthe _handle_records method yourself: from langchain . docstore . document import Document def handle_record ( record , id ) : return Document ( page_content = record . data [ \"title\" ] , metadata = record . data ) loader = AirbyteGongLoader ( config = config , record_handler = handle_record , stream_name = \"calls\" ) docs = loader . load ( ) Incremental loads ​ Some streams allow incremental loading, this means the source keeps\ntrack of synced records and won’t load them again. This is useful for\nsources that have a high volume of data and are updated frequently. To take advantage of this, store the last_state property of the loader\nand pass it in when creating the loader again. This will ensure that\nonly new records are loaded. last_state = loader . last_state # store safely incremental_loader = AirbyteGongLoader ( config = config , stream_name = \"calls\" , state = last_state ) new_docs = incremental_loader . load ( ) Previous Airbyte CDK Next Airbyte Hubspot"
    },
    {
        "title": "Airbyte Hubspot | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_hubspot",
        "text_content": "Components Document loaders Airbyte Hubspot On this page Airbyte Hubspot Airbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases. This loader exposes the Hubspot connector as a document loader, allowing\nyou to load various Hubspot objects as documents. Installation ​ First, you need to install the airbyte-source-hubspot python package. #!pip install airbyte-source-hubspot Example ​ Check out the Airbyte documentation\npage for\ndetails about how to configure the reader. The JSON schema the config\nobject should adhere to can be found on Github: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hubspot/source_hubspot/spec.yaml . The general shape looks like this: { \"start_date\" : \"<date from which to start retrieving records from in ISO format, e.g. 2020-10-20T00:00:00Z>\" , \"credentials\" : { \"credentials_title\" : \"Private App Credentials\" , \"access_token\" : \"<access token of your private app>\" } } By default all fields are stored as metadata in the documents and the\ntext is set to an empty string. Construct the text of the document by\ntransforming the documents returned by the reader. from langchain . document_loaders . airbyte import AirbyteHubspotLoader config = { # your hubspot configuration } loader = AirbyteHubspotLoader ( config = config , stream_name = \"products\" ) # check the documentation linked above for a list of all streams Now you can load documents the usual way docs = loader . load ( ) As load returns a list, it will block until all documents are loaded.\nTo have better control over this process, you can also you the lazy_load method which returns an iterator instead: docs_iterator = loader . lazy_load ( ) Keep in mind that by default the page content is empty and the metadata\nobject contains all the information from the record. To process\ndocuments, create a class inheriting from the base loader and implement\nthe _handle_records method yourself: from langchain . docstore . document import Document def handle_record ( record , id ) : return Document ( page_content = record . data [ \"title\" ] , metadata = record . data ) loader = AirbyteHubspotLoader ( config = config , record_handler = handle_record , stream_name = \"products\" ) docs = loader . load ( ) Incremental loads ​ Some streams allow incremental loading, this means the source keeps\ntrack of synced records and won’t load them again. This is useful for\nsources that have a high volume of data and are updated frequently. To take advantage of this, store the last_state property of the loader\nand pass it in when creating the loader again. This will ensure that\nonly new records are loaded. last_state = loader . last_state # store safely incremental_loader = AirbyteHubspotLoader ( config = config , stream_name = \"products\" , state = last_state ) new_docs = incremental_loader . load ( ) Previous Airbyte Gong Next Airbyte JSON"
    },
    {
        "title": "Airbyte JSON | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_json",
        "text_content": "Components Document loaders Airbyte JSON Airbyte JSON Airbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases. This covers how to load any source from Airbyte into a local JSON file\nthat can be read in as a document Prereqs: Have docker desktop installed Steps: Clone Airbyte from GitHub - git clone https://github.com/airbytehq/airbyte.git Switch into Airbyte directory - cd airbyte Start Airbyte - docker compose up In your browser, just visit http://localhost:8000. You will be asked\nfor a username and password. By default, that’s\nusername airbyte and password password . Setup any source you wish. Set destination as Local JSON, with specified destination path -\nlets say /json_data . Set up manual sync. Run the connection. To see what files are create, you can navigate to: file:///tmp/airbyte_local Find your data and copy path. That path should be saved in the file\nvariable below. It should start with /tmp/airbyte_local from langchain . document_loaders import AirbyteJSONLoader !ls / tmp / airbyte_local / json_data / _airbyte_raw_pokemon.jsonl loader = AirbyteJSONLoader ( \"/tmp/airbyte_local/json_data/_airbyte_raw_pokemon.jsonl\" ) data = loader . load ( ) print ( data [ 0 ] . page_content [ : 500 ] ) abilities: ability: name: blaze url: https://pokeapi.co/api/v2/ability/66/ is_hidden: False slot: 1 ability: name: solar-power url: https://pokeapi.co/api/v2/ability/94/ is_hidden: True slot: 3 base_experience: 267 forms: name: charizard url: https://pokeapi.co/api/v2/pokemon-form/6/ game_indices: game_index: 180 version: name: red url: https://pokeapi.co/api/v2/version/1/ game_index: 180 version: name: blue url: https://pokeapi.co/api/v2/version/2/ game_index: 180 version: n Previous Airbyte Hubspot Next Airbyte Salesforce"
    },
    {
        "title": "Airbyte Salesforce | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_salesforce",
        "text_content": "Components Document loaders Airbyte Salesforce On this page Airbyte Salesforce Airbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases. This loader exposes the Salesforce connector as a document loader,\nallowing you to load various Salesforce objects as documents. Installation ​ First, you need to install the airbyte-source-salesforce python\npackage. #!pip install airbyte-source-salesforce Example ​ Check out the Airbyte documentation\npage for\ndetails about how to configure the reader. The JSON schema the config\nobject should adhere to can be found on Github: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-salesforce/source_salesforce/spec.yaml . The general shape looks like this: { \"client_id\" : \"<oauth client id>\" , \"client_secret\" : \"<oauth client secret>\" , \"refresh_token\" : \"<oauth refresh token>\" , \"start_date\" : \"<date from which to start retrieving records from in ISO format, e.g. 2020-10-20T00:00:00Z>\" , \"is_sandbox\" : False , # set to True if you're using a sandbox environment \"streams_criteria\" : [ # Array of filters for salesforce objects that should be loadable { \"criteria\" : \"exacts\" , \"value\" : \"Account\" } , # Exact name of salesforce object { \"criteria\" : \"starts with\" , \"value\" : \"Asset\" } , # Prefix of the name # Other allowed criteria: ends with, contains, starts not with, ends not with, not contains, not exacts ] , } By default all fields are stored as metadata in the documents and the\ntext is set to an empty string. Construct the text of the document by\ntransforming the documents returned by the reader. from langchain . document_loaders . airbyte import AirbyteSalesforceLoader config = { # your salesforce configuration } loader = AirbyteSalesforceLoader ( config = config , stream_name = \"asset\" ) # check the documentation linked above for a list of all streams Now you can load documents the usual way docs = loader . load ( ) As load returns a list, it will block until all documents are loaded.\nTo have better control over this process, you can also you the lazy_load method which returns an iterator instead: docs_iterator = loader . lazy_load ( ) Keep in mind that by default the page content is empty and the metadata\nobject contains all the information from the record. To create documents\nin a different, pass in a record_handler function when creating the\nloader: from langchain . docstore . document import Document def handle_record ( record , id ) : return Document ( page_content = record . data [ \"title\" ] , metadata = record . data ) loader = AirbyteSalesforceLoader ( config = config , record_handler = handle_record , stream_name = \"asset\" ) docs = loader . load ( ) Incremental loads ​ Some streams allow incremental loading, this means the source keeps\ntrack of synced records and won’t load them again. This is useful for\nsources that have a high volume of data and are updated frequently. To take advantage of this, store the last_state property of the loader\nand pass it in when creating the loader again. This will ensure that\nonly new records are loaded. last_state = loader . last_state # store safely incremental_loader = AirbyteSalesforceLoader ( config = config , stream_name = \"asset\" , state = last_state ) new_docs = incremental_loader . load ( ) Previous Airbyte JSON Next Airbyte Shopify"
    },
    {
        "title": "Airbyte Shopify | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_shopify",
        "text_content": "Components Document loaders Airbyte Shopify On this page Airbyte Shopify Airbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases. This loader exposes the Shopify connector as a document loader, allowing\nyou to load various Shopify objects as documents. Installation ​ First, you need to install the airbyte-source-shopify python package. #!pip install airbyte-source-shopify Example ​ Check out the Airbyte documentation\npage for\ndetails about how to configure the reader. The JSON schema the config\nobject should adhere to can be found on Github: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shopify/source_shopify/spec.json . The general shape looks like this: { \"start_date\" : \"<date from which to start retrieving records from in ISO format, e.g. 2020-10-20T00:00:00Z>\" , \"shop\" : \"<name of the shop you want to retrieve documents from>\" , \"credentials\" : { \"auth_method\" : \"api_password\" , \"api_password\" : \"<your api password>\" } } By default all fields are stored as metadata in the documents and the\ntext is set to an empty string. Construct the text of the document by\ntransforming the documents returned by the reader. from langchain . document_loaders . airbyte import AirbyteShopifyLoader config = { # your shopify configuration } loader = AirbyteShopifyLoader ( config = config , stream_name = \"orders\" ) # check the documentation linked above for a list of all streams Now you can load documents the usual way docs = loader . load ( ) As load returns a list, it will block until all documents are loaded.\nTo have better control over this process, you can also you the lazy_load method which returns an iterator instead: docs_iterator = loader . lazy_load ( ) Keep in mind that by default the page content is empty and the metadata\nobject contains all the information from the record. To create documents\nin a different, pass in a record_handler function when creating the\nloader: from langchain . docstore . document import Document def handle_record ( record , id ) : return Document ( page_content = record . data [ \"title\" ] , metadata = record . data ) loader = AirbyteShopifyLoader ( config = config , record_handler = handle_record , stream_name = \"orders\" ) docs = loader . load ( ) Incremental loads ​ Some streams allow incremental loading, this means the source keeps\ntrack of synced records and won’t load them again. This is useful for\nsources that have a high volume of data and are updated frequently. To take advantage of this, store the last_state property of the loader\nand pass it in when creating the loader again. This will ensure that\nonly new records are loaded. last_state = loader . last_state # store safely incremental_loader = AirbyteShopifyLoader ( config = config , stream_name = \"orders\" , state = last_state ) new_docs = incremental_loader . load ( ) Previous Airbyte Salesforce Next Airbyte Stripe"
    },
    {
        "title": "Airbyte Stripe | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_stripe",
        "text_content": "Components Document loaders Airbyte Stripe On this page Airbyte Stripe Airbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases. This loader exposes the Stripe connector as a document loader, allowing\nyou to load various Stripe objects as documents. Installation ​ First, you need to install the airbyte-source-stripe python package. #!pip install airbyte-source-stripe Example ​ Check out the Airbyte documentation\npage for details\nabout how to configure the reader. The JSON schema the config object\nshould adhere to can be found on Github: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-stripe/source_stripe/spec.yaml . The general shape looks like this: { \"client_secret\" : \"<secret key>\" , \"account_id\" : \"<account id>\" , \"start_date\" : \"<date from which to start retrieving records from in ISO format, e.g. 2020-10-20T00:00:00Z>\" , } By default all fields are stored as metadata in the documents and the\ntext is set to an empty string. Construct the text of the document by\ntransforming the documents returned by the reader. from langchain . document_loaders . airbyte import AirbyteStripeLoader config = { # your stripe configuration } loader = AirbyteStripeLoader ( config = config , stream_name = \"invoices\" ) # check the documentation linked above for a list of all streams Now you can load documents the usual way docs = loader . load ( ) As load returns a list, it will block until all documents are loaded.\nTo have better control over this process, you can also you the lazy_load method which returns an iterator instead: docs_iterator = loader . lazy_load ( ) Keep in mind that by default the page content is empty and the metadata\nobject contains all the information from the record. To create documents\nin a different, pass in a record_handler function when creating the\nloader: from langchain . docstore . document import Document def handle_record ( record , id ) : return Document ( page_content = record . data [ \"title\" ] , metadata = record . data ) loader = AirbyteStripeLoader ( config = config , record_handler = handle_record , stream_name = \"invoices\" ) docs = loader . load ( ) Incremental loads ​ Some streams allow incremental loading, this means the source keeps\ntrack of synced records and won’t load them again. This is useful for\nsources that have a high volume of data and are updated frequently. To take advantage of this, store the last_state property of the loader\nand pass it in when creating the loader again. This will ensure that\nonly new records are loaded. last_state = loader . last_state # store safely incremental_loader = AirbyteStripeLoader ( config = config , record_handler = handle_record , stream_name = \"invoices\" , state = last_state , ) new_docs = incremental_loader . load ( ) Previous Airbyte Shopify Next Airbyte Typeform"
    },
    {
        "title": "Airbyte Typeform | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_typeform",
        "text_content": "Components Document loaders Airbyte Typeform On this page Airbyte Typeform Airbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases. This loader exposes the Typeform connector as a document loader,\nallowing you to load various Typeform objects as documents. Installation ​ First, you need to install the airbyte-source-typeform python package. #!pip install airbyte-source-typeform Example ​ Check out the Airbyte documentation\npage for\ndetails about how to configure the reader. The JSON schema the config\nobject should adhere to can be found on Github: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-typeform/source_typeform/spec.json . The general shape looks like this: { \"credentials\" : { \"auth_type\" : \"Private Token\" , \"access_token\" : \"<your auth token>\" } , \"start_date\" : \"<date from which to start retrieving records from in ISO format, e.g. 2020-10-20T00:00:00Z>\" , \"form_ids\" : [ \"<id of form to load records for>\" ] # if omitted, records from all forms will be loaded } By default all fields are stored as metadata in the documents and the\ntext is set to an empty string. Construct the text of the document by\ntransforming the documents returned by the reader. from langchain . document_loaders . airbyte import AirbyteTypeformLoader config = { # your typeform configuration } loader = AirbyteTypeformLoader ( config = config , stream_name = \"forms\" ) # check the documentation linked above for a list of all streams Now you can load documents the usual way docs = loader . load ( ) As load returns a list, it will block until all documents are loaded.\nTo have better control over this process, you can also you the lazy_load method which returns an iterator instead: docs_iterator = loader . lazy_load ( ) Keep in mind that by default the page content is empty and the metadata\nobject contains all the information from the record. To create documents\nin a different, pass in a record_handler function when creating the\nloader: from langchain . docstore . document import Document def handle_record ( record , id ) : return Document ( page_content = record . data [ \"title\" ] , metadata = record . data ) loader = AirbyteTypeformLoader ( config = config , record_handler = handle_record , stream_name = \"forms\" ) docs = loader . load ( ) Incremental loads ​ Some streams allow incremental loading, this means the source keeps\ntrack of synced records and won’t load them again. This is useful for\nsources that have a high volume of data and are updated frequently. To take advantage of this, store the last_state property of the loader\nand pass it in when creating the loader again. This will ensure that\nonly new records are loaded. last_state = loader . last_state # store safely incremental_loader = AirbyteTypeformLoader ( config = config , record_handler = handle_record , stream_name = \"forms\" , state = last_state ) new_docs = incremental_loader . load ( ) Previous Airbyte Stripe Next Airbyte Zendesk Support"
    },
    {
        "title": "Airbyte Zendesk Support | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_zendesk_support",
        "text_content": "Components Document loaders Airbyte Zendesk Support On this page Airbyte Zendesk Support Airbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases. This loader exposes the Zendesk Support connector as a document loader,\nallowing you to load various objects as documents. Installation ​ First, you need to install the airbyte-source-zendesk-support python\npackage. #!pip install airbyte-source-zendesk-support Example ​ Check out the Airbyte documentation\npage for details about how to configure the reader. The JSON schema the\nconfig object should adhere to can be found on Github: https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-support/source_zendesk_support/spec.json . The general shape looks like this: { \"subdomain\" : \"<your zendesk subdomain>\" , \"start_date\" : \"<date from which to start retrieving records from in ISO format, e.g. 2020-10-20T00:00:00Z>\" , \"credentials\" : { \"credentials\" : \"api_token\" , \"email\" : \"<your email>\" , \"api_token\" : \"<your api token>\" } } By default all fields are stored as metadata in the documents and the\ntext is set to an empty string. Construct the text of the document by\ntransforming the documents returned by the reader. from langchain . document_loaders . airbyte import AirbyteZendeskSupportLoader config = { # your zendesk-support configuration } loader = AirbyteZendeskSupportLoader ( config = config , stream_name = \"tickets\" ) # check the documentation linked above for a list of all streams Now you can load documents the usual way docs = loader . load ( ) As load returns a list, it will block until all documents are loaded.\nTo have better control over this process, you can also you the lazy_load method which returns an iterator instead: docs_iterator = loader . lazy_load ( ) Keep in mind that by default the page content is empty and the metadata\nobject contains all the information from the record. To create documents\nin a different, pass in a record_handler function when creating the\nloader: from langchain . docstore . document import Document def handle_record ( record , id ) : return Document ( page_content = record . data [ \"title\" ] , metadata = record . data ) loader = AirbyteZendeskSupportLoader ( config = config , record_handler = handle_record , stream_name = \"tickets\" ) docs = loader . load ( ) Incremental loads ​ Some streams allow incremental loading, this means the source keeps\ntrack of synced records and won’t load them again. This is useful for\nsources that have a high volume of data and are updated frequently. To take advantage of this, store the last_state property of the loader\nand pass it in when creating the loader again. This will ensure that\nonly new records are loaded. last_state = loader . last_state # store safely incremental_loader = AirbyteZendeskSupportLoader ( config = config , stream_name = \"tickets\" , state = last_state ) new_docs = incremental_loader . load ( ) Previous Airbyte Typeform Next Airtable"
    },
    {
        "title": "Airtable | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/airtable",
        "text_content": "Components Document loaders Airtable Airtable ! pip install pyairtable from langchain . document_loaders import AirtableLoader Get your API key here . Get ID of your base here . Get your table ID from the table url as shown here . api_key = \"xxx\" base_id = \"xxx\" table_id = \"xxx\" loader = AirtableLoader ( api_key , table_id , base_id ) docs = loader . load ( ) Returns each table row as dict . len ( docs ) 3 eval ( docs [ 0 ] . page_content ) {'id': 'recF3GbGZCuh9sXIQ', 'createdTime': '2023-06-09T04:47:21.000Z', 'fields': {'Priority': 'High', 'Status': 'In progress', 'Name': 'Document Splitters'}} Previous Airbyte Zendesk Support Next Alibaba Cloud MaxCompute"
    },
    {
        "title": "Alibaba Cloud MaxCompute | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/alibaba_cloud_maxcompute",
        "text_content": "Components Document loaders Alibaba Cloud MaxCompute On this page Alibaba Cloud MaxCompute Alibaba Cloud\nMaxCompute (previously known as ODPS) is a general purpose, fully managed,\nmulti-tenancy data processing platform for large-scale data\nwarehousing. MaxCompute supports various data importing solutions and\ndistributed computing models, enabling users to effectively query\nmassive datasets, reduce production costs, and ensure data security. The MaxComputeLoader lets you execute a MaxCompute SQL query and loads\nthe results as one document per row. !pip install pyodps Collecting pyodps Downloading pyodps-0.11.4.post0-cp39-cp39-macosx_10_9_universal2.whl (2.0 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 1.7 MB/s eta 0:00:0000:0100:010m Requirement already satisfied: charset-normalizer>=2 in /Users/newboy/anaconda3/envs/langchain/lib/python3.9/site-packages (from pyodps) (3.1.0) Requirement already satisfied: urllib3<2.0,>=1.26.0 in /Users/newboy/anaconda3/envs/langchain/lib/python3.9/site-packages (from pyodps) (1.26.15) Requirement already satisfied: idna>=2.5 in /Users/newboy/anaconda3/envs/langchain/lib/python3.9/site-packages (from pyodps) (3.4) Requirement already satisfied: certifi>=2017.4.17 in /Users/newboy/anaconda3/envs/langchain/lib/python3.9/site-packages (from pyodps) (2023.5.7) Installing collected packages: pyodps Successfully installed pyodps-0.11.4.post0 Basic Usage ​ To instantiate the loader you’ll need a SQL query to execute, your\nMaxCompute endpoint and project name, and you access ID and secret\naccess key. The access ID and secret access key can either be passed in\ndirect via the access_id and secret_access_key parameters or they\ncan be set as environment variables MAX_COMPUTE_ACCESS_ID and MAX_COMPUTE_SECRET_ACCESS_KEY . from langchain . document_loaders import MaxComputeLoader base_query = \"\"\" SELECT * FROM ( SELECT 1 AS id, 'content1' AS content, 'meta_info1' AS meta_info UNION ALL SELECT 2 AS id, 'content2' AS content, 'meta_info2' AS meta_info UNION ALL SELECT 3 AS id, 'content3' AS content, 'meta_info3' AS meta_info ) mydata; \"\"\" endpoint = \"<ENDPOINT>\" project = \"<PROJECT>\" ACCESS_ID = \"<ACCESS ID>\" SECRET_ACCESS_KEY = \"<SECRET ACCESS KEY>\" loader = MaxComputeLoader . from_params ( base_query , endpoint , project , access_id = ACCESS_ID , secret_access_key = SECRET_ACCESS_KEY , ) data = loader . load ( ) print ( data ) [Document(page_content='id: 1\\ncontent: content1\\nmeta_info: meta_info1', metadata={}), Document(page_content='id: 2\\ncontent: content2\\nmeta_info: meta_info2', metadata={}), Document(page_content='id: 3\\ncontent: content3\\nmeta_info: meta_info3', metadata={})] print ( data [ 0 ] . page_content ) id: 1 content: content1 meta_info: meta_info1 print ( data [ 0 ] . metadata ) {} Specifying Which Columns are Content vs Metadata ​ You can configure which subset of columns should be loaded as the\ncontents of the Document and which as the metadata using the page_content_columns and metadata_columns parameters. loader = MaxComputeLoader . from_params ( base_query , endpoint , project , page_content_columns = [ \"content\" ] , # Specify Document page content metadata_columns = [ \"id\" , \"meta_info\" ] , # Specify Document metadata access_id = ACCESS_ID , secret_access_key = SECRET_ACCESS_KEY , ) data = loader . load ( ) print ( data [ 0 ] . page_content ) content: content1 print ( data [ 0 ] . metadata ) {'id': 1, 'meta_info': 'meta_info1'} Previous Airtable Next Amazon Textract"
    },
    {
        "title": "Amazon Textract | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/amazon_textract",
        "text_content": "Components Document loaders Amazon Textract On this page Amazon Textract Amazon\nTextract is a machine learning (ML) service that automatically extracts text,\nhandwriting, and data from scanned documents. It goes beyond simple optical character recognition (OCR) to identify,\nunderstand, and extract data from forms and tables. Today, many\ncompanies manually extract data from scanned documents such as PDFs,\nimages, tables, and forms, or through simple OCR software that\nrequires manual configuration (which often must be updated when the\nform changes). To overcome these manual and expensive processes, Textract uses ML to read and process any type of document,\naccurately extracting text, handwriting, tables, and other data with\nno manual effort. This sample demonstrates the use of Amazon Textract in combination\nwith LangChain as a DocumentLoader. Textract supports PDF , TIF F, PNG and JPEG format. Textract supports these document sizes, languages and\ncharacters . #!pip install boto3 openai tiktoken python-dotenv #!pip install \"amazon-textract-caller>=0.2.0\" Sample 1 ​ The first example uses a local file, which internally will be send to\nAmazon Textract sync API DetectDocumentText . Local files or URL endpoints like HTTP:// are limited to one page\ndocuments for Textract. Multi-page documents have to reside on S3. This\nsample file is a jpeg. from langchain . document_loaders import AmazonTextractPDFLoader loader = AmazonTextractPDFLoader ( \"example_data/alejandro_rosalez_sample-small.jpeg\" ) documents = loader . load ( ) Output from the file documents [Document(page_content='Patient Information First Name: ALEJANDRO Last Name: ROSALEZ Date of Birth: 10/10/1982 Sex: M Marital Status: MARRIED Email Address: Address: 123 ANY STREET City: ANYTOWN State: CA Zip Code: 12345 Phone: 646-555-0111 Emergency Contact 1: First Name: CARLOS Last Name: SALAZAR Phone: 212-555-0150 Relationship to Patient: BROTHER Emergency Contact 2: First Name: JANE Last Name: DOE Phone: 650-555-0123 Relationship FRIEND to Patient: Did you feel fever or feverish lately? Yes No Are you having shortness of breath? Yes No Do you have a cough? Yes No Did you experience loss of taste or smell? Yes No Where you in contact with any confirmed COVID-19 positive patients? Yes No Did you travel in the past 14 days to any regions affected by COVID-19? Yes No Patient Information First Name: ALEJANDRO Last Name: ROSALEZ Date of Birth: 10/10/1982 Sex: M Marital Status: MARRIED Email Address: Address: 123 ANY STREET City: ANYTOWN State: CA Zip Code: 12345 Phone: 646-555-0111 Emergency Contact 1: First Name: CARLOS Last Name: SALAZAR Phone: 212-555-0150 Relationship to Patient: BROTHER Emergency Contact 2: First Name: JANE Last Name: DOE Phone: 650-555-0123 Relationship FRIEND to Patient: Did you feel fever or feverish lately? Yes No Are you having shortness of breath? Yes No Do you have a cough? Yes No Did you experience loss of taste or smell? Yes No Where you in contact with any confirmed COVID-19 positive patients? Yes No Did you travel in the past 14 days to any regions affected by COVID-19? Yes No ', metadata={'source': 'example_data/alejandro_rosalez_sample-small.jpeg', 'page': 1})] Sample 2 ​ The next sample loads a file from an HTTPS endpoint. It has to be single\npage, as Amazon Textract requires all multi-page documents to be stored\non S3. from langchain . document_loaders import AmazonTextractPDFLoader loader = AmazonTextractPDFLoader ( \"https://amazon-textract-public-content.s3.us-east-2.amazonaws.com/langchain/alejandro_rosalez_sample_1.jpg\" ) documents = loader . load ( ) documents [Document(page_content='Patient Information First Name: ALEJANDRO Last Name: ROSALEZ Date of Birth: 10/10/1982 Sex: M Marital Status: MARRIED Email Address: Address: 123 ANY STREET City: ANYTOWN State: CA Zip Code: 12345 Phone: 646-555-0111 Emergency Contact 1: First Name: CARLOS Last Name: SALAZAR Phone: 212-555-0150 Relationship to Patient: BROTHER Emergency Contact 2: First Name: JANE Last Name: DOE Phone: 650-555-0123 Relationship FRIEND to Patient: Did you feel fever or feverish lately? Yes No Are you having shortness of breath? Yes No Do you have a cough? Yes No Did you experience loss of taste or smell? Yes No Where you in contact with any confirmed COVID-19 positive patients? Yes No Did you travel in the past 14 days to any regions affected by COVID-19? Yes No Patient Information First Name: ALEJANDRO Last Name: ROSALEZ Date of Birth: 10/10/1982 Sex: M Marital Status: MARRIED Email Address: Address: 123 ANY STREET City: ANYTOWN State: CA Zip Code: 12345 Phone: 646-555-0111 Emergency Contact 1: First Name: CARLOS Last Name: SALAZAR Phone: 212-555-0150 Relationship to Patient: BROTHER Emergency Contact 2: First Name: JANE Last Name: DOE Phone: 650-555-0123 Relationship FRIEND to Patient: Did you feel fever or feverish lately? Yes No Are you having shortness of breath? Yes No Do you have a cough? Yes No Did you experience loss of taste or smell? Yes No Where you in contact with any confirmed COVID-19 positive patients? Yes No Did you travel in the past 14 days to any regions affected by COVID-19? Yes No ', metadata={'source': 'example_data/alejandro_rosalez_sample-small.jpeg', 'page': 1})] Sample 3 ​ Processing a multi-page document requires the document to be on S3. The\nsample document resides in a bucket in us-east-2 and Textract needs to\nbe called in that same region to be successful, so we set the\nregion_name on the client and pass that in to the loader to ensure\nTextract is called from us-east-2. You could also to have your notebook\nrunning in us-east-2, setting the AWS_DEFAULT_REGION set to us-east-2 or\nwhen running in a different environment, pass in a boto3 Textract client\nwith that region name like in the cell below. import boto3 textract_client = boto3 . client ( \"textract\" , region_name = \"us-east-2\" ) file_path = \"s3://amazon-textract-public-content/langchain/layout-parser-paper.pdf\" loader = AmazonTextractPDFLoader ( file_path , client = textract_client ) documents = loader . load ( ) Now getting the number of pages to validate the response (printing out\nthe full response would be quite long…). We expect 16 pages. len ( documents ) 16 Using the AmazonTextractPDFLoader in an LangChain chain (e. g. OpenAI) ​ The AmazonTextractPDFLoader can be used in a chain the same way the\nother loaders are used. Textract itself does have a Query\nfeature ,\nwhich offers similar functionality to the QA chain in this sample, which\nis worth checking out as well. # You can store your OPENAI_API_KEY in a .env file as well # import os # from dotenv import load_dotenv # load_dotenv() # Or set the OpenAI key in the environment directly import os os . environ [ \"OPENAI_API_KEY\" ] = \"your-OpenAI-API-key\" from langchain . chains . question_answering import load_qa_chain from langchain . llms import OpenAI chain = load_qa_chain ( llm = OpenAI ( ) , chain_type = \"map_reduce\" ) query = [ \"Who are the autors?\" ] chain . run ( input_documents = documents , question = query ) ' The authors are Zejiang Shen, Ruochen Zhang, Melissa Dell, Benjamin Charles Germain Lee, Jacob Carlson, Weining Li, Gardner, M., Grus, J., Neumann, M., Tafjord, O., Dasigi, P., Liu, N., Peters, M., Schmitz, M., Zettlemoyer, L., Lukasz Garncarek, Powalski, R., Stanislawek, T., Topolski, B., Halama, P., Gralinski, F., Graves, A., Fernández, S., Gomez, F., Schmidhuber, J., Harley, A.W., Ufkes, A., Derpanis, K.G., He, K., Gkioxari, G., Dollár, P., Girshick, R., He, K., Zhang, X., Ren, S., Sun, J., Kay, A., Lamiroy, B., Lopresti, D., Mears, J., Jakeway, E., Ferriter, M., Adams, C., Yarasavage, N., Thomas, D., Zwaard, K., Li, M., Cui, L., Huang,' Previous Alibaba Cloud MaxCompute Next Apify Dataset"
    },
    {
        "title": "Apify Dataset | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/apify_dataset",
        "text_content": "Components Document loaders Apify Dataset On this page Apify Dataset Apify Dataset is a\nscalable append-only storage with sequential access built for storing\nstructured web scraping results, such as a list of products or Google\nSERPs, and then export them to various formats like JSON, CSV, or\nExcel. Datasets are mainly used to save results of Apify\nActors —serverless cloud programs for various\nweb scraping, crawling, and data extraction use cases. This notebook shows how to load Apify datasets to LangChain. Prerequisites ​ You need to have an existing dataset on the Apify platform. If you don’t\nhave one, please first check out this\nnotebook on how to use Apify to extract\ncontent from documentation, knowledge bases, help centers, or blogs. #!pip install apify-client First, import ApifyDatasetLoader into your source code: from langchain . document_loaders import ApifyDatasetLoader from langchain . document_loaders . base import Document Then provide a function that maps Apify dataset record fields to\nLangChain Document format. For example, if your dataset items are structured like this: { \"url\" : \"https://apify.com\" , \"text\" : \"Apify is the best web scraping and automation platform.\" } The mapping function in the code below will convert them to LangChain Document format, so that you can use them further with any LLM model\n(e.g. for question answering). loader = ApifyDatasetLoader ( dataset_id = \"your-dataset-id\" , dataset_mapping_function = lambda dataset_item : Document ( page_content = dataset_item [ \"text\" ] , metadata = { \"source\" : dataset_item [ \"url\" ] } ) , ) data = loader . load ( ) An example with question answering ​ In this example, we use data from a dataset to answer a question. from langchain . docstore . document import Document from langchain . document_loaders import ApifyDatasetLoader from langchain . indexes import VectorstoreIndexCreator loader = ApifyDatasetLoader ( dataset_id = \"your-dataset-id\" , dataset_mapping_function = lambda item : Document ( page_content = item [ \"text\" ] or \"\" , metadata = { \"source\" : item [ \"url\" ] } ) , ) index = VectorstoreIndexCreator ( ) . from_loaders ( [ loader ] ) query = \"What is Apify?\" result = index . query_with_sources ( query ) print ( result [ \"answer\" ] ) print ( result [ \"sources\" ] ) Apify is a platform for developing, running, and sharing serverless cloud programs. It enables users to create web scraping and automation tools and publish them on the Apify platform. https://docs.apify.com/platform/actors, https://docs.apify.com/platform/actors/running/actors-in-store, https://docs.apify.com/platform/security, https://docs.apify.com/platform/actors/examples Previous Amazon Textract Next ArcGIS"
    },
    {
        "title": "ArcGIS | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/arcgis",
        "text_content": "Components Document loaders ArcGIS On this page ArcGIS This notebook demonstrates the use of the langchain.document_loaders.ArcGISLoader class. You will need to install the ArcGIS API for Python arcgis and,\noptionally, bs4.BeautifulSoup . You can use an arcgis.gis.GIS object for authenticated data loading,\nor leave it blank to access public data. from langchain . document_loaders import ArcGISLoader URL = \"https://maps1.vcgov.org/arcgis/rest/services/Beaches/MapServer/7\" loader = ArcGISLoader ( URL ) docs = loader . load ( ) Let’s measure loader latency. % % time docs = loader . load ( ) CPU times: user 2.37 ms, sys: 5.83 ms, total: 8.19 ms Wall time: 1.05 s docs [ 0 ] . metadata {'accessed': '2023-09-13T19:58:32.546576+00:00Z', 'name': 'Beach Ramps', 'url': 'https://maps1.vcgov.org/arcgis/rest/services/Beaches/MapServer/7', 'layer_description': '(Not Provided)', 'item_description': '(Not Provided)', 'layer_properties': { \"currentVersion\": 10.81, \"id\": 7, \"name\": \"Beach Ramps\", \"type\": \"Feature Layer\", \"description\": \"\", \"geometryType\": \"esriGeometryPoint\", \"sourceSpatialReference\": { \"wkid\": 2881, \"latestWkid\": 2881 }, \"copyrightText\": \"\", \"parentLayer\": null, \"subLayers\": [], \"minScale\": 750000, \"maxScale\": 0, \"drawingInfo\": { \"renderer\": { \"type\": \"simple\", \"symbol\": { \"type\": \"esriPMS\", \"url\": \"9bb2e5ca499bb68aa3ee0d4e1ecc3849\", \"imageData\": \"iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAAXNSR0IB2cksfwAAAAlwSFlzAAAOxAAADsQBlSsOGwAAAJJJREFUOI3NkDEKg0AQRZ9kkSnSGBshR7DJqdJYeg7BMpcS0uQWQsqoCLExkcUJzGqT38zw2fcY1rEzbp7vjXz0EXC7gBxs1ABcG/8CYkCcDqwyLqsV+RlV0I/w7PzuJBArr1VB20H58Ls6h+xoFITkTwWpQJX7XSIBAnFwVj7MLAjJV/AC6G3QoAmK+74Lom04THTBEp/HCSc6AAAAAElFTkSuQmCC\", \"contentType\": \"image/png\", \"width\": 12, \"height\": 12, \"angle\": 0, \"xoffset\": 0, \"yoffset\": 0 }, \"label\": \"\", \"description\": \"\" }, \"transparency\": 0, \"labelingInfo\": null }, \"defaultVisibility\": true, \"extent\": { \"xmin\": -81.09480168806815, \"ymin\": 28.858349245353473, \"xmax\": -80.77512908572814, \"ymax\": 29.41078388840041, \"spatialReference\": { \"wkid\": 4326, \"latestWkid\": 4326 } }, \"hasAttachments\": false, \"htmlPopupType\": \"esriServerHTMLPopupTypeNone\", \"displayField\": \"AccessName\", \"typeIdField\": null, \"subtypeFieldName\": null, \"subtypeField\": null, \"defaultSubtypeCode\": null, \"fields\": [ { \"name\": \"OBJECTID\", \"type\": \"esriFieldTypeOID\", \"alias\": \"OBJECTID\", \"domain\": null }, { \"name\": \"Shape\", \"type\": \"esriFieldTypeGeometry\", \"alias\": \"Shape\", \"domain\": null }, { \"name\": \"AccessName\", \"type\": \"esriFieldTypeString\", \"alias\": \"AccessName\", \"length\": 40, \"domain\": null }, { \"name\": \"AccessID\", \"type\": \"esriFieldTypeString\", \"alias\": \"AccessID\", \"length\": 50, \"domain\": null }, { \"name\": \"AccessType\", \"type\": \"esriFieldTypeString\", \"alias\": \"AccessType\", \"length\": 25, \"domain\": null }, { \"name\": \"GeneralLoc\", \"type\": \"esriFieldTypeString\", \"alias\": \"GeneralLoc\", \"length\": 100, \"domain\": null }, { \"name\": \"MilePost\", \"type\": \"esriFieldTypeDouble\", \"alias\": \"MilePost\", \"domain\": null }, { \"name\": \"City\", \"type\": \"esriFieldTypeString\", \"alias\": \"City\", \"length\": 50, \"domain\": null }, { \"name\": \"AccessStatus\", \"type\": \"esriFieldTypeString\", \"alias\": \"AccessStatus\", \"length\": 50, \"domain\": null }, { \"name\": \"Entry_Date_Time\", \"type\": \"esriFieldTypeDate\", \"alias\": \"Entry_Date_Time\", \"length\": 8, \"domain\": null }, { \"name\": \"DrivingZone\", \"type\": \"esriFieldTypeString\", \"alias\": \"DrivingZone\", \"length\": 50, \"domain\": null } ], \"geometryField\": { \"name\": \"Shape\", \"type\": \"esriFieldTypeGeometry\", \"alias\": \"Shape\" }, \"indexes\": null, \"subtypes\": [], \"relationships\": [], \"canModifyLayer\": true, \"canScaleSymbols\": false, \"hasLabels\": false, \"capabilities\": \"Map,Query,Data\", \"maxRecordCount\": 1000, \"supportsStatistics\": true, \"supportsAdvancedQueries\": true, \"supportedQueryFormats\": \"JSON, geoJSON\", \"isDataVersioned\": false, \"ownershipBasedAccessControlForFeatures\": { \"allowOthersToQuery\": true }, \"useStandardizedQueries\": true, \"advancedQueryCapabilities\": { \"useStandardizedQueries\": true, \"supportsStatistics\": true, \"supportsHavingClause\": true, \"supportsCountDistinct\": true, \"supportsOrderBy\": true, \"supportsDistinct\": true, \"supportsPagination\": true, \"supportsTrueCurve\": true, \"supportsReturningQueryExtent\": true, \"supportsQueryWithDistance\": true, \"supportsSqlExpression\": true }, \"supportsDatumTransformation\": true, \"dateFieldsTimeReference\": null, \"supportsCoordinatesQuantization\": true }} Retrieving Geometries ​ If you want to retrieve feature geometries, you may do so with the return_geometry keyword. Each document’s geometry will be stored in its metadata dictionary. loader_geom = ArcGISLoader ( URL , return_geometry = True ) % % time docs = loader_geom . load ( ) CPU times: user 9.6 ms, sys: 5.84 ms, total: 15.4 ms Wall time: 1.06 s docs [ 0 ] . metadata [ \"geometry\" ] {'x': -81.01508803280349, 'y': 29.24246579525828, 'spatialReference': {'wkid': 4326, 'latestWkid': 4326}} for doc in docs : print ( doc . page_content ) {\"OBJECTID\": 4, \"AccessName\": \"UNIVERSITY BLVD\", \"AccessID\": \"DB-048\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"900 BLK N ATLANTIC AV\", \"MilePost\": 13.74, \"City\": \"DAYTONA BEACH\", \"AccessStatus\": \"OPEN\", \"Entry_Date_Time\": 1694597536000, \"DrivingZone\": \"BOTH\"} {\"OBJECTID\": 18, \"AccessName\": \"BEACHWAY AV\", \"AccessID\": \"NS-106\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"1400 N ATLANTIC AV\", \"MilePost\": 1.57, \"City\": \"NEW SMYRNA BEACH\", \"AccessStatus\": \"OPEN\", \"Entry_Date_Time\": 1694600478000, \"DrivingZone\": \"YES\"} {\"OBJECTID\": 24, \"AccessName\": \"27TH AV\", \"AccessID\": \"NS-141\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"3600 BLK S ATLANTIC AV\", \"MilePost\": 4.83, \"City\": \"NEW SMYRNA BEACH\", \"AccessStatus\": \"CLOSED FOR HIGH TIDE\", \"Entry_Date_Time\": 1694619363000, \"DrivingZone\": \"BOTH\"} {\"OBJECTID\": 26, \"AccessName\": \"SEABREEZE BLVD\", \"AccessID\": \"DB-051\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"500 BLK N ATLANTIC AV\", \"MilePost\": 14.24, \"City\": \"DAYTONA BEACH\", \"AccessStatus\": \"OPEN\", \"Entry_Date_Time\": 1694597536000, \"DrivingZone\": \"BOTH\"} {\"OBJECTID\": 30, \"AccessName\": \"INTERNATIONAL SPEEDWAY BLVD\", \"AccessID\": \"DB-059\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"300 BLK S ATLANTIC AV\", \"MilePost\": 15.27, \"City\": \"DAYTONA BEACH\", \"AccessStatus\": \"OPEN\", \"Entry_Date_Time\": 1694598638000, \"DrivingZone\": \"BOTH\"} {\"OBJECTID\": 33, \"AccessName\": \"GRANADA BLVD\", \"AccessID\": \"OB-030\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"20 BLK OCEAN SHORE BLVD\", \"MilePost\": 10.02, \"City\": \"ORMOND BEACH\", \"AccessStatus\": \"4X4 ONLY\", \"Entry_Date_Time\": 1694595424000, \"DrivingZone\": \"BOTH\"} {\"OBJECTID\": 39, \"AccessName\": \"BEACH ST\", \"AccessID\": \"PI-097\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"4890 BLK S ATLANTIC AV\", \"MilePost\": 25.85, \"City\": \"PONCE INLET\", \"AccessStatus\": \"4X4 ONLY\", \"Entry_Date_Time\": 1694596294000, \"DrivingZone\": \"BOTH\"} {\"OBJECTID\": 44, \"AccessName\": \"SILVER BEACH AV\", \"AccessID\": \"DB-064\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"1000 BLK S ATLANTIC AV\", \"MilePost\": 15.98, \"City\": \"DAYTONA BEACH\", \"AccessStatus\": \"OPEN\", \"Entry_Date_Time\": 1694598638000, \"DrivingZone\": \"YES\"} {\"OBJECTID\": 45, \"AccessName\": \"BOTEFUHR AV\", \"AccessID\": \"DBS-067\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"1900 BLK S ATLANTIC AV\", \"MilePost\": 16.68, \"City\": \"DAYTONA BEACH SHORES\", \"AccessStatus\": \"OPEN\", \"Entry_Date_Time\": 1694598638000, \"DrivingZone\": \"YES\"} {\"OBJECTID\": 46, \"AccessName\": \"MINERVA RD\", \"AccessID\": \"DBS-069\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"2300 BLK S ATLANTIC AV\", \"MilePost\": 17.52, \"City\": \"DAYTONA BEACH SHORES\", \"AccessStatus\": \"OPEN\", \"Entry_Date_Time\": 1694598638000, \"DrivingZone\": \"YES\"} {\"OBJECTID\": 56, \"AccessName\": \"3RD AV\", \"AccessID\": \"NS-118\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"1200 BLK HILL ST\", \"MilePost\": 3.25, \"City\": \"NEW SMYRNA BEACH\", \"AccessStatus\": \"OPEN\", \"Entry_Date_Time\": 1694600478000, \"DrivingZone\": \"YES\"} {\"OBJECTID\": 65, \"AccessName\": \"MILSAP RD\", \"AccessID\": \"OB-037\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"700 BLK S ATLANTIC AV\", \"MilePost\": 11.52, \"City\": \"ORMOND BEACH\", \"AccessStatus\": \"4X4 ONLY\", \"Entry_Date_Time\": 1694595749000, \"DrivingZone\": \"YES\"} {\"OBJECTID\": 72, \"AccessName\": \"ROCKEFELLER DR\", \"AccessID\": \"OB-034\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"400 BLK S ATLANTIC AV\", \"MilePost\": 10.9, \"City\": \"ORMOND BEACH\", \"AccessStatus\": \"CLOSED - SEASONAL\", \"Entry_Date_Time\": 1694591351000, \"DrivingZone\": \"YES\"} {\"OBJECTID\": 74, \"AccessName\": \"DUNLAWTON BLVD\", \"AccessID\": \"DBS-078\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"3400 BLK S ATLANTIC AV\", \"MilePost\": 20.61, \"City\": \"DAYTONA BEACH SHORES\", \"AccessStatus\": \"OPEN\", \"Entry_Date_Time\": 1694601124000, \"DrivingZone\": \"YES\"} {\"OBJECTID\": 77, \"AccessName\": \"EMILIA AV\", \"AccessID\": \"DBS-082\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"3790 BLK S ATLANTIC AV\", \"MilePost\": 21.38, \"City\": \"DAYTONA BEACH SHORES\", \"AccessStatus\": \"OPEN\", \"Entry_Date_Time\": 1694601124000, \"DrivingZone\": \"BOTH\"} {\"OBJECTID\": 84, \"AccessName\": \"VAN AV\", \"AccessID\": \"DBS-075\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"3100 BLK S ATLANTIC AV\", \"MilePost\": 19.6, \"City\": \"DAYTONA BEACH SHORES\", \"AccessStatus\": \"OPEN\", \"Entry_Date_Time\": 1694601124000, \"DrivingZone\": \"YES\"} {\"OBJECTID\": 104, \"AccessName\": \"HARVARD DR\", \"AccessID\": \"OB-038\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"900 BLK S ATLANTIC AV\", \"MilePost\": 11.72, \"City\": \"ORMOND BEACH\", \"AccessStatus\": \"OPEN\", \"Entry_Date_Time\": 1694597536000, \"DrivingZone\": \"YES\"} {\"OBJECTID\": 106, \"AccessName\": \"WILLIAMS AV\", \"AccessID\": \"DB-042\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"2200 BLK N ATLANTIC AV\", \"MilePost\": 12.5, \"City\": \"DAYTONA BEACH\", \"AccessStatus\": \"OPEN\", \"Entry_Date_Time\": 1694597536000, \"DrivingZone\": \"YES\"} {\"OBJECTID\": 109, \"AccessName\": \"HARTFORD AV\", \"AccessID\": \"DB-043\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"1890 BLK N ATLANTIC AV\", \"MilePost\": 12.76, \"City\": \"DAYTONA BEACH\", \"AccessStatus\": \"CLOSED - SEASONAL\", \"Entry_Date_Time\": 1694591351000, \"DrivingZone\": \"YES\"} {\"OBJECTID\": 138, \"AccessName\": \"CRAWFORD RD\", \"AccessID\": \"NS-108\", \"AccessType\": \"OPEN VEHICLE RAMP - PASS\", \"GeneralLoc\": \"800 BLK N ATLANTIC AV\", \"MilePost\": 2.19, \"City\": \"NEW SMYRNA BEACH\", \"AccessStatus\": \"OPEN\", \"Entry_Date_Time\": 1694600478000, \"DrivingZone\": \"YES\"} {\"OBJECTID\": 140, \"AccessName\": \"FLAGLER AV\", \"AccessID\": \"NS-110\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"500 BLK FLAGLER AV\", \"MilePost\": 2.57, \"City\": \"NEW SMYRNA BEACH\", \"AccessStatus\": \"OPEN\", \"Entry_Date_Time\": 1694600478000, \"DrivingZone\": \"YES\"} {\"OBJECTID\": 144, \"AccessName\": \"CARDINAL DR\", \"AccessID\": \"OB-036\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"600 BLK S ATLANTIC AV\", \"MilePost\": 11.27, \"City\": \"ORMOND BEACH\", \"AccessStatus\": \"4X4 ONLY\", \"Entry_Date_Time\": 1694595749000, \"DrivingZone\": \"YES\"} {\"OBJECTID\": 174, \"AccessName\": \"EL PORTAL ST\", \"AccessID\": \"DBS-076\", \"AccessType\": \"OPEN VEHICLE RAMP\", \"GeneralLoc\": \"3200 BLK S ATLANTIC AV\", \"MilePost\": 20.04, \"City\": \"DAYTONA BEACH SHORES\", \"AccessStatus\": \"OPEN\", \"Entry_Date_Time\": 1694601124000, \"DrivingZone\": \"YES\"} Previous Apify Dataset Next Arxiv"
    },
    {
        "title": "Arxiv | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/arxiv",
        "text_content": "Components Document loaders Arxiv On this page Arxiv arXiv is an open-access archive for 2 million\nscholarly articles in the fields of physics, mathematics, computer\nscience, quantitative biology, quantitative finance, statistics,\nelectrical engineering and systems science, and economics. This notebook shows how to load scientific articles from Arxiv.org into a document format that we can use downstream. Installation ​ First, you need to install arxiv python package. #!pip install arxiv Second, you need to install PyMuPDF python package which transforms\nPDF files downloaded from the arxiv.org site into the text format. #!pip install pymupdf Examples ​ ArxivLoader has these arguments: - query : free text which used to\nfind documents in the Arxiv - optional load_max_docs : default=100. Use\nit to limit number of downloaded documents. It takes time to download\nall 100 documents, so use a small number for experiments. - optional load_all_available_meta : default=False. By default only the most\nimportant fields downloaded: Published (date when document was\npublished/last updated), Title , Authors , Summary . If True, other\nfields also downloaded. from langchain . document_loaders import ArxivLoader docs = ArxivLoader ( query = \"1605.08386\" , load_max_docs = 2 ) . load ( ) len ( docs ) docs [ 0 ] . metadata # meta-information of the Document {'Published': '2016-05-26', 'Title': 'Heat-bath random walks with Markov bases', 'Authors': 'Caprice Stanley, Tobias Windisch', 'Summary': 'Graphs on lattice points are studied whose edges come from a finite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on\\nfibers of a fixed integer matrix can be bounded from above by a constant. We\\nthen study the mixing behaviour of heat-bath random walks on these graphs. We\\nalso state explicit conditions on the set of moves so that the heat-bath random\\nwalk, a generalization of the Glauber dynamics, is an expander in fixed\\ndimension.'} docs [ 0 ] . page_content [ : 400 ] # all pages of the Document content 'arXiv:1605.08386v1  [math.CO]  26 May 2016\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nAbstract. Graphs on lattice points are studied whose edges come from a ﬁnite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on ﬁbers of a\\nﬁxed integer matrix can be bounded from above by a constant. We then study the mixing\\nbehaviour of heat-b' Previous ArcGIS Next AssemblyAI Audio Transcripts"
    },
    {
        "title": "AssemblyAI Audio Transcripts | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/assemblyai",
        "text_content": "Components Document loaders AssemblyAI Audio Transcripts On this page AssemblyAI Audio Transcripts The AssemblyAIAudioTranscriptLoader allows to transcribe audio files\nwith the AssemblyAI API and loads the\ntranscribed text into documents. To use it, you should have the assemblyai python package installed,\nand the environment variable ASSEMBLYAI_API_KEY set with your API key.\nAlternatively, the API key can also be passed as an argument. More info about AssemblyAI: Website Get a Free API key AssemblyAI API Docs Installation ​ First, you need to install the assemblyai python package. You can find more info about it inside the assemblyai-python-sdk GitHub\nrepo . #!pip install assemblyai Example ​ The AssemblyAIAudioTranscriptLoader needs at least the file_path argument. Audio files can be specified as an URL or a local file path. from langchain . document_loaders import AssemblyAIAudioTranscriptLoader audio_file = \"https://storage.googleapis.com/aai-docs-samples/nbc.mp3\" # or a local file path: audio_file = \"./nbc.mp3\" loader = AssemblyAIAudioTranscriptLoader ( file_path = audio_file ) docs = loader . load ( ) Note: Calling loader.load() blocks until the transcription is\nfinished. The transcribed text is available in the page_content : docs [ 0 ] . page_content \"Load time, a new president and new congressional makeup. Same old ...\" The metadata contains the full JSON response with more meta\ninformation: docs [ 0 ] . metadata {'language_code': <LanguageCode.en_us: 'en_us'>, 'audio_url': 'https://storage.googleapis.com/aai-docs-samples/nbc.mp3', 'punctuate': True, 'format_text': True, ... } Transcript Formats ​ You can specify the transcript_format argument for different formats. Depending on the format, one or more documents are returned. These are\nthe different TranscriptFormat options: TEXT : One document with the transcription text SENTENCES : Multiple documents, splits the transcription by each\nsentence PARAGRAPHS : Multiple documents, splits the transcription by each\nparagraph SUBTITLES_SRT : One document with the transcript exported in SRT\nsubtitles format SUBTITLES_VTT : One document with the transcript exported in VTT\nsubtitles format from langchain . document_loaders . assemblyai import TranscriptFormat loader = AssemblyAIAudioTranscriptLoader ( file_path = \"./your_file.mp3\" , transcript_format = TranscriptFormat . SENTENCES , ) docs = loader . load ( ) Transcription Config ​ You can also specify the config argument to use different audio\nintelligence models. Visit the AssemblyAI API\nDocumentation to get an overview of\nall available models! import assemblyai as aai config = aai . TranscriptionConfig ( speaker_labels = True , auto_chapters = True , entity_detection = True ) loader = AssemblyAIAudioTranscriptLoader ( file_path = \"./your_file.mp3\" , config = config ) Pass the API Key as argument ​ Next to setting the API key as environment variable ASSEMBLYAI_API_KEY , it is also possible to pass it as argument. loader = AssemblyAIAudioTranscriptLoader ( file_path = \"./your_file.mp3\" , api_key = \"YOUR_KEY\" ) Previous Arxiv Next Async Chromium"
    },
    {
        "title": "Async Chromium | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/async_chromium",
        "text_content": "Components Document loaders Async Chromium Async Chromium Chromium is one of the browsers supported by Playwright, a library used\nto control browser automation. By running p.chromium.launch(headless=True) , we are launching a\nheadless instance of Chromium. Headless mode means that the browser is running without a graphical user\ninterface. AsyncChromiumLoader load the page, and then we use Html2TextTransformer to trasnform to text. ! pip install - q playwright beautifulsoup4 ! playwright install from langchain . document_loaders import AsyncChromiumLoader urls = [ \"https://www.wsj.com\" ] loader = AsyncChromiumLoader ( urls ) docs = loader . load ( ) docs [ 0 ] . page_content [ 0 : 100 ] '<!DOCTYPE html><html lang=\"en\"><head><script src=\"https://s0.2mdn.net/instream/video/client.js\" asyn' from langchain . document_transformers import Html2TextTransformer html2text = Html2TextTransformer ( ) docs_transformed = html2text . transform_documents ( docs ) docs_transformed [ 0 ] . page_content [ 0 : 500 ] \"Skip to Main ContentSkip to SearchSkip to... Select * Top News * What's News *\\nFeatured Stories * Retirement * Life & Arts * Hip-Hop * Sports * Video *\\nEconomy * Real Estate * Sports * CMO * CIO * CFO * Risk & Compliance *\\nLogistics Report * Sustainable Business * Heard on the Street * Barron’s *\\nMarketWatch * Mansion Global * Penta * Opinion * Journal Reports * Sponsored\\nOffers Explore Our Brands * WSJ * * * * * Barron's * * * * * MarketWatch * * *\\n* * IBD # The Wall Street Journal SubscribeSig\" Previous AssemblyAI Audio Transcripts Next AsyncHtml"
    },
    {
        "title": "AsyncHtml | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/async_html",
        "text_content": "Components Document loaders AsyncHtml AsyncHtml AsyncHtmlLoader loads raw HTML from a list of URLs concurrently. from langchain . document_loaders import AsyncHtmlLoader urls = [ \"https://www.espn.com\" , \"https://lilianweng.github.io/posts/2023-06-23-agent/\" ] loader = AsyncHtmlLoader ( urls ) docs = loader . load ( ) Fetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s] docs [ 0 ] . page_content [ 1000 : 2000 ] ' news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta property=\"og:image\" content=\"https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png\"/>\\n<meta property=\"og:image:width\" content=\"1200\" />\\n<meta property=\"og:image:height\" content=\"630\" />\\n<meta property=\"og:type\" content=\"website\" />\\n<meta name=\"twitter:site\" content=\"espn\" />\\n<meta name=\"twitter:url\" content=\"https://www.espn.com\" />\\n<meta name=\"twitter:title\" content=\"ESPN - Serving Sports Fans. Anytime. Anywhere.\"/>\\n<meta name=\"twitter:description\" content=\"Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta name=\"twitter:card\" content=\"summary\">\\n<meta name=\"twitter:app:name:iphone\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:iphone\" content=\"317469184\"/>\\n<meta name=\"twitter:app:name:googleplay\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:googleplay\" content=\"com.espn.score_center\"/>\\n<meta name=\"title\" content=\"ESPN - ' docs [ 1 ] . page_content [ 1000 : 2000 ] 'al\" href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" />\\n<link crossorigin=\"anonymous\" href=\"/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css\" integrity=\"sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=\" rel=\"preload stylesheet\" as=\"style\">\\n<script defer crossorigin=\"anonymous\" src=\"/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js\" integrity=\"sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=\"\\n    onload=\"hljs.initHighlightingOnLoad();\"></script>\\n<link rel=\"icon\" href=\"https://lilianweng.github.io/favicon_peach.ico\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://lilianweng.github.io/favicon-16x16.png\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://lilianweng.github.io/favicon-32x32.png\">\\n<link rel=\"apple-touch-icon\" href=\"https://lilianweng.github.io/apple-touch-icon.png\">\\n<link rel=\"mask-icon\" href=\"https://lilianweng.github.io/safari-pinned-tab.' Previous Async Chromium Next AWS S3 Directory"
    },
    {
        "title": "AWS S3 Directory | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/aws_s3_directory",
        "text_content": "Components Document loaders AWS S3 Directory On this page AWS S3 Directory Amazon Simple Storage Service (Amazon\nS3) is an object storage service AWS S3\nDirectory This covers how to load document objects from an AWS S3 Directory object. #!pip install boto3 from langchain . document_loaders import S3DirectoryLoader loader = S3DirectoryLoader ( \"testing-hwc\" ) loader . load ( ) Specifying a prefix ​ You can also specify a prefix for more finegrained control over what\nfiles to load. loader = S3DirectoryLoader ( \"testing-hwc\" , prefix = \"fake\" ) loader . load ( ) [Document(page_content='Lorem ipsum dolor sit amet.', lookup_str='', metadata={'source': 's3://testing-hwc/fake.docx'}, lookup_index=0)] Configuring the AWS Boto3 client ​ You can configure the AWS Boto3 client by passing named arguments when creating the S3DirectoryLoader.\nThis is useful for instance when AWS credentials can’t be set as\nenvironment variables. See the list of\nparameters that can be configured. loader = S3DirectoryLoader ( \"testing-hwc\" , aws_access_key_id = \"xxxx\" , aws_secret_access_key = \"yyyy\" ) loader . load ( ) Previous AsyncHtml Next AWS S3 File"
    },
    {
        "title": "AWS S3 File | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/aws_s3_file",
        "text_content": "Components Document loaders AWS S3 File On this page AWS S3 File Amazon Simple Storage Service (Amazon\nS3) is an object storage service. AWS S3\nBuckets This covers how to load document objects from an AWS S3 File object. from langchain . document_loaders import S3FileLoader #!pip install boto3 loader = S3FileLoader ( \"testing-hwc\" , \"fake.docx\" ) loader . load ( ) [Document(page_content='Lorem ipsum dolor sit amet.', lookup_str='', metadata={'source': 's3://testing-hwc/fake.docx'}, lookup_index=0)] Configuring the AWS Boto3 client ​ You can configure the AWS Boto3 client by passing named arguments when creating the S3DirectoryLoader.\nThis is useful for instance when AWS credentials can’t be set as\nenvironment variables. See the list of\nparameters that can be configured. loader = S3FileLoader ( \"testing-hwc\" , \"fake.docx\" , aws_access_key_id = \"xxxx\" , aws_secret_access_key = \"yyyy\" ) loader . load ( ) Previous AWS S3 Directory Next AZLyrics"
    },
    {
        "title": "AZLyrics | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/azlyrics",
        "text_content": "Components Document loaders AZLyrics AZLyrics AZLyrics is a large, legal, every day\ngrowing collection of lyrics. This covers how to load AZLyrics webpages into a document format that we\ncan use downstream. from langchain . document_loaders import AZLyricsLoader loader = AZLyricsLoader ( \"https://www.azlyrics.com/lyrics/mileycyrus/flowers.html\" ) data = loader . load ( ) data [Document(page_content=\"Miley Cyrus - Flowers Lyrics | AZLyrics.com\\n\\r\\nWe were good, we were gold\\nKinda dream that can't be sold\\nWe were right till we weren't\\nBuilt a home and watched it burn\\n\\nI didn't wanna leave you\\nI didn't wanna lie\\nStarted to cry but then remembered I\\n\\nI can buy myself flowers\\nWrite my name in the sand\\nTalk to myself for hours\\nSay things you don't understand\\nI can take myself dancing\\nAnd I can hold my own hand\\nYeah, I can love me better than you can\\n\\nCan love me better\\nI can love me better, baby\\nCan love me better\\nI can love me better, baby\\n\\nPaint my nails, cherry red\\nMatch the roses that you left\\nNo remorse, no regret\\nI forgive every word you said\\n\\nI didn't wanna leave you, baby\\nI didn't wanna fight\\nStarted to cry but then remembered I\\n\\nI can buy myself flowers\\nWrite my name in the sand\\nTalk to myself for hours, yeah\\nSay things you don't understand\\nI can take myself dancing\\nAnd I can hold my own hand\\nYeah, I can love me better than you can\\n\\nCan love me better\\nI can love me better, baby\\nCan love me better\\nI can love me better, baby\\nCan love me better\\nI can love me better, baby\\nCan love me better\\nI\\n\\nI didn't wanna wanna leave you\\nI didn't wanna fight\\nStarted to cry but then remembered I\\n\\nI can buy myself flowers\\nWrite my name in the sand\\nTalk to myself for hours (Yeah)\\nSay things you don't understand\\nI can take myself dancing\\nAnd I can hold my own hand\\nYeah, I can love me better than\\nYeah, I can love me better than you can, uh\\n\\nCan love me better\\nI can love me better, baby\\nCan love me better\\nI can love me better, baby (Than you can)\\nCan love me better\\nI can love me better, baby\\nCan love me better\\nI\\n\", lookup_str='', metadata={'source': 'https://www.azlyrics.com/lyrics/mileycyrus/flowers.html'}, lookup_index=0)] Previous AWS S3 File Next Azure AI Data"
    },
    {
        "title": "Azure AI Data | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/azure_ai_data",
        "text_content": "Components Document loaders Azure AI Data On this page Azure AI Data Azure AI Studio provides the capability to\nupload data assets to cloud storage and register existing data assets\nfrom the following sources: Microsoft OneLake Azure Blob Storage Azure Data Lake gen 2 The benefit of this approach over AzureBlobStorageContainerLoader and AzureBlobStorageFileLoader is that authentication is handled\nseamlessly to cloud storage. You can use either identity-based data\naccess control to the data or credential-based (e.g. SAS token,\naccount key). In the case of credential-based data access you do not\nneed to specify secrets in your code or set up key vaults - the system\nhandles that for you. This notebook covers how to load document objects from a data asset in\nAI Studio. #!pip install azureml-fsspec, azure-ai-generative from azure . ai . resources . client import AIClient from azure . identity import DefaultAzureCredential from langchain . document_loaders import AzureAIDataLoader # Create a connection to your project client = AIClient ( credential = DefaultAzureCredential ( ) , subscription_id = \"<subscription_id>\" , resource_group_name = \"<resource_group_name>\" , project_name = \"<project_name>\" , ) # get the latest version of your data asset data_asset = client . data . get ( name = \"<data_asset_name>\" , label = \"latest\" ) # load the data asset loader = AzureAIDataLoader ( url = data_asset . path ) loader . load ( ) [Document(page_content='Lorem ipsum dolor sit amet.', lookup_str='', metadata={'source': '/var/folders/y6/8_bzdg295ld6s1_97_12m4lr0000gn/T/tmpaa9xl6ch/fake.docx'}, lookup_index=0)] Specifying a glob pattern ​ You can also specify a glob pattern for more finegrained control over\nwhat files to load. In the example below, only files with a pdf extension will be loaded. loader = AzureAIDataLoader ( url = data_asset . path , glob = \"*.pdf\" ) loader . load ( ) [Document(page_content='Lorem ipsum dolor sit amet.', lookup_str='', metadata={'source': '/var/folders/y6/8_bzdg295ld6s1_97_12m4lr0000gn/T/tmpujbkzf_l/fake.docx'}, lookup_index=0)] Previous AZLyrics Next Azure Blob Storage Container"
    },
    {
        "title": "Azure Blob Storage Container | 🦜️🔗 Langchain",
        "url": "https://python.langchain.com/docs/integrations/document_loaders/azure_blob_storage_container",
        "text_content": "Components Document loaders Azure Blob Storage Container On this page Azure Blob Storage Container Azure Blob\nStorage is Microsoft’s object storage solution for the cloud. Blob Storage is\noptimized for storing massive amounts of unstructured data.\nUnstructured data is data that doesn’t adhere to a particular data\nmodel or definition, such as text or binary data. Azure Blob Storage is designed for: - Serving images or documents\ndirectly to a browser. - Storing files for distributed access. -\nStreaming video and audio. - Writing to log files. - Storing data for\nbackup and restore, disaster recovery, and archiving. - Storing data for\nanalysis by an on-premises or Azure-hosted service. This notebook covers how to load document objects from a container on Azure Blob Storage . #!pip install azure-storage-blob from langchain . document_loaders import AzureBlobStorageContainerLoader loader = AzureBlobStorageContainerLoader ( conn_str = \"<conn_str>\" , container = \"<container>\" ) loader . load ( ) [Document(page_content='Lorem ipsum dolor sit amet.', lookup_str='', metadata={'source': '/var/folders/y6/8_bzdg295ld6s1_97_12m4lr0000gn/T/tmpaa9xl6ch/fake.docx'}, lookup_index=0)] Specifying a prefix ​ You can also specify a prefix for more finegrained control over what\nfiles to load. loader = AzureBlobStorageContainerLoader ( conn_str = \"<conn_str>\" , container = \"<container>\" , prefix = \"<prefix>\" ) loader . load ( ) [Document(page_content='Lorem ipsum dolor sit amet.', lookup_str='', metadata={'source': '/var/folders/y6/8_bzdg295ld6s1_97_12m4lr0000gn/T/tmpujbkzf_l/fake.docx'}, lookup_index=0)] Previous Azure AI Data Next Azure Blob Storage File"
    }
]